# -*- coding: utf-8 -*-
"""
agri-news-brief main.py (production)

Fixes included (per latest request):
1) "Î∏åÎ¶¨Ìïë Ïó¥Í∏∞"Í∞Ä gist.github.com ÏúºÎ°ú ÎÑòÏñ¥Í∞ÄÎäî ÏπòÎ™ÖÏ†Å Î¨∏Ï†ú Î∞©ÏßÄ:
   - PAGES_BASE_URLÏù¥ gist/rawÎ°ú ÏûòÎ™ª ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÏñ¥ÎèÑ ÏûêÎèô Î¨¥ÏãúÌïòÍ≥† GitHub Pages Í∏∞Î≥∏ URLÎ°ú Î≥µÍµ¨
   - ÏµúÏ¢Ö daily_urlÏù¥ gist/rawÎ©¥ Ï¶âÏãú Ï§ëÎã® (ÏïàÏ†ÑÏû•Ïπò)

2) Ïπ¥ÌÜ° Î©îÏãúÏßÄ Ìè¨Îß∑ÏùÑ ÏÇ¨Ïö©ÏûêÍ∞Ä ÏöîÍµ¨Ìïú Ï§ÑÎ∞îÍøà/ÎùÑÏñ¥Ïì∞Í∏∞ ÌòïÌÉúÎ°ú "Í≥†Ï†ï" ÏÉùÏÑ±:
   - Ï†úÎ™©, Í≥µÎ∞± Ï§Ñ, Í∏∞ÏÇ¨ ÏßëÍ≥Ñ(Ï¥ù/Ï§ëÏïô/ÏßÄÎ∞©), ÏÑπÏÖòÎ≥Ñ 2Í±¥Ïî© Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏
   - ÏÑπÏÖò ÏàúÏÑú: ÌíàÎ™© ‚Üí Ï†ïÏ±Ö ‚Üí Ïú†ÌÜµ ‚Üí Î∞©Ï†ú (Í≥†Ï†ï)

Í∏∞Îä•:
- Naver News API Í≤ÄÏÉâ(ÏÑπÏÖòÎ≥Ñ Î©ÄÌã∞ ÏøºÎ¶¨)
- Í∞ïÌïú Í¥ÄÎ†®ÎèÑ ÌïÑÌÑ∞ÎßÅ(Ïó∞Ïòà/Ïó¨Ìñâ/Ï£ºÏãù/Î¨¥Í¥Ä Í∏∞ÏÇ¨ Ï∞®Îã®)
- ÏòÅÏóÖÏùº Í∏∞Ï§Ä ÏúàÎèÑÏö∞(Ìú¥Ïùº/Ï£ºÎßêÏùÄ Îã§Ïùå ÏòÅÏóÖÏùºÏóê ÎàÑÏ†Å)
- OpenAI ÏöîÏïΩ(ÏòµÏÖò): Ïã§Ìå®/ÏøºÌÑ∞/ÌÇ§ ÏóÜÏùåÏù¥Î©¥ description Í∏∞Î∞òÏúºÎ°ú ÏûêÎèô Ìè¥Î∞±
- GitHub Pages Ï∂úÎ†•:
  - docs/index.html (ÏµúÏã†/ÏïÑÏπ¥Ïù¥Î∏å)
  - docs/archive/YYYY-MM-DD.html (ÏùºÏûêÎ≥Ñ Ïä§ÎÉÖÏÉ∑)
- Ïπ¥Ïπ¥Ïò§ "ÎÇòÏóêÍ≤å Î≥¥ÎÇ¥Í∏∞" Îã®Ïùº Î©îÏãúÏßÄ + "Î∏åÎ¶¨Ìïë Ïó¥Í∏∞" Î≤ÑÌäº(Ìï¥Îãπ ÎÇ†Ïßú ÌéòÏù¥ÏßÄÎ°ú)

ENV REQUIRED:
- NAVER_CLIENT_ID
- NAVER_CLIENT_SECRET
- GITHUB_REPO              (e.g., HongTaeHwa/agri-news-brief)  ÎòêÎäî Actions Í∏∞Î≥∏ GITHUB_REPOSITORY
- GH_TOKEN or GITHUB_TOKEN (Actions built-in token OK if permissions: contents: write)
- KAKAO_REST_API_KEY
- KAKAO_REFRESH_TOKEN

OPTIONAL:
- OPENAI_API_KEY           (ÏóÜÍ±∞ÎÇò/Ïã§Ìå®ÌïòÎ©¥ Ìè¥Î∞±)
- OPENAI_MODEL             (default: gpt-5.2)
- KAKAO_CLIENT_SECRET
- PAGES_BASE_URL           (Ïª§Ïä§ÌÖÄ ÎèÑÎ©îÏù∏/Ï°∞ÏßÅ ÌéòÏù¥ÏßÄ Îì±)
- REPORT_HOUR_KST          (default: 7)
- MAX_PER_SECTION          (default: 10)
- MIN_PER_SECTION          (default: 5)
- EXTRA_HOLIDAYS           (comma dates, e.g., 2026-02-17,2026-02-18)
- EXCLUDE_HOLIDAYS         (comma dates to treat as business day)
- KAKAO_INCLUDE_LINK_IN_TEXT (true/false, default false)
- FORCE_REPORT_DATE        (YYYY-MM-DD) backfill test
- FORCE_RUN_ANYDAY         (true/false) Ìú¥Ïùº/Ï£ºÎßêÏóêÎèÑ Í∞ïÏ†ú Ïã§Ìñâ(ÌÖåÏä§Ìä∏Ïö©)
- FORCE_END_NOW            (true/false) endÎ•º "ÏßÄÍ∏à"ÏúºÎ°ú(ÌÖåÏä§Ìä∏Ïö©, Í∏∞ÏÇ¨Îüâ Ï¶ùÍ∞Ä)
"""

import os
import re
import json
import base64
import html
import logging
import hashlib
from dataclasses import dataclass
from datetime import datetime, timedelta, date, timezone
from email.utils import parsedate_to_datetime
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import requests


# -----------------------------
# Logging
# -----------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
log = logging.getLogger("agri-brief")


# -----------------------------
# Config
# -----------------------------
KST = timezone(timedelta(hours=9))

REPORT_HOUR_KST = int(os.getenv("REPORT_HOUR_KST", "7"))
MAX_PER_SECTION = int(os.getenv("MAX_PER_SECTION", "10"))
MIN_PER_SECTION = int(os.getenv("MIN_PER_SECTION", "5"))

STATE_FILE_PATH = ".agri_state.json"
ARCHIVE_MANIFEST_PATH = ".agri_archive.json"

DOCS_INDEX_PATH = "docs/index.html"
DOCS_ARCHIVE_DIR = "docs/archive"

DEFAULT_REPO = (os.getenv("GITHUB_REPO") or os.getenv("GITHUB_REPOSITORY") or "").strip()
GH_TOKEN = (os.getenv("GH_TOKEN") or os.getenv("GITHUB_TOKEN") or "").strip()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "").strip()
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "").strip()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-5.2").strip()

KAKAO_REST_API_KEY = os.getenv("KAKAO_REST_API_KEY", "").strip()
KAKAO_REFRESH_TOKEN = os.getenv("KAKAO_REFRESH_TOKEN", "").strip()
KAKAO_CLIENT_SECRET = os.getenv("KAKAO_CLIENT_SECRET", "").strip()

KAKAO_INCLUDE_LINK_IN_TEXT = os.getenv("KAKAO_INCLUDE_LINK_IN_TEXT", "false").strip().lower() in ("1", "true", "yes")

FORCE_REPORT_DATE = os.getenv("FORCE_REPORT_DATE", "").strip()  # YYYY-MM-DD
FORCE_RUN_ANYDAY = os.getenv("FORCE_RUN_ANYDAY", "false").strip().lower() in ("1", "true", "yes")
FORCE_END_NOW = os.getenv("FORCE_END_NOW", "false").strip().lower() in ("1", "true", "yes")

EXTRA_HOLIDAYS = set([s.strip() for s in os.getenv("EXTRA_HOLIDAYS", "").split(",") if s.strip()])
EXCLUDE_HOLIDAYS = set([s.strip() for s in os.getenv("EXCLUDE_HOLIDAYS", "").split(",") if s.strip()])


# -----------------------------
# Domain blocks
# -----------------------------
BLOCKED_DOMAINS = {
    "wikitree.co.kr",
    "theqoo.net",
    "instiz.net",
    "namu.wiki",
    "allurekorea.com",
    "vogue.co.kr",
    "marieclairekorea.com",
    "cosmopolitan.co.kr",
    "gqkorea.co.kr",
}

# Strong agriculture context keywords (raise relevance)
AGRI_STRONG_TERMS = [
    "Í∞ÄÎùΩÏãúÏû•", "ÎèÑÎß§ÏãúÏû•", "Í≥µÌåêÏû•", "Í≤ΩÎùΩ", "Í≤ΩÎùΩÍ∞Ä", "Í≤ΩÎß§", "Ï≤≠Í≥º", "ÏÇ∞ÏßÄ", "Ï∂úÌïò", "Î¨ºÎüâ", "Î∞òÏûÖ",
    "ÏÇ∞ÏßÄÏú†ÌÜµ", "APC", "ÏÇ∞ÏßÄÏú†ÌÜµÏÑºÌÑ∞", "ÏÑ†Î≥Ñ", "CAÏ†ÄÏû•", "Ï†ÄÏû•Í≥†", "Ï†ÄÏû•Îüâ",
    "ÏãúÏÑ∏", "ÎèÑÎß§Í∞ÄÍ≤©", "ÏÜåÎß§Í∞ÄÍ≤©", "Í∞ÄÍ≤©", "ÏàòÍ∏â", "ÏàòÍ∏âÎèôÌñ•", "ÏûëÌô©", "ÏÉùÏÇ∞Îüâ", "Ïû¨Î∞∞", "ÏàòÌôï", "Î©¥Ï†Å",
    "ÎÜçÎ¶ºÏ∂ïÏÇ∞ÏãùÌíàÎ∂Ä", "ÎÜçÏãùÌíàÎ∂Ä", "aT", "ÌïúÍµ≠ÎÜçÏàòÏÇ∞ÏãùÌíàÏú†ÌÜµÍ≥µÏÇ¨", "ÎÜçÍ¥ÄÏõê", "Íµ≠Î¶ΩÎÜçÏÇ∞Î¨ºÌíàÏßàÍ¥ÄÎ¶¨Ïõê",
    "Í≤ÄÏó≠", "Ìï†ÎãπÍ¥ÄÏÑ∏", "ÏàòÏûÖ", "ÏàòÏ∂ú", "Í¥ÄÏÑ∏", "ÌÜµÍ¥Ä", "ÏõêÏÇ∞ÏßÄ", "Î∂ÄÏ†ïÏú†ÌÜµ", "Ïò®ÎùºÏù∏ ÎèÑÎß§ÏãúÏû•",
    "ÎπÑÏ∂ïÎØ∏", "Ï†ïÎ∂Ä", "ÎåÄÏ±Ö", "ÏßÄÏõê", "Ìï†Ïù∏ÏßÄÏõê", "ÏÑ±ÏàòÌíà",
    "Î≥ëÌï¥Ï∂©", "Î∞©Ï†ú", "ÏïΩÏ†ú", "ÏÇ¥Ìè¨", "ÏòàÏ∞∞", "Í≥ºÏàòÌôîÏÉÅÎ≥ë", "ÌÉÑÏ†ÄÎ≥ë", "ÎèôÌï¥", "ÎÉâÌï¥", "ÏõîÎèô",
]

# Very common off-topic hints (penalize)
OFFTOPIC_HINTS = [
    "Î∞∞Ïö∞", "ÏïÑÏù¥Îèå", "ÎìúÎùºÎßà", "ÏòÅÌôî", "ÏòàÎä•", "ÏΩòÏÑúÌä∏", "Ìå¨", "Ïú†ÌäúÎ∏å", "ÎÆ§ÏßÅ",
    "ÎåÄÌÜµÎ†π", "Íµ≠Ìöå", "Ï¥ùÏÑ†", "Í≤ÄÏ∞∞", "Ïû¨Ìåê", "ÌÉÑÌïµ", "Ï†ïÎãπ",
    "ÏΩîÏä§Ìîº", "ÏΩîÏä§Îã•", "Ï£ºÍ∞Ä", "Í∏âÎì±", "Í∏âÎùΩ", "ÎπÑÌä∏ÏΩîÏù∏", "ÌôòÏú®",
    "Ïó¨Ìñâ", "Í¥ÄÍ¥ë", "Ìò∏ÌÖî", "Î¶¨Ï°∞Ìä∏", "Î†àÏä§ÌÜ†Îûë", "ÏôÄÏù∏", "Ìï¥Î≥Ä", "Ìú¥Ïñë", "ÌååÏö¥Îìú", "Îã¨Îü¨", "Ïú†Î°ú",
]

TRAVEL_MARKET_HINTS = [
    "ÌòÑÏßÄ", "Ï†ÑÌÜµÏãúÏû•", "ÎÖ∏Ï†ê", "ÌååÏö¥Îìú", "Î°úÏ†ú", "ÌÉÄÌååÏä§", "Î¶¨ÎπÑÏóêÎùº", "ÌîÑÎûëÏä§", "ÎëêÎ∞îÏù¥",
]

KOREA_CONTEXT_HINTS = [
    "Íµ≠ÎÇ¥", "ÌïúÍµ≠", "Ïö∞Î¶¨ÎÇòÎùº", "ÎÜçÌòë", "ÏßÄÏûêÏ≤¥", "Íµ∞", "Ïãú", "ÎèÑ", "ÎÜçÍ∞Ä", "ÏÇ∞ÏßÄ", "Í∞ÄÎùΩÏãúÏû•",
    "ÎÜçÏãùÌíàÎ∂Ä", "aT", "ÎÜçÍ¥ÄÏõê", "ÎåÄÌïúÎØºÍµ≠", "ÏÑ§", "Î™ÖÏ†à",
]


# -----------------------------
# Section configuration (order fixed in rendering; Kakao message order is separately fixed)
# -----------------------------
SECTIONS = [
    {
        "key": "supply",
        "title": "ÌíàÎ™© Î∞è ÏàòÍ∏â ÎèôÌñ•",
        "color": "#0f766e",
        "queries": [
            "Í∏∞ÌõÑÎ≥ÄÌôî ÏÇ¨Í≥º Ïû¨Î∞∞ÏßÄ Î∂ÅÏÉÅ Í∞ïÏõêÎèÑ",
            "Í≥ºÏàò Ïû¨Î∞∞Î©¥Ï†Å Î≥ÄÌôî ÏÇ¨Í≥º Î∞∞",
            "ÏÇ¨Í≥º ÎèÑÎß§ÏãúÏû• Í∞ÄÍ≤© ÏãúÏÑ∏",
            "ÏÇ¨Í≥º Ï†ÄÏû•Îüâ Ï∂úÌïò ÏàòÍ∏â",
            "Î∞∞(Í≥ºÏùº) ÎèÑÎß§ÏãúÏû• ÏãúÏÑ∏",
            "Îã®Í∞ê ÏãúÏÑ∏ Ï†ÄÏû•Îüâ",
            "Îñ´ÏùÄÍ∞ê Í≥∂Í∞ê ÌÉÑÏ†ÄÎ≥ë ÏÉùÏÇ∞Îüâ Í∞ÄÍ≤©",
            "Îë•Ïãú Í≥∂Í∞ê Î¨ºÎüâ ÏãúÏÑ∏",
            "Í∞êÍ∑§ ÌïúÎùºÎ¥â Î†àÎìúÌñ• Ï≤úÌòúÌñ• ÏãúÏÑ∏",
            "Ï†úÏ£º ÎßåÍ∞êÎ•ò Ï∂úÌïò Í∞ÄÍ≤©",
            "Ï∞∏Îã§Îûò ÌÇ§ÏúÑ ÏãúÏÑ∏",
            "ÏÉ§Ïù∏Î®∏Ïä§Ï∫£ Ìè¨ÎèÑ ÏãúÏÑ∏ Ï∂úÌïò",
            "Ï†àÌôî Ï°∏ÏóÖ ÏûÖÌïô ÏãúÏ¶å Í∞ÄÍ≤©",
            "ÌíãÍ≥†Ï∂î Ïò§Ïù¥ ÏãúÏÑ§Ï±ÑÏÜå Í∞ÄÍ≤© ÏùºÏ°∞Îüâ",
            "ÏåÄ ÏÇ∞ÏßÄ Í∞ÄÍ≤© ÎπÑÏ∂ïÎØ∏ Î∞©Ï∂ú",
        ],
        "must_terms": ["Í∞ÄÍ≤©", "ÏãúÏÑ∏", "ÏàòÍ∏â", "Ï∂úÌïò", "ÎèÑÎß§", "Í≤ΩÎùΩ", "Ï†ÄÏû•Îüâ", "ÏûëÌô©", "ÏÉùÏÇ∞Îüâ", "Ïû¨Î∞∞", "ÏàòÌôï", "Î©¥Ï†Å", "Î¨ºÎüâ"],
    },
    {
        "key": "policy",
        "title": "Ï£ºÏöî Ïù¥Ïäà Î∞è Ï†ïÏ±Ö",
        "color": "#1d4ed8",
        "queries": [
            "ÎÜçÏÇ∞Î¨º Ïò®ÎùºÏù∏ ÎèÑÎß§ÏãúÏû• ÌóàÏúÑÍ±∞Îûò Ï†ÑÏàòÏ°∞ÏÇ¨",
            "ÎÜçÏ∂ïÏàòÏÇ∞Î¨º Ìï†Ïù∏ÏßÄÏõê Ïó∞Ïû• 3Ïõî",
            "Ìï†ÎãπÍ¥ÄÏÑ∏ ÏàòÏûÖ Í≥ºÏùº Í≤ÄÏó≠ ÏôÑÌôî",
            "ÏÑ±ÏàòÌíà Í∞ÄÍ≤© ÏïàÏ†ï ÎåÄÏ±Ö ÎÜçÏ∂ïÏàòÏÇ∞Î¨º",
            "ÎåÄÌïúÎØºÍµ≠ Ï†ïÏ±ÖÎ∏åÎ¶¨Ìïë ÎÜçÏ∂ïÏàòÏÇ∞Î¨º",
            "korea.kr ÎÜçÏ∂ïÏàòÏÇ∞Î¨º Ìï†Ïù∏",
            "ÎÜçÏãùÌíàÎ∂Ä Ï†ïÏ±Ö ÎÜçÏ∂ïÏàòÏÇ∞Î¨º Ìï†ÎãπÍ¥ÄÏÑ∏",
        ],
        "must_terms": ["Ï†ïÏ±Ö", "ÎåÄÏ±Ö", "ÏßÄÏõê", "Ìï†Ïù∏", "Ìï†ÎãπÍ¥ÄÏÑ∏", "Í≤ÄÏó≠", "Ïò®ÎùºÏù∏ ÎèÑÎß§ÏãúÏû•", "ÎπÑÏ∂ïÎØ∏", "ÏÑ±ÏàòÌíà", "ÏàòÍ∏â"],
    },
    {
        "key": "pest",
        "title": "Î≥ëÌï¥Ï∂© Î∞è Î∞©Ï†ú",
        "color": "#b45309",
        "queries": [
            "Í≥ºÏàòÌôîÏÉÅÎ≥ë ÏïΩÏ†ú Ïã†Ï≤≠ ÎßàÍ∞ê",
            "Í≥ºÏàòÌôîÏÉÅÎ≥ë Í∂§Ïñë Ï†úÍ±∞ Í≥®Îì†ÌÉÄÏûÑ",
            "ÏõîÎèô Ìï¥Ï∂© Î∞©Ï†ú Í∏∞Í≥ÑÏú†Ïú†Ï†ú ÏÇ¥Ìè¨",
            "ÌÉÑÏ†ÄÎ≥ë ÏòàÎ∞© Î∞©Ï†ú",
            "ÎèôÌï¥ ÎÉâÌï¥ Í≥ºÏàò ÌîºÌï¥ ÎåÄÎπÑ",
        ],
        "must_terms": ["Î∞©Ï†ú", "Î≥ëÌï¥Ï∂©", "ÏïΩÏ†ú", "ÏÇ¥Ìè¨", "ÏòàÏ∞∞", "Í≥ºÏàòÌôîÏÉÅÎ≥ë", "ÌÉÑÏ†ÄÎ≥ë", "ÎÉâÌï¥", "ÎèôÌï¥", "ÏõîÎèô"],
    },
    {
        "key": "dist",
        "title": "Ïú†ÌÜµ Î∞è ÌòÑÏû• (APC/ÏàòÏ∂ú)",
        "color": "#6d28d9",
        "queries": [
            "APC Ïä§ÎßàÌä∏Ìôî AI ÏÑ†Î≥ÑÍ∏∞ CAÏ†ÄÏû•",
            "ÎÜçÌòë APC ÏÑ†Î≥Ñ Ï†ÄÏû•",
            "ÎÜçÏãùÌíà ÏàòÏ∂ú Ïã§Ï†Å Î∞∞ Îî∏Í∏∞ Ìè¨ÎèÑ",
            "Í∞ÄÎùΩÏãúÏû• Í≤ΩÎß§ Ïû¨Í∞ú ÏùºÏ†ï Ìú¥Î¨¥",
            "ÏõêÏÇ∞ÏßÄ Îã®ÏÜç ÎÜçÏÇ∞Î¨º Î∂ÄÏ†ïÏú†ÌÜµ",
        ],
        "must_terms": ["APC", "ÏÑ†Î≥Ñ", "CAÏ†ÄÏû•", "Í≥µÌåêÏû•", "ÎèÑÎß§ÏãúÏû•", "Í∞ÄÎùΩÏãúÏû•", "ÏàòÏ∂ú", "Í≤ÄÏó≠", "ÏõêÏÇ∞ÏßÄ", "Ïú†ÌÜµ"],
    },
]


POLICY_DOMAINS = {
    "korea.kr", "www.korea.kr",
    "mafra.go.kr", "www.mafra.go.kr",
    "at.or.kr", "www.at.or.kr",
    "naqs.go.kr", "www.naqs.go.kr",
    "krei.re.kr", "www.krei.re.kr",
}

AGRI_POLICY_KEYWORDS = [
    "ÎÜçÏ∂ïÏàòÏÇ∞Î¨º", "ÎÜçÏ∂ïÏÇ∞Î¨º", "ÏÑ±ÏàòÌíà", "Ìï†Ïù∏ÏßÄÏõê", "Ìï†ÎãπÍ¥ÄÏÑ∏", "Í≤ÄÏó≠", "ÏàòÍ∏â", "Í∞ÄÍ≤©", "Í≥ºÏùº", "ÎπÑÏ∂ïÎØ∏", "ÏõêÏÇ∞ÏßÄ"
]


# -----------------------------
# Data model
# -----------------------------
@dataclass
class Article:
    section: str
    title: str
    description: str
    link: str
    originallink: str
    pub_dt_kst: datetime
    domain: str
    press: str
    norm_key: str
    score: float = 0.0
    summary: str = ""


# -----------------------------
# Utilities
# -----------------------------
def now_kst() -> datetime:
    return datetime.now(tz=KST)

def dt_kst(d: date, hour: int) -> datetime:
    return datetime(d.year, d.month, d.day, hour, 0, 0, tzinfo=KST)

def parse_pubdate_to_kst(pubdate_str: str) -> datetime:
    try:
        dt = parsedate_to_datetime(pubdate_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(KST)
    except Exception:
        return datetime.min.replace(tzinfo=KST)

def clean_text(s: str) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def domain_of(url: str) -> str:
    try:
        return (urlparse(url).netloc or "").lower()
    except Exception:
        return ""

def strip_tracking_params(url: str) -> str:
    try:
        u = urlparse(url)
        q = [(k, v) for (k, v) in parse_qsl(u.query, keep_blank_values=True)
             if not k.lower().startswith("utm_") and k.lower() not in ("gclid", "fbclid", "igshid", "ref")]
        new_q = urlencode(q, doseq=True)
        return urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))
    except Exception:
        return url

def norm_title_key(title: str) -> str:
    t = title.lower()
    t = re.sub(r"\[[^\]]+\]", " ", t)
    t = re.sub(r"[^0-9a-zÍ∞Ä-Ìû£]+", "", t)
    return t[:80]

def make_norm_key(originallink: str, link: str, title: str) -> str:
    u = strip_tracking_params(originallink or link or "")
    if u:
        h = hashlib.sha1(u.encode("utf-8")).hexdigest()[:16]
        return f"url:{h}"
    return f"title:{norm_title_key(title)}"

def has_any(text: str, words) -> bool:
    return any(w in text for w in words)

def count_any(text: str, words) -> int:
    return sum(1 for w in words if w in text)


# -----------------------------
# KR business day / holidays
# -----------------------------
def is_weekend(d: date) -> bool:
    return d.weekday() >= 5

def is_korean_holiday(d: date) -> bool:
    s = d.isoformat()
    if s in EXCLUDE_HOLIDAYS:
        return False
    if s in EXTRA_HOLIDAYS:
        return True
    try:
        import holidays  # type: ignore
        kr = holidays.KR(years=[d.year], observed=True)
        return d in kr
    except Exception:
        return False

def is_business_day_kr(d: date) -> bool:
    if is_weekend(d):
        return False
    if is_korean_holiday(d):
        return False
    return True

def previous_business_day(d: date) -> date:
    cur = d - timedelta(days=1)
    while not is_business_day_kr(cur):
        cur -= timedelta(days=1)
    return cur


# -----------------------------
# GitHub Contents API helpers
# -----------------------------
def github_api_headers(token: str):
    return {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github+json",
        "User-Agent": "agri-news-brief-bot",
    }

def github_get_file(repo: str, path: str, token: str, ref: str = "main"):
    url = f"https://api.github.com/repos/{repo}/contents/{path}"
    r = requests.get(url, headers=github_api_headers(token), params={"ref": ref}, timeout=30)
    if r.status_code == 404:
        return None, None
    if not r.ok:
        log.error("[GitHub GET ERROR] %s", r.text)
        r.raise_for_status()
    j = r.json()
    content_b64 = j.get("content", "")
    sha = j.get("sha")
    if content_b64:
        raw = base64.b64decode(content_b64).decode("utf-8", errors="replace")
    else:
        raw = ""
    return raw, sha

def github_put_file(repo: str, path: str, content: str, token: str, message: str, sha: str = None, branch: str = "main"):
    url = f"https://api.github.com/repos/{repo}/contents/{path}"
    payload = {
        "message": message,
        "content": base64.b64encode(content.encode("utf-8")).decode("utf-8"),
        "branch": branch,
    }
    if sha:
        payload["sha"] = sha
    r = requests.put(url, headers=github_api_headers(token), json=payload, timeout=30)
    if not r.ok:
        log.error("[GitHub PUT ERROR] %s", r.text)
        r.raise_for_status()
    return r.json()


# -----------------------------
# State / archive manifest (legacy-safe)
# -----------------------------
def load_state(repo: str, token: str):
    raw, _sha = github_get_file(repo, STATE_FILE_PATH, token, ref="main")
    if not raw:
        return {"last_end_iso": None}
    try:
        obj = json.loads(raw)
        if isinstance(obj, dict):
            return obj
        return {"last_end_iso": None}
    except Exception:
        return {"last_end_iso": None}

def save_state(repo: str, token: str, last_end: datetime):
    payload = {"last_end_iso": last_end.isoformat()}
    _raw_old, sha = github_get_file(repo, STATE_FILE_PATH, token, ref="main")
    github_put_file(repo, STATE_FILE_PATH, json.dumps(payload, ensure_ascii=False, indent=2), token,
                    f"Update state {last_end.date().isoformat()}", sha=sha, branch="main")

def _normalize_manifest(obj):
    # supports legacy list format OR dict format
    if obj is None:
        return {"dates": []}
    if isinstance(obj, list):
        return {"dates": [str(x) for x in obj if str(x).strip()]}
    if isinstance(obj, dict):
        dates = obj.get("dates", [])
        if isinstance(dates, list):
            return {"dates": [str(x) for x in dates if str(x).strip()]}
        if isinstance(dates, str) and dates.strip():
            return {"dates": [dates.strip()]}
        return {"dates": []}
    return {"dates": []}

def load_archive_manifest(repo: str, token: str):
    raw, sha = github_get_file(repo, ARCHIVE_MANIFEST_PATH, token, ref="main")
    if not raw:
        return {"dates": []}, sha
    try:
        obj = json.loads(raw)
        return _normalize_manifest(obj), sha
    except Exception:
        return {"dates": []}, sha

def save_archive_manifest(repo: str, token: str, manifest: dict, sha: str):
    manifest = _normalize_manifest(manifest)
    github_put_file(repo, ARCHIVE_MANIFEST_PATH, json.dumps(manifest, ensure_ascii=False, indent=2), token,
                    "Update archive manifest", sha=sha, branch="main")


# -----------------------------
# Naver News search
# -----------------------------
def naver_news_search(query: str, display: int = 30, start: int = 1, sort: str = "date"):
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        raise RuntimeError("NAVER_CLIENT_ID / NAVER_CLIENT_SECRET not set")
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    params = {"query": query, "display": display, "start": start, "sort": sort}
    r = requests.get(url, headers=headers, params=params, timeout=30)
    if not r.ok:
        log.error("[NAVER ERROR] %s", r.text)
        r.raise_for_status()
    return r.json()


# -----------------------------
# Relevance scoring / filtering
# -----------------------------
def is_blocked_domain(dom: str) -> bool:
    if not dom:
        return False
    dom = dom.lower()
    if dom in BLOCKED_DOMAINS:
        return True
    for b in BLOCKED_DOMAINS:
        if dom.endswith("." + b):
            return True
    return False

def agri_strength_score(text: str) -> int:
    return count_any(text, AGRI_STRONG_TERMS)

def off_topic_penalty(text: str) -> int:
    return count_any(text, OFFTOPIC_HINTS)

def travel_penalty(text: str) -> int:
    return count_any(text, TRAVEL_MARKET_HINTS)

def korea_context_score(text: str) -> int:
    return count_any(text, KOREA_CONTEXT_HINTS)

def section_must_terms_ok(text: str, must_terms) -> bool:
    return has_any(text, must_terms)

def policy_domain_override(dom: str, text: str) -> bool:
    if dom in POLICY_DOMAINS:
        return has_any(text, AGRI_POLICY_KEYWORDS)
    return False

def is_relevant(article: Article, section_conf: dict) -> bool:
    dom = article.domain
    if is_blocked_domain(dom):
        return False

    text = (article.title + " " + article.description).lower()

    # must_terms gate (policy domains can override)
    if not section_must_terms_ok(text, [t.lower() for t in section_conf["must_terms"]]):
        if not policy_domain_override(dom, text):
            return False

    strength = agri_strength_score(text)
    offp = off_topic_penalty(text)
    trav = travel_penalty(text)
    korea = korea_context_score(text)

    if trav >= 1 and korea == 0 and strength < 3:
        return False

    if offp >= 1 and strength < 3:
        return False

    # disambiguation: "ÏÇ¨Í≥º" apology
    if re.search(r"(Í≥µÍ∞ú\s*)?ÏÇ¨Í≥º(ÌñàÎã§|Ìï¥Ïïº|ÌïòÎùº|Î¨∏|ÏöîÍµ¨|ÏöîÏ≤≠|Î∞úÌëú)", article.title) and strength < 4:
        return False

    # disambiguation: "Î∞∞" ship
    if re.search(r"(ÏÑ†Î∞ï|Ìï¥Íµ∞|Ìï≠Îßå|Ï°∞ÏÑ†|Ìï®Ï†ï|ÏäπÏÑ†|Ìï≠Ìï¥)", text) and strength < 4:
        return False

    return True

def compute_rank_score(article: Article, section_conf: dict) -> float:
    text = (article.title + " " + article.description).lower()
    strength = agri_strength_score(text)
    korea = korea_context_score(text)
    offp = off_topic_penalty(text)
    trav = travel_penalty(text)

    score = 0.0
    score += strength * 2.0
    score += korea * 0.8
    score -= offp * 2.5
    score -= trav * 2.0

    if article.domain in POLICY_DOMAINS:
        score += 3.0

    age_hours = max(0.0, (datetime.now(tz=KST) - article.pub_dt_kst).total_seconds() / 3600.0)
    score += max(0.0, 24.0 - min(age_hours, 24.0)) * 0.05

    for t in section_conf["must_terms"]:
        if t.lower() in article.title.lower():
            score += 0.6

    return score


# -----------------------------
# Collect articles
# -----------------------------
def collect_articles_for_section(section_conf: dict, start_kst: datetime, end_kst: datetime):
    items: list[Article] = []
    seen_keys = set()

    display = 40

    for q in section_conf["queries"]:
        try:
            data = naver_news_search(q, display=display, start=1, sort="date")
            for it in data.get("items", []):
                title = clean_text(it.get("title", ""))
                desc = clean_text(it.get("description", ""))
                link = strip_tracking_params(it.get("link", "") or "")
                origin = strip_tracking_params(it.get("originallink", "") or link)
                pub = parse_pubdate_to_kst(it.get("pubDate", ""))

                if pub < start_kst or pub >= end_kst:
                    continue

                dom = domain_of(origin) or domain_of(link)
                if is_blocked_domain(dom):
                    continue

                press = dom
                PRESS_MAP = {
                    "www.yna.co.kr": "Ïó∞Ìï©Îâ¥Ïä§", "yna.co.kr": "Ïó∞Ìï©Îâ¥Ïä§",
                    "www.mk.co.kr": "Îß§ÏùºÍ≤ΩÏ†ú", "mk.co.kr": "Îß§ÏùºÍ≤ΩÏ†ú",
                    "www.joongang.co.kr": "Ï§ëÏïôÏùºÎ≥¥", "joongang.co.kr": "Ï§ëÏïôÏùºÎ≥¥",
                    "www.chosun.com": "Ï°∞ÏÑ†ÏùºÎ≥¥", "chosun.com": "Ï°∞ÏÑ†ÏùºÎ≥¥",
                    "www.donga.com": "ÎèôÏïÑÏùºÎ≥¥", "donga.com": "ÎèôÏïÑÏùºÎ≥¥",
                    "www.hani.co.kr": "ÌïúÍ≤®Î†à", "hani.co.kr": "ÌïúÍ≤®Î†à",
                    "www.khan.co.kr": "Í≤ΩÌñ•Ïã†Î¨∏", "khan.co.kr": "Í≤ΩÌñ•Ïã†Î¨∏",
                    "www.sedaily.com": "ÏÑúÏö∏Í≤ΩÏ†ú", "sedaily.com": "ÏÑúÏö∏Í≤ΩÏ†ú",
                    "www.hankyung.com": "ÌïúÍµ≠Í≤ΩÏ†ú", "hankyung.com": "ÌïúÍµ≠Í≤ΩÏ†ú",
                    "www.asiae.co.kr": "ÏïÑÏãúÏïÑÍ≤ΩÏ†ú", "asiae.co.kr": "ÏïÑÏãúÏïÑÍ≤ΩÏ†ú",
                    "www.mt.co.kr": "Î®∏ÎãàÌà¨Îç∞Ïù¥", "mt.co.kr": "Î®∏ÎãàÌà¨Îç∞Ïù¥",
                    "www.edaily.co.kr": "Ïù¥Îç∞ÏùºÎ¶¨", "edaily.co.kr": "Ïù¥Îç∞ÏùºÎ¶¨",
                    "www.heraldcorp.com": "Ìó§Îü¥ÎìúÍ≤ΩÏ†ú", "heraldcorp.com": "Ìó§Îü¥ÎìúÍ≤ΩÏ†ú",
                    "www.newsis.com": "Îâ¥ÏãúÏä§", "newsis.com": "Îâ¥ÏãúÏä§",
                    "www.news1.kr": "Îâ¥Ïä§1", "news1.kr": "Îâ¥Ïä§1",
                    "www.fnnews.com": "ÌååÏù¥ÎÇ∏ÏÖúÎâ¥Ïä§", "fnnews.com": "ÌååÏù¥ÎÇ∏ÏÖúÎâ¥Ïä§",
                    "www.korea.kr": "Ï†ïÏ±ÖÎ∏åÎ¶¨Ìïë", "korea.kr": "Ï†ïÏ±ÖÎ∏åÎ¶¨Ìïë",
                }
                if dom in PRESS_MAP:
                    press = PRESS_MAP[dom]

                norm_key = make_norm_key(origin, link, title)
                if norm_key in seen_keys:
                    continue

                art = Article(
                    section=section_conf["key"],
                    title=title,
                    description=desc,
                    link=link,
                    originallink=origin,
                    pub_dt_kst=pub,
                    domain=dom,
                    press=press,
                    norm_key=norm_key,
                )

                if not is_relevant(art, section_conf):
                    continue

                art.score = compute_rank_score(art, section_conf)
                seen_keys.add(norm_key)
                items.append(art)

        except Exception as e:
            log.warning("[WARN] query failed: %s (%s)", q, e)

    items.sort(key=lambda a: (a.score, a.pub_dt_kst), reverse=True)
    return items[:MAX_PER_SECTION]

def collect_all_sections(start_kst: datetime, end_kst: datetime):
    by_section: dict[str, list[Article]] = {}
    for sec in SECTIONS:
        by_section[sec["key"]] = collect_articles_for_section(sec, start_kst, end_kst)

    # light broad fill if too few
    for sec in SECTIONS:
        key = sec["key"]
        if len(by_section[key]) >= MIN_PER_SECTION:
            continue

        if key == "supply":
            broad_queries = ["ÎÜçÏÇ∞Î¨º Í∞ÄÍ≤©", "Í≥ºÏùº ÎèÑÎß§ÏãúÏû• ÏãúÏÑ∏", "Ï≤≠Í≥º Í≤ΩÎùΩÍ∞Ä", "ÏÇ∞ÏßÄ Ï∂úÌïò Î¨ºÎüâ"]
        elif key == "policy":
            broad_queries = ["ÎÜçÏ∂ïÏàòÏÇ∞Î¨º Ìï†Ïù∏ ÏßÄÏõê", "Ìï†ÎãπÍ¥ÄÏÑ∏ Í≥ºÏùº", "ÎÜçÏÇ∞Î¨º Î¨ºÍ∞Ä ÎåÄÏ±Ö"]
        elif key == "pest":
            broad_queries = ["Í≥ºÏàò Î≥ëÌï¥Ï∂© Î∞©Ï†ú ÏïΩÏ†ú", "Í≥ºÏàòÌôîÏÉÅÎ≥ë Î∞©Ï†ú", "ÏõîÎèô Ìï¥Ï∂© Î∞©Ï†ú"]
        else:
            broad_queries = ["APC ÏÑ†Î≥Ñ Ï†ÄÏû•", "ÎÜçÏãùÌíà ÏàòÏ∂ú Ïã§Ï†Å", "Í∞ÄÎùΩÏãúÏû• Í≤ΩÎß§ ÏùºÏ†ï"]

        tmp = dict(sec)
        tmp["queries"] = broad_queries

        extra = collect_articles_for_section(tmp, start_kst, end_kst)
        merged = {a.norm_key: a for a in by_section[key]}
        for a in extra:
            merged.setdefault(a.norm_key, a)

        merged_list = list(merged.values())
        merged_list.sort(key=lambda a: (a.score, a.pub_dt_kst), reverse=True)
        by_section[key] = merged_list[:MAX_PER_SECTION]

    return by_section


# -----------------------------
# OpenAI summaries (batch) - optional
# -----------------------------
def openai_extract_text(resp_json: dict) -> str:
    try:
        out = resp_json.get("output", [])
        if not out:
            return ""
        for block in out:
            for c in block.get("content", []):
                if c.get("type") in ("output_text", "text") and "text" in c:
                    return c["text"]
        return ""
    except Exception:
        return ""

def openai_summarize_batch(articles: list[Article]) -> dict:
    if not OPENAI_API_KEY or not articles:
        return {}

    rows = []
    for a in articles:
        rows.append({
            "id": a.norm_key,
            "press": a.press,
            "title": a.title[:180],
            "desc": a.description[:260],
            "section": a.section,
            "url": a.originallink or a.link,
        })

    system = (
        "ÎÑàÎäî ÎÜçÌòë Í≤ΩÏ†úÏßÄÏ£º ÏõêÏòàÏàòÍ∏âÎ∂Ä(Í≥ºÏàòÌôîÌõº) Ïã§Î¨¥ÏûêÎ•º ÏúÑÌïú 'ÎÜçÏÇ∞Î¨º Îâ¥Ïä§ ÏöîÏïΩÍ∞Ä'Îã§.\n"
        "- Ï†àÎåÄ ÏÉÅÏÉÅ/Ï∂îÏ†ïÏúºÎ°ú ÏÇ¨Ïã§ÏùÑ ÎßåÎì§ÏßÄ ÎßàÎùº.\n"
        "- Í∞Å Í∏∞ÏÇ¨ ÏöîÏïΩÏùÄ 'ÏóÖÎ¨¥Ï†ÅÏúºÎ°ú Ïì∏Î™® ÏûàÎäî Ìå©Ìä∏' ÏúÑÏ£ºÎ°ú 2~3Î¨∏Ïû•(Ï§ÑÎ∞îÍøà Ìè¨Ìï® Í∞ÄÎä•), 120~220Ïûê ÎÇ¥.\n"
        "- Í∞ÄÍ≤©/ÏàòÍ∏â/Ï†ïÏ±Ö/Î∞©Ï†ú/Ïú†ÌÜµ Ìè¨Ïù∏Ìä∏Î•º Îπ†Î•¥Í≤å ÌååÏïÖÎêòÍ≤å Ïç®Îùº.\n"
        "Ï∂úÎ†• ÌòïÏãù(Î∞òÎìúÏãú): Í∞Å Ï§ÑÏóê 'id\\tÏöîÏïΩ' ÌòïÌÉúÎ°úÎßå Ï∂úÎ†•."
    )
    user = "Í∏∞ÏÇ¨ Î™©Î°ù(JSON):\n" + json.dumps(rows, ensure_ascii=False)

    payload = {
        "model": OPENAI_MODEL,
        "input": [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
    }

    try:
        r = requests.post(
            "https://api.openai.com/v1/responses",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"},
            json=payload,
            timeout=60,
        )
        if not r.ok:
            # quota/429/invalid Îì±: ÏöîÏïΩÎßå Ìè¨Í∏∞ÌïòÍ≥† Ìè¥Î∞±
            try:
                body = r.json()
            except Exception:
                body = {"raw": r.text}
            msg = (body.get("error") or {}).get("message") or r.text
            code = (body.get("error") or {}).get("code") or str(r.status_code)
            log.warning("[OpenAI] summarize skipped (%s): %s", code, msg)
            return {}

        text = openai_extract_text(r.json()).strip()
        out = {}
        for line in text.splitlines():
            if "\t" not in line:
                continue
            k, v = line.split("\t", 1)
            k = k.strip()
            v = v.strip()
            if k:
                out[k] = v
        return out

    except Exception as e:
        log.warning("[OpenAI] summarize failed, fallback: %s", e)
        return {}

def fill_summaries(by_section: dict):
    all_articles: list[Article] = []
    for sec in SECTIONS:
        all_articles.extend(by_section.get(sec["key"], []))

    mapping = openai_summarize_batch(all_articles)

    for a in all_articles:
        s = mapping.get(a.norm_key, "").strip()
        if not s:
            s = a.description.strip() or a.title.strip()
        a.summary = s

    return by_section


# -----------------------------
# Rendering (HTML)
# -----------------------------
def esc(s: str) -> str:
    return html.escape(s or "")

def fmt_dt(dt_: datetime) -> str:
    return dt_.strftime("%m/%d %H:%M")

def render_daily_page(report_date: str, start_kst: datetime, end_kst: datetime, by_section: dict, base_url: str) -> str:
    chips = []
    total = 0
    for sec in SECTIONS:
        n = len(by_section.get(sec["key"], []))
        total += n
        chips.append((sec["key"], sec["title"], n, sec["color"]))

    def chip_html(k, title, n, color):
        return (
            f'<a class="chip" style="border-color:{color};color:{color}" href="#sec-{k}">'
            f'{esc(title)} <span class="chipN">{n}</span></a>'
        )

    chips_html = "\n".join([chip_html(*c) for c in chips])

    sections_html = []
    for sec in SECTIONS:
        key = sec["key"]
        title = sec["title"]
        color = sec["color"]
        lst = by_section.get(key, [])
        cards = []
        for a in lst:
            url = a.originallink or a.link
            summary_html = "<br>".join(esc(a.summary).splitlines())
            cards.append(
                f"""
                <div class="card" style="border-left-color:{color}">
                  <div class="meta">
                    <span class="press">{esc(a.press)}</span>
                    <span class="dot">¬∑</span>
                    <span class="time">{esc(fmt_dt(a.pub_dt_kst))}</span>
                  </div>
                  <div class="ttl">{esc(a.title)}</div>
                  <div class="sum">{summary_html}</div>
                  <div class="lnk"><a href="{esc(url)}" target="_blank" rel="noopener">ÏõêÎ¨∏ Ïó¥Í∏∞</a></div>
                </div>
                """
            )

        cards_html = '<div class="empty">ÌäπÏù¥ÏÇ¨Ìï≠ ÏóÜÏùå</div>' if not cards else "\n".join(cards)

        sections_html.append(
            f"""
            <section id="sec-{key}" class="sec">
              <div class="secHead" style="background:linear-gradient(90deg,{color},#111827);">
                <div class="secTitle">{esc(title)}</div>
                <div class="secCount">{len(lst)}Í±¥</div>
              </div>
              <div class="secBody">
                {cards_html}
              </div>
            </section>
            """
        )

    sections_html = "\n".join(sections_html)

    title = f"[{report_date} ÎÜçÏÇ∞Î¨º Îâ¥Ïä§ Brief]"
    period = f"{start_kst.strftime('%Y-%m-%d %H:%M')} ~ {end_kst.strftime('%Y-%m-%d %H:%M')}"
    index_url = f"{base_url}/"

    return f"""<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>{esc(title)}</title>
  <style>
    :root {{
      --bg:#0b1220; --text:#e5e7eb; --muted:#94a3b8; --line:#1f2937;
    }}
    *{{box-sizing:border-box}}
    body{{margin:0;background:radial-gradient(1200px 600px at 20% 10%, #111827, var(--bg)); color:var(--text);
         font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Noto Sans KR", Arial;}}
    .wrap{{max-width:1100px;margin:0 auto;padding:22px 16px 80px;}}
    .topbar{{display:flex;gap:10px;align-items:center;justify-content:space-between;flex-wrap:wrap}}
    h1{{margin:0;font-size:20px;letter-spacing:-0.2px}}
    .sub{{color:var(--muted);font-size:13px;margin-top:6px}}
    .nav a{{color:#cbd5e1;text-decoration:none;font-size:13px;border:1px solid var(--line);padding:8px 10px;border-radius:10px;background:rgba(255,255,255,0.02)}}
    .chips{{display:flex;gap:8px;flex-wrap:wrap;margin-top:14px}}
    .chip{{text-decoration:none;border:1px solid var(--line);padding:8px 10px;border-radius:999px;
          background:rgba(255,255,255,0.02);font-size:13px}}
    .chipN{{margin-left:6px;background:rgba(255,255,255,0.08);padding:2px 8px;border-radius:999px;color:var(--text)}}
    .sec{{margin-top:18px;border:1px solid var(--line);border-radius:16px;overflow:hidden;background:rgba(255,255,255,0.02)}}
    .secHead{{display:flex;align-items:center;justify-content:space-between;padding:12px 14px}}
    .secTitle{{font-size:15px;font-weight:700}}
    .secCount{{font-size:13px;color:#e2e8f0;background:rgba(0,0,0,0.25);padding:4px 10px;border-radius:999px}}
    .secBody{{padding:12px 12px 14px}}
    .card{{background:rgba(15,23,42,0.55);border:1px solid var(--line);border-left:4px solid #334155;
          border-radius:14px;padding:12px 12px 12px;margin:10px 0}}
    .meta{{color:var(--muted);font-size:12px;display:flex;align-items:center;gap:6px}}
    .press{{color:#e2e8f0}}
    .dot{{opacity:.6}}
    .ttl{{margin-top:6px;font-size:15px;line-height:1.35}}
    .sum{{margin-top:8px;color:#cbd5e1;font-size:13px;line-height:1.55}}
    .lnk{{margin-top:10px}}
    .lnk a{{display:inline-block;color:#e5e7eb;text-decoration:none;border:1px solid var(--line);
           padding:8px 10px;border-radius:10px;background:rgba(255,255,255,0.03)}}
    .empty{{color:var(--muted);font-size:13px;padding:10px 2px}}
    .footer{{margin-top:22px;color:var(--muted);font-size:12px}}
  </style>
</head>
<body>
  <div class="wrap">
    <div class="topbar">
      <div>
        <h1>{esc(title)}</h1>
        <div class="sub">Í∏∞Í∞Ñ: {esc(period)} ¬∑ Í∏∞ÏÇ¨ {total}Í±¥</div>
      </div>
      <div class="nav">
        <a href="{esc(index_url)}">ÏµúÏã†/ÏïÑÏπ¥Ïù¥Î∏å</a>
      </div>
    </div>

    <div class="chips">
      {chips_html}
    </div>

    {sections_html}

    <div class="footer">
      * ÏûêÎèô ÏàòÏßë Í≤∞Í≥ºÏù¥Î©∞, Ï†úÎ™©/ÏöîÏïΩÏùÄ ÏõêÎ¨∏ Í∏∞Î∞ò Ï†ïÎ¶¨ÏûÖÎãàÎã§. (ÌïÑÏöî Ïãú ÏõêÎ¨∏ ÌôïÏù∏)
    </div>
  </div>
</body>
</html>
"""

def render_index_page(manifest: dict, base_url: str) -> str:
    manifest = _normalize_manifest(manifest)
    dates = sorted(manifest.get("dates", []), reverse=True)
    latest = dates[0] if dates else None

    items_html = []
    for d in dates[:30]:
        url = f"{base_url}/archive/{d}.html"
        items_html.append(f'<li><a href="{esc(url)}">{esc(d)}</a></li>')
    ul = "\n".join(items_html) if items_html else "<li>ÏïÑÏπ¥Ïù¥Î∏åÍ∞Ä ÏïÑÏßÅ ÏóÜÏäµÎãàÎã§.</li>"

    latest_link = f"{base_url}/archive/{latest}.html" if latest else base_url

    return f"""<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ÎÜçÏÇ∞Î¨º Îâ¥Ïä§ Î∏åÎ¶¨Ìïë</title>
  <style>
    body{{margin:0;background:#0b1220;color:#e5e7eb;font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Noto Sans KR", Arial;}}
    .wrap{{max-width:900px;margin:0 auto;padding:24px 16px 60px}}
    h1{{margin:0;font-size:22px}}
    .sub{{color:#94a3b8;margin-top:8px;font-size:13px}}
    .btn{{display:inline-block;margin-top:14px;text-decoration:none;color:#e5e7eb;border:1px solid #1f2937;
         padding:10px 12px;border-radius:12px;background:rgba(255,255,255,0.03)}}
    .panel{{margin-top:18px;border:1px solid #1f2937;border-radius:16px;background:rgba(255,255,255,0.02);padding:14px}}
    ul{{margin:10px 0 0 18px}}
    a{{color:#cbd5e1}}
  </style>
</head>
<body>
  <div class="wrap">
    <h1>ÎÜçÏÇ∞Î¨º Îâ¥Ïä§ Î∏åÎ¶¨Ìïë</h1>
    <div class="sub">ÏµúÏã† Î∏åÎ¶¨ÌïëÍ≥º ÎÇ†ÏßúÎ≥Ñ ÏïÑÏπ¥Ïù¥Î∏åÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.</div>

    <a class="btn" href="{esc(latest_link)}">ÏµúÏã† Î∏åÎ¶¨Ìïë Ïó¥Í∏∞</a>

    <div class="panel">
      <div style="font-weight:700;margin-bottom:6px;">ÎÇ†ÏßúÎ≥Ñ ÏïÑÏπ¥Ïù¥Î∏å</div>
      <ul>
        {ul}
      </ul>
    </div>
  </div>
</body>
</html>
"""


# -----------------------------
# Pages URL (anti-gist safeguard)
# -----------------------------
def get_pages_base_url(repo: str) -> str:
    """
    - If PAGES_BASE_URL is set and valid: use it
    - If it's mistakenly set to gist/raw or invalid: ignore and fallback to default GitHub Pages URL
    """
    owner, name = repo.split("/", 1)
    default_url = f"https://{owner.lower()}.github.io/{name}".rstrip("/")

    env_url = os.getenv("PAGES_BASE_URL", "").strip().rstrip("/")
    if not env_url:
        return default_url

    bad = ("gist.github.com", "raw.githubusercontent.com")
    if any(b in env_url for b in bad):
        log.warning("[WARN] PAGES_BASE_URL points to gist/raw. Ignoring and using default: %s", default_url)
        return default_url

    if not env_url.startswith("http://") and not env_url.startswith("https://"):
        log.warning("[WARN] PAGES_BASE_URL is invalid (no http/https). Ignoring and using default: %s", default_url)
        return default_url

    return env_url


# -----------------------------
# Press tier (central/local) + Kakao message builder
# -----------------------------
CENTRAL_PRESS_NAMES = {
    "Ïó∞Ìï©Îâ¥Ïä§", "Îß§ÏùºÍ≤ΩÏ†ú", "Ï§ëÏïôÏùºÎ≥¥", "Ï°∞ÏÑ†ÏùºÎ≥¥", "ÎèôÏïÑÏùºÎ≥¥", "ÌïúÍ≤®Î†à", "Í≤ΩÌñ•Ïã†Î¨∏",
    "ÏÑúÏö∏Í≤ΩÏ†ú", "ÌïúÍµ≠Í≤ΩÏ†ú", "ÏïÑÏãúÏïÑÍ≤ΩÏ†ú", "Î®∏ÎãàÌà¨Îç∞Ïù¥", "Ìó§Îü¥ÎìúÍ≤ΩÏ†ú", "Ïù¥Îç∞ÏùºÎ¶¨",
    "Îâ¥ÏãúÏä§", "Îâ¥Ïä§1", "ÌååÏù¥ÎÇ∏ÏÖúÎâ¥Ïä§", "Ï†ïÏ±ÖÎ∏åÎ¶¨Ìïë",
    "SBS", "KBS", "MBC", "YTN", "JTBC",
}

LOCAL_PRESS_HINTS = (
    "Í∞ïÏõê", "Í≤ΩÍ∏∞", "Ïù∏Ï≤ú", "ÎåÄÏ†Ñ", "Ï∂©Ï≤≠", "Ï∂©Î∂Å", "Ï∂©ÎÇ®", "Ï†ÑÎ∂Å", "Ï†ÑÎÇ®",
    "Í¥ëÏ£º", "ÎåÄÍµ¨", "Í≤ΩÎ∂Å", "Î∂ÄÏÇ∞", "Ïö∏ÏÇ∞", "Í≤ΩÎÇ®", "Ï†úÏ£º", "ÏÑ∏Ï¢Ö",
)

def press_tier(press: str, domain: str) -> str:
    """
    Returns 'central' or 'local'
    - unknown defaults to central (so Ï§ëÏïô+ÏßÄÎ∞© Ìï©Ïù¥ Ï¥ù Í∏∞ÏÇ¨ÏàòÏôÄ ÏùºÏπòÌïòÎèÑÎ°ù Î≥¥Ïû•)
    """
    p = (press or "").strip()
    d = (domain or "").lower()

    if p in CENTRAL_PRESS_NAMES:
        return "central"

    if d.endswith("korea.kr") or d == "korea.kr" or "mafra.go.kr" in d or "at.or.kr" in d:
        return "central"

    if any(h in p for h in LOCAL_PRESS_HINTS) and any(x in p for x in ("ÏùºÎ≥¥", "Ïã†Î¨∏", "Î∞©ÏÜ°", "Îâ¥Ïä§")):
        return "local"

    return "central"


# Ïπ¥ÌÜ° Î©îÏãúÏßÄ ÏÑπÏÖò ÏàúÏÑú(ÏöîÏ≤≠ Í≥†Ï†ï)
KAKAO_MESSAGE_SECTION_ORDER = ["supply", "policy", "dist", "pest"]

def _get_section_conf(key: str):
    for s in SECTIONS:
        if s["key"] == key:
            return s
    return None

def build_kakao_message(report_date: str, by_section: dict) -> str:
    """
    Generates Kakao text with exact spacing/line breaks requested by user.
    """
    total = 0
    per = {}
    central = 0
    local = 0

    for sec in SECTIONS:
        key = sec["key"]
        lst = by_section.get(key, [])
        per[key] = len(lst)
        total += len(lst)
        for a in lst:
            t = press_tier(a.press, a.domain)
            if t == "local":
                local += 1
            else:
                central += 1

    # --- EXACT FORMAT START ---
    lines = []
    lines.append(f"[{report_date} ÎÜçÏÇ∞Î¨º Îâ¥Ïä§ Brief]")
    lines.append("")
    lines.append("")
    lines.append(f"Í∏∞ÏÇ¨ : Ï¥ù {total}Í±¥ (Ï§ëÏïô {central}Í±¥, ÏßÄÎ∞© {local}Í±¥)")
    lines.append("")
    lines.append(f"- ÌíàÎ™© {per.get('supply',0)} ¬∑ Ï†ïÏ±Ö {per.get('policy',0)} ¬∑ Î∞©Ï†ú {per.get('pest',0)} ¬∑ Ïú†ÌÜµ {per.get('dist',0)}")
    lines.append("")
    lines.append("")
    lines.append("Ïò§ÎäòÏùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏")
    lines.append("")
    lines.append("")

    section_num = 0
    for key in KAKAO_MESSAGE_SECTION_ORDER:
        conf = _get_section_conf(key)
        if not conf:
            continue
        section_num += 1

        lines.append(f"{section_num}) {conf['title']}")
        lines.append("")
        lines.append("")

        items = by_section.get(key, [])[:2]
        if not items:
            lines.append("   - (Í∏∞ÏÇ¨ ÏóÜÏùå)")
            lines.append("")
            lines.append("")
            continue

        for a in items:
            lines.append(f"   - ({a.press}) {a.title}")
            lines.append("")
            lines.append("")

    lines.append("üëâ 'Î∏åÎ¶¨Ìïë Ïó¥Í∏∞'ÏóêÏÑú ÏÑπÏÖòÎ≥Ñ Í∏∞ÏÇ¨Î•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.")
    # --- EXACT FORMAT END ---

    return "\n".join(lines)


# -----------------------------
# Kakao API
# -----------------------------
def kakao_refresh_access_token() -> str:
    if not KAKAO_REST_API_KEY or not KAKAO_REFRESH_TOKEN:
        raise RuntimeError("KAKAO_REST_API_KEY / KAKAO_REFRESH_TOKEN not set")

    url = "https://kauth.kakao.com/oauth/token"
    data = {
        "grant_type": "refresh_token",
        "client_id": KAKAO_REST_API_KEY,
        "refresh_token": KAKAO_REFRESH_TOKEN,
    }
    if KAKAO_CLIENT_SECRET:
        data["client_secret"] = KAKAO_CLIENT_SECRET

    r = requests.post(url, data=data, timeout=30)
    if not r.ok:
        log.error("[KAKAO TOKEN ERROR] %s", r.text)
        r.raise_for_status()
    j = r.json()
    return j["access_token"]

def kakao_send_to_me(text: str, web_url: str):
    access_token = kakao_refresh_access_token()

    # Safety: never allow gist/raw
    if "gist.github.com" in web_url or "raw.githubusercontent.com" in web_url:
        raise RuntimeError(f"[FATAL] Kakao web_url points to gist/raw: {web_url}")

    url = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
    headers = {"Authorization": f"Bearer {access_token}"}

    template = {
        "object_type": "text",
        "text": text,
        "link": {"web_url": web_url, "mobile_web_url": web_url},
        "button_title": "Î∏åÎ¶¨Ìïë Ïó¥Í∏∞",
    }

    r = requests.post(url, headers=headers, data={"template_object": json.dumps(template, ensure_ascii=False)}, timeout=30)
    if not r.ok:
        log.error("[KAKAO SEND ERROR] %s", r.text)
        r.raise_for_status()
    return r.json()


# -----------------------------
# Window calculation
# -----------------------------
def compute_end_kst():
    if FORCE_REPORT_DATE:
        d = datetime.strptime(FORCE_REPORT_DATE, "%Y-%m-%d").date()
        return dt_kst(d, REPORT_HOUR_KST)

    if FORCE_END_NOW:
        return now_kst()

    n = now_kst()
    candidate = n.replace(hour=REPORT_HOUR_KST, minute=0, second=0, microsecond=0)
    if n < candidate:
        candidate -= timedelta(days=1)
    return candidate

def compute_window(repo: str, token: str, end_kst: datetime):
    state = load_state(repo, token)
    last_end_iso = state.get("last_end_iso")

    prev_bd = previous_business_day(end_kst.date())
    prev_cutoff = dt_kst(prev_bd, REPORT_HOUR_KST)

    start = prev_cutoff
    if last_end_iso:
        try:
            st = datetime.fromisoformat(last_end_iso)
            if st.tzinfo is None:
                st = st.replace(tzinfo=KST)
            start = min(st.astimezone(KST), prev_cutoff)
        except Exception:
            start = prev_cutoff

    if start >= end_kst:
        start = end_kst - timedelta(hours=24)

    return start, end_kst


# -----------------------------
# Main
# -----------------------------
def main():
    if not DEFAULT_REPO:
        raise RuntimeError("GITHUB_REPO or GITHUB_REPOSITORY is not set (e.g., ORGNAME/agri-news-brief)")
    if not GH_TOKEN:
        raise RuntimeError("GH_TOKEN or GITHUB_TOKEN is not set")
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        raise RuntimeError("NAVER_CLIENT_ID / NAVER_CLIENT_SECRET is not set")
    if not KAKAO_REST_API_KEY or not KAKAO_REFRESH_TOKEN:
        raise RuntimeError("KAKAO_REST_API_KEY / KAKAO_REFRESH_TOKEN is not set")

    repo = DEFAULT_REPO
    end_kst = compute_end_kst()

    # Skip if not business day (unless forced)
    is_bd = is_business_day_kr(end_kst.date())
    if (not FORCE_RUN_ANYDAY) and (not is_bd):
        log.info("[SKIP] Not a business day in KR: %s (weekend/holiday)", end_kst.date().isoformat())
        return
    if FORCE_RUN_ANYDAY and (not is_bd):
        log.info("[FORCE] Non-business day but proceeding for test: %s", end_kst.date().isoformat())

    start_kst, end_kst = compute_window(repo, GH_TOKEN, end_kst)
    log.info("[INFO] Window KST: %s ~ %s", start_kst.isoformat(), end_kst.isoformat())

    report_date = end_kst.date().isoformat()

    base_url = get_pages_base_url(repo)
    base_url = base_url.rstrip("/")
    daily_url = f"{base_url}/archive/{report_date}.html"

    # Safety: never allow gist/raw
    if "gist.github.com" in daily_url or "raw.githubusercontent.com" in daily_url:
        raise RuntimeError(f"[FATAL] daily_url is wrong (gist/raw): {daily_url}")

    # Collect + summarize
    by_section = collect_all_sections(start_kst, end_kst)
    by_section = fill_summaries(by_section)

    # Render pages
    daily_html = render_daily_page(report_date, start_kst, end_kst, by_section, base_url)

    manifest, msha = load_archive_manifest(repo, GH_TOKEN)
    manifest = _normalize_manifest(manifest)
    dates = set(manifest.get("dates", []))
    dates.add(report_date)
    manifest["dates"] = sorted(list(dates))

    index_html = render_index_page(manifest, base_url)

    # Write daily page
    daily_path = f"{DOCS_ARCHIVE_DIR}/{report_date}.html"
    _raw_old, sha_old = github_get_file(repo, daily_path, GH_TOKEN, ref="main")
    github_put_file(repo, daily_path, daily_html, GH_TOKEN, f"Add daily brief {report_date}", sha=sha_old, branch="main")

    # Write index
    _raw_old2, sha_old2 = github_get_file(repo, DOCS_INDEX_PATH, GH_TOKEN, ref="main")
    github_put_file(repo, DOCS_INDEX_PATH, index_html, GH_TOKEN, f"Update index {report_date}", sha=sha_old2, branch="main")

    # Save manifest/state
    save_archive_manifest(repo, GH_TOKEN, manifest, msha)
    save_state(repo, GH_TOKEN, end_kst)

    # Kakao message (exact format)
    kakao_text = build_kakao_message(report_date, by_section)

    # Optional: include URL in text (default false)
    if KAKAO_INCLUDE_LINK_IN_TEXT:
        kakao_text = kakao_text + "\n\n" + daily_url

    kakao_send_to_me(kakao_text, daily_url)
    log.info("[OK] Kakao message sent. URL=%s", daily_url)


if __name__ == "__main__":
    main()
