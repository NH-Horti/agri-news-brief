# main.py
# -*- coding: utf-8 -*-

from __future__ import annotations

import os, re, json, time, base64, html, logging
from dataclasses import dataclass
from datetime import datetime, timedelta, date
from email.utils import parsedate_to_datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse

import requests
from zoneinfo import ZoneInfo

KST = ZoneInfo("Asia/Seoul")
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# =========================
# ìš´ì˜ íŒŒë¼ë¯¸í„°
# =========================
RUN_HOUR_KST = int(os.getenv("RUN_HOUR_KST", "7"))
EARLY_GRACE_MINUTES = int(os.getenv("EARLY_GRACE_MINUTES", "20"))  # 07:00 ì§ì „ ì‹¤í–‰ ë³´ì •
FORCE_SEND = (os.getenv("FORCE_SEND", "0") == "1")

MAX_ARTICLES_PER_SECTION = int(os.getenv("MAX_ARTICLES_PER_SECTION", "10"))
MIN_ARTICLES_PER_SECTION = int(os.getenv("MIN_ARTICLES_PER_SECTION", "7"))
GLOBAL_BACKFILL_LIMIT = int(os.getenv("GLOBAL_BACKFILL_LIMIT", "120"))
MAX_PAGES_PER_QUERY = int(os.getenv("MAX_PAGES_PER_QUERY", "3"))  # ë„¤ì´ë²„ API í˜ì´ì§€(50ê°œì”©)

PUBLISH_MODE = os.getenv("PUBLISH_MODE", "github_pages")
PAGES_BRANCH = os.getenv("PAGES_BRANCH", "main")
PAGES_FILE_PATH = os.getenv("PAGES_FILE_PATH", "docs/index.html")

STATE_BACKEND = os.getenv("STATE_BACKEND", "repo")
STATE_FILE_PATH = os.getenv("STATE_FILE_PATH", ".agri_state.json")

BRIEF_VIEW_URL = os.getenv("BRIEF_VIEW_URL", "").strip()
KAKAO_MESSAGE_SOFT_LIMIT = int(os.getenv("KAKAO_MESSAGE_SOFT_LIMIT", "260"))

# =========================
# Kakao
# =========================
KAKAO_TOKEN_URL = "https://kauth.kakao.com/oauth/token"
KAKAO_MEMO_SEND_API = "https://kapi.kakao.com/v2/api/talk/memo/default/send"

# =========================
# Naver OpenAPI
# =========================
NAVER_NEWS_API = "https://openapi.naver.com/v1/search/news.json"

# =========================
# OpenAI
# =========================
OPENAI_RESPONSES_URL = "https://api.openai.com/v1/responses"
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-5.2")
OPENAI_REASONING_EFFORT = os.getenv("OPENAI_REASONING_EFFORT", "minimal")
OPENAI_MAX_OUTPUT_TOKENS = int(os.getenv("OPENAI_MAX_OUTPUT_TOKENS", "2000"))

# =========================
# ì„¹ì…˜(ìˆœì„œ ê³ ì •)
# =========================
SECTION_ORDER: List[str] = [
    "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥",
    "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…",
    "ë³‘í•´ì¶© ë° ë°©ì œ",
    "ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)",
]

# =========================
# 1) í‚¤ì›Œë“œ ì „ë©´ ì¬ì¡°ì • (ì›ì˜ˆìˆ˜ê¸‰ë¶€/ê³¼ìˆ˜í™”í›¼íŒ€ ê´€ì )
# =========================
FRUITS = [
    "ì‚¬ê³¼","ë°°","ê°ê·¤","ë§Œê°ë¥˜","í•œë¼ë´‰","ë ˆë“œí–¥","ì²œí˜œí–¥","ì°¸ë‹¤ë˜","í‚¤ìœ„",
    "í¬ë„","ìƒ¤ì¸ë¨¸ìŠ¤ìº£","ë³µìˆ­ì•„","ìë‘","ë§¤ì‹¤","ìœ ì","ë°¤",
    "ë‹¨ê°","ë–«ì€ê°","ê³¶ê°","ê°",
]
VEGGIES = [
    "ë”¸ê¸°","ì˜¤ì´","í’‹ê³ ì¶”","ì• í˜¸ë°•","í† ë§ˆí† ","íŒŒí”„ë¦¬ì¹´","ê°€ì§€","ìƒì¶”","ê¹»ì",
    "ë°°ì¶”","ë¬´","ì–‘íŒŒ","ëŒ€íŒŒ","ë§ˆëŠ˜","ê°ì","ê³ êµ¬ë§ˆ",
]
FLOWERS = ["ì ˆí™”","í™”í›¼","ê½ƒê°’","êµ­í™”","ì¥ë¯¸","ë°±í•©","í”„ë¦¬ì§€ì•„"]
STAPLES = ["ìŒ€","ì‚°ì§€ìŒ€ê°’","ë¹„ì¶•ë¯¸"]

# ì›ì˜ˆìˆ˜ê¸‰ë¶€ì—ì„œ â€œê¸°ì‚¬ ë¹ ì§â€ì´ ëœ ë‚˜ì˜¤ëŠ” ë§¥ë½ ë‹¨ì–´(í’ˆëª© ë‹¨ë…ê²€ìƒ‰ì˜ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ê¸°)
AGRI_CONTEXT = ["ë†ì‚°ë¬¼","ì›ì˜ˆ","ê³¼ìˆ˜","ê³¼ì¼","ì²­ê³¼","ì‚°ì§€","ë†ê°€","ë„ë§¤","ê²½ë§¤","ì¶œí•˜","ì €ì¥","ìˆ˜ê¸‰","ì‘í™©"]

# ëª¨ë””íŒŒì´ì–´(=ê°€ê²©ë§Œ ë¶™ì´ë©´ ë¹ ì§ -> ê°€ê²©/ì‹œì„¸ëŠ” â€œë³´ì¡°â€ë¡œ)
SUPPLY_MODS = ["ìˆ˜ê¸‰","ì¶œí•˜","ì €ì¥","ì¬ê³ ","ì‘í™©","ìƒì‚°","ë¬¼ëŸ‰","ë„ë§¤","ê²½ë§¤","ê²½ë½","ì‹œì„¸","ê°€ê²©"]

# êµ¬ì¡°/ê¸°í›„/ì‚°ì§€ì´ë™(ê³¼ìˆ˜í™”í›¼íŒ€ ê´€ì  ì¤‘ìš”)
STRUCTURAL_QUERIES = [
    "ê¸°í›„ë³€í™” ê³¼ìˆ˜ ì¬ë°°ì§€ ë¶ìƒ",
    "ì‚¬ê³¼ ì¬ë°°ì§€ ë¶ìƒ ê°•ì›",
    "ê³¼ìˆ˜ ë™í•´ ëƒ‰í•´ í”¼í•´",
    "ì¼ì¡°ëŸ‰ ë¶€ì¡± ì‹œì„¤ì›ì˜ˆ",
    "ê³ ì˜¨ ê°€ë­„ ê³¼ìˆ˜ ì‘í™©",
]

POLICY_CORE = [
    "ë†ì‚°ë¬¼ ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥", "ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥", "ë„ë§¤ì‹œì¥ ì œë„",
    "ê°€ë½ì‹œì¥ íœ´ë¬´", "ê°€ë½ì‹œì¥ ê²½ë§¤ ì¬ê°œ", "ê³µì˜ë„ë§¤ì‹œì¥",
    "ë†ì‚°ë¬¼ ë¬¼ê°€ ëŒ€ì±…", "ë†ì¶•ì‚°ë¬¼ í• ì¸", "í• ì¸ì§€ì›",
    "í• ë‹¹ê´€ì„¸", "ìˆ˜ì… ê³¼ì¼", "ê²€ì—­ ì™„í™”", "ì‹œì¥ê°œë°©",
    "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€ ë†ì‚°ë¬¼", "ì •ì±…ë¸Œë¦¬í•‘ ë†ì‚°ë¬¼",
]

Pest_CORE = [
    "ê³¼ìˆ˜í™”ìƒë³‘", "í™”ìƒë³‘ ì•½ì œ", "í™”ìƒë³‘ ë°©ì œ", "ê¶¤ì–‘ ì œê±°",
    "ì›”ë™í•´ì¶© ë°©ì œ", "ê¸°ê³„ìœ ìœ ì œ", "íƒ„ì €ë³‘", "ë³‘í•´ì¶© ì˜ˆì°°",
    "ë™í•´ ëŒ€ë¹„", "ëƒ‰í•´ ëŒ€ë¹„", "ì„œë¦¬ í”¼í•´",
]

DIST_CORE = [
    "APC", "ì‚°ì§€ìœ í†µì„¼í„°", "ìŠ¤ë§ˆíŠ¸ APC", "AI ì„ ë³„", "ì„ ë³„ê¸°", "CA ì €ì¥", "ì €ì¥ì‹œì„¤",
    "ê³µíŒì¥", "ë„ë§¤ì‹œì¥ ìœ í†µ", "ì‚°ì§€ìœ í†µ", "ì½œë“œì²´ì¸",
    "ë†ì‹í’ˆ ìˆ˜ì¶œ", "ë†ì‚°ë¬¼ ìˆ˜ì¶œ", "ë”¸ê¸° ìˆ˜ì¶œ", "ë°° ìˆ˜ì¶œ",
    "K-Food ìˆ˜ì¶œ", "aT ìˆ˜ì¶œ",
]

def uniq_keep_order(xs: List[str]) -> List[str]:
    seen = set()
    out = []
    for x in xs:
        x = re.sub(r"\s+", " ", (x or "").strip())
        if not x:
            continue
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out

def build_supply_queries() -> List[str]:
    """
    âœ… í•µì‹¬: 'ì‚¬ê³¼ ê°€ê²©' ê°™ì€ AND ì¡°í•©ë§Œ ì“°ì§€ ë§ê³ ,
    (1) í’ˆëª© + ë†ì—…ë§¥ë½(ë†ì‚°ë¬¼/ê³¼ìˆ˜/ì‚°ì§€/ë„ë§¤/ì¶œí•˜/ì €ì¥)ë¶€í„° í­ë„“ê²Œ ìˆ˜ì§‘
    (2) ê·¸ ë‹¤ìŒ ê°€ê²©/ì‹œì„¸/ê²½ë½ ë“± ë³´ì¡° í‚¤ì›Œë“œ ì¡°í•©
    """
    qs: List[str] = []

    # 0) êµ¬ì¡°/ê¸°í›„ ì´ìŠˆëŠ” ì„ í–‰ ìˆ˜ì§‘(ê³¼ìˆ˜í™”í›¼íŒ€ í•µì‹¬)
    qs += STRUCTURAL_QUERIES

    # 1) í’ˆëª©(ê³¼ìˆ˜/ì±„ì†Œ/í™”í›¼/ìŒ€) + ë†ì—…ë§¥ë½
    def add_item(item: str):
        # ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ê¸° ìœ„í•´ "í’ˆëª© ë‹¨ë…" ëŒ€ì‹  ë§¥ë½ í¬í•¨ ì¿¼ë¦¬ ìš°ì„ 
        # (íŠ¹íˆ ì‚¬ê³¼=Apple, ë°°=ship ê°™ì€ ì˜¤ì—¼ ë°©ì§€)
        qs.append(f"{item} ë†ì‚°ë¬¼")
        qs.append(f"{item} ì‚°ì§€")
        qs.append(f"{item} ë„ë§¤")
        qs.append(f"{item} ì¶œí•˜")
        qs.append(f"{item} ì €ì¥")
        qs.append(f"{item} ìˆ˜ê¸‰")
        qs.append(f"{item} ì‘í™©")

        # ê°€ê²©/ì‹œì„¸ëŠ” ë³´ì¡°(ê·¸ë˜ë„ ì¤‘ìš”í•´ì„œ í¬í•¨)
        qs.append(f"{item} ì‹œì„¸")
        qs.append(f"{item} ê°€ê²©")
        qs.append(f"{item} ê²½ë½")

    for it in FRUITS + VEGGIES + FLOWERS:
        add_item(it)

    # 2) ì¹´í…Œê³ ë¦¬ ê¸°ë°˜(í’ˆëª©ëª…ì´ ê¸°ì‚¬ì— ì—†ì„ ë•Œ ëŒ€ë¹„)
    qs += [
        "ê³¼ì¼ ë„ë§¤ê°€ê²©", "ì²­ê³¼ ë„ë§¤ê°€ê²©", "ê°€ë½ì‹œì¥ ê³¼ì¼", "ê°€ë½ì‹œì¥ ì²­ê³¼",
        "ì‹œì„¤ì±„ì†Œ ìˆ˜ê¸‰", "ì‹œì„¤ì›ì˜ˆ ìˆ˜ê¸‰", "ì‹œì„¤ì±„ì†Œ ê°€ê²©",
        "ë§Œê°ë¥˜ ìˆ˜ê¸‰", "ë§Œê°ë¥˜ ì¶œí•˜", "ê°ê·¤ ìˆ˜ê¸‰",
        "í™”í›¼ ì ˆí™” ê°€ê²©", "ê½ƒê°’ ë™í–¥",
        "ì‚°ì§€ ì¶œí•˜ ë™í–¥ ê³¼ì¼", "ì €ì¥ ê³¼ìˆ˜ ì¬ê³ ",
    ]

    # 3) ìŒ€(ì›ì˜ˆìˆ˜ê¸‰ë¶€ì™€ ì§ì ‘ì€ ì•½í•˜ì§€ë§Œ íŒ€ ë³´ê³ ì— ìì£¼ í¬í•¨)
    qs += [
        "ìŒ€ê°’", "ì‚°ì§€ìŒ€ê°’", "ë¹„ì¶•ë¯¸ ë°©ì¶œ", "ìŒ€ ìˆ˜ê¸‰",
    ]

    return uniq_keep_order(qs)

def build_policy_queries() -> List[str]:
    qs: List[str] = []
    qs += POLICY_CORE
    # ì •ì±…/ë¬¼ê°€ ê´€ë ¨ì€ í‘œí˜„ì´ ë‹¤ì–‘í•˜ë¯€ë¡œ í™•ì¥
    qs += [
        "ë†ì‚°ë¬¼ í• ì¸ í–‰ì‚¬", "ë†ì¶•ì‚°ë¬¼ í• ì¸ ì§€ì›", "ë¬¼ê°€ ì•ˆì • ë†ì‚°ë¬¼",
        "ë„ë§¤ì‹œì¥ ìœ í†µ ê°œì„ ", "ë†ì‚°ë¬¼ ìœ í†µ êµ¬ì¡° ê°œì„ ",
        "ìˆ˜ì…ê³¼ì¼ ë¬¼ëŸ‰", "ìˆ˜ì…ê³¼ì¼ ê°€ê²©", "ê³¼ì¼ ìˆ˜ì…",
    ]
    return uniq_keep_order(qs)

def build_pest_queries() -> List[str]:
    qs: List[str] = []
    qs += Pest_CORE
    qs += [
        "ê³¼ìˆ˜ ë³‘í•´ì¶©", "ê³¼ìˆ˜ ë°©ì œ", "ê³¼ìˆ˜ ì•½ì œ ì‚´í¬",
        "ì‹œì„¤ì›ì˜ˆ ë³‘í•´ì¶©", "ì§„ë”§ë¬¼ ë°©ì œ", "ì‘ì•  ë°©ì œ",
        "ëƒ‰í•´ í”¼í•´ ê³¼ìˆ˜", "ë™í•´ í”¼í•´ ê³¼ìˆ˜",
    ]
    return uniq_keep_order(qs)

def build_dist_queries() -> List[str]:
    qs: List[str] = []
    qs += DIST_CORE
    qs += [
        "ë†í˜‘ ì‚°ì§€ìœ í†µ", "ë†í˜‘ APC", "ì‚°ì§€ìœ í†µ í˜ì‹ ", "ìŠ¤ë§ˆíŠ¸íŒœ ìœ í†µ",
        "ê³µíŒì¥ ê²½ë§¤", "ë„ë§¤ì‹œì¥ ê²½ë§¤", "ë„ë§¤ì‹œì¥ ë¬¼ëŸ‰",
        "ìˆ˜ì¶œ ê²€ì—­", "ìˆ˜ì¶œ ë¬¼ë¥˜",
    ]
    return uniq_keep_order(qs)

SECTION_QUERIES: Dict[str, List[str]] = {
    "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥": build_supply_queries(),
    "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…": build_policy_queries(),
    "ë³‘í•´ì¶© ë° ë°©ì œ": build_pest_queries(),
    "ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)": build_dist_queries(),
}

# ì„¹ì…˜ ë¶€ì¡± ì‹œ ë°±í•„(ë„“ê²Œ ê¸ê³  ë¶„ë¥˜)
GLOBAL_BACKFILL_QUERIES = uniq_keep_order(
    ["ë†ì‚°ë¬¼", "ì›ì˜ˆ", "ê³¼ìˆ˜", "í™”í›¼", "ì²­ê³¼", "ë„ë§¤ì‹œì¥", "ê°€ë½ì‹œì¥", "ê³µíŒì¥", "ìˆ˜ê¸‰", "ì¶œí•˜", "ì €ì¥", "ì¬ê³ "]
    + ["ê³¼ì¼", "ì±„ì†Œ", "ì ˆí™”", "ê½ƒê°’", "ë§Œê°ë¥˜", "ê°ê·¤", "ì‚¬ê³¼", "ë°°", "ë”¸ê¸°", "í¬ë„"]
    + ["ë¬¼ê°€", "í• ì¸", "í• ë‹¹ê´€ì„¸", "ê²€ì—­", "ìˆ˜ì…ê³¼ì¼"]
    + ["APC", "ì‚°ì§€ìœ í†µ", "ì„ ë³„", "CA ì €ì¥", "ìˆ˜ì¶œ"]
    + ["ê³¼ìˆ˜í™”ìƒë³‘", "ë³‘í•´ì¶©", "ë°©ì œ", "ëƒ‰í•´", "ë™í•´"]
)

# =========================
# 2) ë§¤ì²´ ì •ì±…: ì§€ë°©ì§€/ì§€ë°©ë°©ì†¡ í¬í•¨ + êµ°ì†Œì°¨ë‹¨
# =========================
# í™•ì‹¤íˆ í•„ìš” ì—†ëŠ” êµ°ì†Œ/ë‚šì‹œ/ì´ìƒ ë„ë©”ì¸(í•„ìš”ì‹œ ê³„ì† ì¶”ê°€)
BLOCKED_DOMAINS = {
    "wikitree.co.kr", "donghaengmedia.net", "sidae.com",
    "namu.wiki", "blog.naver.com", "post.naver.com",
}

# ì‹ ë¢°/ê°€ì  ë„ë©”ì¸(ë©”ì´ì €+ì¤‘ê²¬+ì „ë¬¸+ê³µê³µ+ì§€ë°©ì§€/ì§€ë°©ë°©ì†¡ ì¼ë¶€)
# âœ… ì—¬ê¸° ì—†ëŠ” ì§€ë°©ì§€ëŠ” â€œì°¨ë‹¨ë§Œ ì•„ë‹ˆë©´â€ ìˆ˜ì§‘ë  ìˆ˜ ìˆìŒ(ì ìˆ˜ë§Œ ë‚®ìŒ)
TRUSTED_DOMAINS = {
    # í†µì‹ /ì •ì±…/ê³µê³µ
    "yna.co.kr", "newsis.com", "korea.kr", "mafra.go.kr", "at.or.kr", "krei.re.kr", "naqs.go.kr",
    # ì¤‘ì•™/ê²½ì œ/ì¢…í•©
    "mk.co.kr", "mt.co.kr", "hankyung.com", "sedaily.com", "edaily.co.kr", "asiae.co.kr", "heraldcorp.com",
    "joongang.co.kr", "donga.com", "hani.co.kr", "khan.co.kr", "chosun.com",
    # ì¤‘ê²¬
    "fnnews.com", "kmib.co.kr", "munhwa.com", "segye.com", "dt.co.kr", "nocutnews.co.kr", "news1.kr",
    # ë°©ì†¡(ì „êµ­)
    "kbs.co.kr","imbc.com","sbs.co.kr","jtbc.co.kr","ytn.co.kr","mbn.co.kr","yonhapnewstv.co.kr",
    # ì „ë¬¸/ë†ì—…
    "nongmin.com","ikpnews.net","aflnews.co.kr",
    # ì§€ë°©ì§€/ì§€ë°©ë°©ì†¡(ëŒ€í‘œì ì¸ ê²ƒ ì¼ë¶€)
    "kwnews.co.kr","kado.net","kyeonggi.com","joongboo.com","cctoday.co.kr","imaeil.com","yeongnam.com",
    "gnnews.co.kr","namdonews.com","jeonmae.co.kr","newsis.com",
    "g1tv.co.kr","cjb.co.kr","tjb.co.kr","kbc.co.kr","jibs.co.kr","obn.co.kr",
}

PRESS_MAP = {
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "newsis.com": "ë‰´ì‹œìŠ¤",
    "korea.kr": "ì •ì±…ë¸Œë¦¬í•‘",
    "mafra.go.kr": "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€",
    "at.or.kr": "aT",
    "krei.re.kr": "KREI",
    "naqs.go.kr": "ë†ê´€ì›",

    "mk.co.kr": "ë§¤ì¼ê²½ì œ",
    "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
    "hankyung.com": "í•œêµ­ê²½ì œ",
    "sedaily.com": "ì„œìš¸ê²½ì œ",
    "edaily.co.kr": "ì´ë°ì¼ë¦¬",
    "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
    "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
    "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
    "donga.com": "ë™ì•„ì¼ë³´",
    "hani.co.kr": "í•œê²¨ë ˆ",
    "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
    "chosun.com": "ì¡°ì„ ì¼ë³´",

    "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "kmib.co.kr": "êµ­ë¯¼ì¼ë³´",
    "munhwa.com": "ë¬¸í™”ì¼ë³´",
    "segye.com": "ì„¸ê³„ì¼ë³´",
    "dt.co.kr": "ë””ì§€í„¸íƒ€ì„ìŠ¤",
    "nocutnews.co.kr": "ë…¸ì»·ë‰´ìŠ¤",
    "news1.kr": "ë‰´ìŠ¤1",
    "yonhapnewstv.co.kr": "ì—°í•©ë‰´ìŠ¤TV",

    "kbs.co.kr": "KBS",
    "imbc.com": "MBC",
    "sbs.co.kr": "SBS",
    "jtbc.co.kr": "JTBC",
    "ytn.co.kr": "YTN",
    "mbn.co.kr": "MBN",

    "nongmin.com": "ë†ë¯¼ì‹ ë¬¸",
    "aflnews.co.kr": "ë†ìˆ˜ì¶•ì‚°ì‹ ë¬¸",
    "ikpnews.net": "í•œêµ­ë†ì–´ë¯¼ì‹ ë¬¸",

    "kwnews.co.kr": "ê°•ì›ì¼ë³´",
    "kado.net": "ê°•ì›ë„ë¯¼ì¼ë³´",
    "kyeonggi.com": "ê²½ê¸°ì¼ë³´",
    "joongboo.com": "ì¤‘ë¶€ì¼ë³´",
    "cctoday.co.kr": "ì¶©ì²­íˆ¬ë°ì´",
    "imaeil.com": "ë§¤ì¼ì‹ ë¬¸",
    "yeongnam.com": "ì˜ë‚¨ì¼ë³´",
    "gnnews.co.kr": "ê²½ë‚¨ì‹ ë¬¸",
    "namdonews.com": "ë‚¨ë„ì¼ë³´",
    "jeonmae.co.kr": "ì „êµ­ë§¤ì¼ì‹ ë¬¸",

    "g1tv.co.kr": "G1",
    "cjb.co.kr": "CJB",
    "tjb.co.kr": "TJB",
    "kbc.co.kr": "kbc",
    "jibs.co.kr": "JIBS",
    "obn.co.kr": "OBN",
}

LOW_RELEVANCE_HINTS = ["ì˜¨ëˆ„ë¦¬ìƒí’ˆê¶Œ", "í™˜ê¸‰", "ì§€ì—­í™”í", "ì¶•ì œ", "í–‰ì‚¬", "ê´€ê´‘", "ë§›ì§‘", "ë ˆì‹œí”¼", "í™ë³´", "ì²´í—˜"]
OUT_OF_SEASON_HINTS = ["10ì›”", "11ì›”", "ì¶”ì„", "ìˆ˜í™•ê¸°", "ê°€ì„ ìˆ˜í™•", "í–‡ì‚¬ê³¼", "í–‡ë°°"]
HIGH_RELEVANCE_HINTS = [
    "ê°€ê²©","ì‹œì„¸","ë„ë§¤","ê²½ë½","ìˆ˜ê¸‰","ë¬¼ëŸ‰","ì¶œí•˜","ì €ì¥","ì¬ê³ ","ì‘í™©","ìƒì‚°ëŸ‰",
    "ë¬¼ê°€","í• ì¸","í• ë‹¹ê´€ì„¸","ê²€ì—­","ìˆ˜ì…","ë„ë§¤ì‹œì¥","ê°€ë½ì‹œì¥","ê³µíŒì¥",
    "APC","ì„ ë³„","CA","ì €ì¥","ìˆ˜ì¶œ","ë°©ì œ","ë³‘í•´ì¶©","í™”ìƒë³‘","íƒ„ì €","ë™í•´","ëƒ‰í•´"
]

# =========================
# íœ´ì¼/ì˜ì—…ì¼ ìœ í‹¸
# =========================
def is_weekend(d: date) -> bool:
    return d.weekday() >= 5

def is_korean_holiday(d: date) -> bool:
    try:
        import holidays  # type: ignore
        return d in holidays.KR()
    except Exception:
        return False

def is_business_day(d: date) -> bool:
    return (not is_weekend(d)) and (not is_korean_holiday(d))

def compute_fixed_end_kst(now_kst: datetime, run_hour: int, early_grace_minutes: int) -> datetime:
    today_end = now_kst.replace(hour=run_hour, minute=0, second=0, microsecond=0)
    if now_kst >= today_end:
        return today_end
    if (today_end - now_kst) <= timedelta(minutes=early_grace_minutes):
        return today_end
    return today_end - timedelta(days=1)

def clean_html(s: str) -> str:
    s = html.unescape(s or "")
    s = re.sub(r"<[^>]+>", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def domain_of(url: str) -> str:
    try:
        host = (urlparse(url).hostname or "").lower()
        return host[4:] if host.startswith("www.") else host
    except Exception:
        return ""

def press_name(dom: str) -> str:
    if not dom:
        return "ë¯¸ìƒ"
    if dom in PRESS_MAP:
        return PRESS_MAP[dom]
    for k, v in PRESS_MAP.items():
        if dom.endswith("." + k):
            return v
    return dom

def has_any(text: str, keys: List[str]) -> bool:
    return any(k in text for k in keys)

def looks_low_relevance(text: str) -> bool:
    return has_any(text, LOW_RELEVANCE_HINTS) and (not has_any(text, HIGH_RELEVANCE_HINTS))

def looks_out_of_season(text: str) -> bool:
    out = has_any(text, OUT_OF_SEASON_HINTS)
    current_ok = has_any(text, ["ì €ì¥", "ì €ì¥ëŸ‰", "ì¬ê³ ", "ì „ì •", "ì„¤", "ì„¤ ì´í›„", "ìµœê·¼", "í˜„ì¬"])
    return out and (not current_ok)

def is_blocked_domain(dom: str) -> bool:
    if not dom:
        return True
    if dom in BLOCKED_DOMAINS:
        return True
    return False

def is_trusted_domain(dom: str) -> bool:
    if not dom:
        return False
    if dom in TRUSTED_DOMAINS:
        return True
    # ê³µê³µ/ê¸°ê´€ ë„ë©”ì¸ ê°€ì 
    if dom.endswith(".go.kr") or dom.endswith(".or.kr"):
        return True
    return False

def clamp(s: str, n: int) -> str:
    return s if len(s) <= n else (s[: max(0, n-1)] + "â€¦")

# =========================
# GitHub repo íŒŒì¼ read/write
# =========================
def github_get_file(repo: str, path: str, token: str, ref: str = "main") -> Tuple[Optional[str], Optional[str]]:
    url = f"https://api.github.com/repos/{repo}/contents/{path}?ref={ref}"
    r = requests.get(url, headers={"Authorization": f"Bearer {token}"}, timeout=20)
    if r.status_code == 404:
        return None, None
    if not r.ok:
        logging.error("[GitHub GET ERROR] %s", r.text)
        return None, None
    j = r.json()
    sha = j.get("sha")
    b64 = j.get("content", "")
    if j.get("encoding") == "base64" and b64:
        raw = base64.b64decode(b64).decode("utf-8", errors="replace")
        return raw, sha
    return None, sha

def github_put_file(repo: str, path: str, token: str, content_text: str,
                    branch: str = "main", sha: Optional[str] = None, message: str = "Update file") -> bool:
    url = f"https://api.github.com/repos/{repo}/contents/{path}"
    b64 = base64.b64encode(content_text.encode("utf-8")).decode("ascii")
    payload = {"message": message, "content": b64, "branch": branch}
    if sha:
        payload["sha"] = sha
    r = requests.put(url, headers={"Authorization": f"Bearer {token}"}, json=payload, timeout=20)
    if not r.ok:
        logging.error("[GitHub PUT ERROR] %s", r.text)
        return False
    return True

@dataclass
class State:
    last_end_kst_iso: str

def load_state(default_last_end: datetime) -> State:
    fallback = State(last_end_kst_iso=(default_last_end - timedelta(hours=24)).isoformat())
    if STATE_BACKEND != "repo":
        return fallback
    repo = os.getenv("GITHUB_REPOSITORY", "")
    token = os.getenv("GITHUB_TOKEN", "")
    if not repo or not token:
        return fallback
    raw, _ = github_get_file(repo, STATE_FILE_PATH, token, ref=PAGES_BRANCH)
    if not raw:
        return fallback
    try:
        j = json.loads(raw)
        v = j.get("last_end_kst_iso")
        return State(last_end_kst_iso=v) if v else fallback
    except Exception:
        return fallback

def save_state(end_kst: datetime) -> None:
    if STATE_BACKEND != "repo":
        return
    repo = os.getenv("GITHUB_REPOSITORY", "")
    token = os.getenv("GITHUB_TOKEN", "")
    if not repo or not token:
        return
    raw, sha = github_get_file(repo, STATE_FILE_PATH, token, ref=PAGES_BRANCH)
    _ = raw
    content = json.dumps({"last_end_kst_iso": end_kst.isoformat()}, ensure_ascii=False, indent=2)
    github_put_file(repo, STATE_FILE_PATH, token, content, branch=PAGES_BRANCH, sha=sha, message="Update agri-news state")

# =========================
# Kakao
# =========================
def kakao_refresh_access_token(refresh_token: str) -> str:
    rest_api_key = os.getenv("KAKAO_REST_API_KEY", "").strip()
    client_secret = os.getenv("KAKAO_CLIENT_SECRET", "").strip()
    if not rest_api_key or not client_secret:
        raise RuntimeError("Missing KAKAO_REST_API_KEY / KAKAO_CLIENT_SECRET")
    data = {
        "grant_type": "refresh_token",
        "client_id": rest_api_key,
        "client_secret": client_secret,
        "refresh_token": refresh_token,
    }
    r = requests.post(KAKAO_TOKEN_URL, data=data, timeout=20)
    if not r.ok:
        logging.error("[Kakao token ERROR] %s", r.text)
        r.raise_for_status()
    access = r.json().get("access_token")
    if not access:
        raise RuntimeError("Kakao access_token missing")
    return access

def kakao_send_text(access_token: str, text: str, link_url: str) -> None:
    template_object = {
        "object_type": "text",
        "text": text,
        "link": {"web_url": link_url, "mobile_web_url": link_url},
        "button_title": "ë¸Œë¦¬í•‘ ì—´ê¸°",
    }
    r = requests.post(
        KAKAO_MEMO_SEND_API,
        headers={"Authorization": f"Bearer {access_token}"},
        data={"template_object": json.dumps(template_object, ensure_ascii=False)},
        timeout=20,
    )
    if not r.ok:
        logging.error("[Kakao send ERROR] %s", r.text)
    r.raise_for_status()

# =========================
# Naver OpenAPI
# =========================
def naver_api_search(query: str, display: int = 50, start: int = 1) -> List[dict]:
    cid = os.getenv("NAVER_CLIENT_ID", "").strip()
    csec = os.getenv("NAVER_CLIENT_SECRET", "").strip()
    if not cid or not csec:
        raise RuntimeError("Missing NAVER_CLIENT_ID / NAVER_CLIENT_SECRET")
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    params = {"query": query, "display": display, "start": start, "sort": "date"}
    r = requests.get(NAVER_NEWS_API, headers=headers, params=params, timeout=20)
    r.raise_for_status()
    return r.json().get("items", [])

def naver_search_window(query: str, start_kst: datetime, end_kst: datetime, max_pages: int) -> List[dict]:
    collected: List[dict] = []
    display = 50
    start_idx = 1

    for _ in range(max_pages):
        items = naver_api_search(query, display=display, start=start_idx)
        if not items:
            break

        stop_early = False
        for it in items:
            pub = it.get("pubDate", "")
            try:
                dt = parsedate_to_datetime(pub).astimezone(KST)
            except Exception:
                continue

            if dt < start_kst:
                stop_early = True
                continue
            if not (start_kst <= dt < end_kst):
                continue

            title = clean_html(it.get("title", ""))
            desc = clean_html(it.get("description", ""))
            origin = (it.get("originallink") or "").strip()
            nlink = (it.get("link") or "").strip()
            url = origin or nlink
            dom = domain_of(url)

            if (not url) or is_blocked_domain(dom):
                continue

            text = f"{title} {desc}"
            if looks_low_relevance(text) or looks_out_of_season(text):
                continue

            collected.append({
                "title": title,
                "description": desc,
                "published_kst": dt.isoformat(),
                "published_hm": dt.strftime("%m/%d %H:%M"),
                "domain": dom,
                "press": press_name(dom),
                "url": url,
                "query": query,
            })

        if stop_early:
            break

        start_idx += display
        time.sleep(0.05)

    return collected

def dedupe(items: List[dict]) -> List[dict]:
    seen = set()
    out = []
    for it in items:
        u = (it.get("url") or "").strip()
        t = (it.get("title") or "").strip().lower()
        if not u or not t:
            continue
        # URL ìš°ì„  + ì œëª© ì •ê·œí™”ë¡œ ì¤‘ë³µ ì œê±°
        key = u[:280] + "|" + re.sub(r"\s+", " ", t)[:140]
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def quality_score(it: dict) -> int:
    """
    ì ìˆ˜ë¡œ ë©”ì´ì €/ì¤‘ê²¬/ì§€ë°©ì§€/ì§€ë°©ë°©ì†¡/ê³µê³µì„ â€œì‚´ë¦¬ë˜â€
    êµ°ì†ŒëŠ” ì°¨ë‹¨ ë˜ëŠ” ë‚®ì€ ì ìˆ˜ë¡œ ë’¤ë¡œ.
    """
    s = 0
    dom = it.get("domain", "")
    text = f"{it.get('title','')} {it.get('description','')}"

    if is_trusted_domain(dom):
        s += 6
    # ê³µê³µ/ê¸°ê´€ì€ ì¶”ê°€ ê°€ì 
    if dom.endswith(".go.kr") or dom.endswith(".or.kr"):
        s += 2
    if has_any(text, HIGH_RELEVANCE_HINTS):
        s += 3
    # ì¿¼ë¦¬ ìì²´ê°€ ì„¹ì…˜ í•µì‹¬ì´ë©´ ì•½ê°„ ê°€ì (ì‹¤ë¬´ ì í•©)
    q = it.get("query","")
    if any(k in q for k in ["ìˆ˜ê¸‰","ì¶œí•˜","ì €ì¥","ë„ë§¤","ê²½ë§¤","ë°©ì œ","í™”ìƒë³‘","APC","ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥","í• ë‹¹ê´€ì„¸"]):
        s += 1
    return s

def classify_section(it: dict) -> str:
    t = f"{it.get('title','')} {it.get('description','')}"
    if any(k in t for k in ["ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥", "í—ˆìœ„ê±°ë˜", "ì´ìƒê±°ë˜", "ì „ìˆ˜ì¡°ì‚¬", "í• ë‹¹ê´€ì„¸", "ê²€ì—­", "í• ì¸", "ë¬¼ê°€", "ëŒ€ì±…", "íœ´ë¬´", "ê²½ë§¤ ì¬ê°œ", "ê°€ë½ì‹œì¥", "ë„ë§¤ì‹œì¥"]):
        return "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…"
    if any(k in t for k in ["í™”ìƒë³‘", "ë³‘í•´ì¶©", "ë°©ì œ", "ì•½ì œ", "íƒ„ì €", "ê¸°ê³„ìœ ", "ë™í•´", "ëƒ‰í•´", "ì›”ë™í•´ì¶©", "ì„œë¦¬"]):
        return "ë³‘í•´ì¶© ë° ë°©ì œ"
    if any(k in t for k in ["APC", "ì‚°ì§€ìœ í†µ", "ì„ ë³„", "CA", "ì €ì¥ì‹œì„¤", "ê³µíŒì¥", "ìˆ˜ì¶œ", "ë¬¼ë¥˜", "ì½œë“œì²´ì¸"]):
        return "ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)"
    return "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥"

def collect_articles(start_kst: datetime, end_kst: datetime) -> Dict[str, List[dict]]:
    buckets: Dict[str, List[dict]] = {s: [] for s in SECTION_ORDER}
    seen_urls: set[str] = set()

    # 1) ì„¹ì…˜ë³„ ì •ë°€ ìˆ˜ì§‘(í‚¤ì›Œë“œê°€ ë„“ì–´ì¡Œìœ¼ë¯€ë¡œ â€œí•„ìš”ëŸ‰ ì±„ìš°ë©´ ì¤‘ë‹¨â€)
    for sec in SECTION_ORDER:
        local: List[dict] = []
        for q in SECTION_QUERIES.get(sec, []):
            local.extend(naver_search_window(q, start_kst, end_kst, max_pages=MAX_PAGES_PER_QUERY))

            # ì¶©ë¶„íˆ ëª¨ì´ë©´ ì¤‘ë‹¨(ì†ë„/ì¿¼í„° ì ˆì•½)
            if len(local) >= MAX_ARTICLES_PER_SECTION * 10:
                break

        local = dedupe(local)
        local.sort(key=lambda x: (quality_score(x), x.get("published_kst","")), reverse=True)

        picked: List[dict] = []
        for it in local:
            u = it["url"]
            if u in seen_urls:
                continue
            picked.append(it)
            seen_urls.add(u)
            if len(picked) >= MAX_ARTICLES_PER_SECTION:
                break

        buckets[sec] = picked
        logging.info("[Collect] %s: %d", sec, len(buckets[sec]))

    # 2) ë°±í•„: ë¶€ì¡± ì„¹ì…˜ì´ ìˆìœ¼ë©´ ë„“ê²Œ ê¸ê³  ìë™ ë¶„ë¥˜í•˜ì—¬ ì±„ì›€
    if any(len(buckets[s]) < MIN_ARTICLES_PER_SECTION for s in SECTION_ORDER):
        pool: List[dict] = []
        for q in GLOBAL_BACKFILL_QUERIES:
            pool.extend(naver_search_window(q, start_kst, end_kst, max_pages=MAX_PAGES_PER_QUERY))

        pool = dedupe(pool)
        pool.sort(key=lambda x: (quality_score(x), x.get("published_kst","")), reverse=True)
        pool = pool[:GLOBAL_BACKFILL_LIMIT]

        for it in pool:
            u = it["url"]
            if u in seen_urls:
                continue
            sec = classify_section(it)
            if len(buckets[sec]) >= MAX_ARTICLES_PER_SECTION:
                continue
            buckets[sec].append(it)
            seen_urls.add(u)

        for sec in SECTION_ORDER:
            logging.info("[Backfill] %s: %d", sec, len(buckets[sec]))

    return buckets

# =========================
# OpenAI ìš”ì•½(2~3ë¬¸ì¥ + ì²´í¬í¬ì¸íŠ¸)
# =========================
def openai_summarize(buckets: Dict[str, List[dict]], start_kst: datetime, end_kst: datetime) -> Dict[str, List[dict]]:
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    total = sum(len(v) for v in buckets.values())
    if total == 0:
        return buckets

    if not api_key:
        for sec in SECTION_ORDER:
            for a in buckets[sec]:
                a["summary"] = a.get("description","") or a.get("title","")
                a["point"] = ""
        return buckets

    compact = []
    for sec in SECTION_ORDER:
        for a in buckets[sec]:
            compact.append({
                "section": sec,
                "press": a.get("press",""),
                "title": a.get("title",""),
                "description": a.get("description",""),
                "url": a.get("url",""),
            })

    schema = {
        "type": "object",
        "properties": {
            "items": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string"},
                        "summary": {"type": "string"},
                        "point": {"type": "string"},
                    },
                    "required": ["url","summary","point"],
                    "additionalProperties": False,
                },
            }
        },
        "required": ["items"],
        "additionalProperties": False,
    }

    system = (
        "ë„ˆëŠ” ë†í˜‘ ê²½ì œì§€ì£¼ ì›ì˜ˆìˆ˜ê¸‰ë¶€(ê³¼ìˆ˜í™”í›¼íŒ€ ì¤‘ì‹¬) ë‚´ë¶€ ê³µìœ ìš© ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì‘ì„±ìë‹¤.\n"
        "ê° ê¸°ì‚¬ë§ˆë‹¤ ì•„ë˜ ìˆœì„œë¥¼ ë°˜ë“œì‹œ ì§€ì¼œë¼:\n"
        "1) summary: 2~3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ(ìˆ˜ê¸‰/ê°€ê²©/ë¬¼ëŸ‰/ì¶œí•˜/ì €ì¥/ìœ í†µ/ì •ì±…/ë°©ì œ ê´€ì )\n"
        "2) point: ì²´í¬í¬ì¸íŠ¸ 1ë¬¸ì¥(íŒ€ì´ ë¬´ì—‡ì„ í™•ì¸/ëŒ€ì‘í•´ì•¼ í•˜ëŠ”ì§€)\n"
        "ê³¼ì¥/ì¶”ì¸¡ ê¸ˆì§€. ì• ë§¤í•˜ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ.\n"
        "ë¬¸ì¥ ì§§ê³  ê°€ë…ì„± ì¢‹ê²Œ.\n"
    )
    user = f"ê¸°ê°„(KST): {start_kst.isoformat()} ~ {end_kst.isoformat()}\n{json.dumps(compact, ensure_ascii=False)}"

    payload = {
        "model": OPENAI_MODEL,
        "input": [{"role":"system","content":system},{"role":"user","content":user}],
        "reasoning_effort": OPENAI_REASONING_EFFORT,
        "max_output_tokens": OPENAI_MAX_OUTPUT_TOKENS,
        "text": {"format": {"type":"json_schema","name":"agri_summaries","strict":True,"schema":schema}},
        "store": False,
    }

    r = requests.post(
        OPENAI_RESPONSES_URL,
        headers={"Authorization": f"Bearer {api_key}", "Content-Type":"application/json"},
        json=payload,
        timeout=90,
    )
    if not r.ok:
        logging.error("[OpenAI ERROR] %s", r.text)
        for sec in SECTION_ORDER:
            for a in buckets[sec]:
                a["summary"] = a.get("description","") or a.get("title","")
                a["point"] = ""
        return buckets

    data = r.json()
    out_text = ""
    for item in data.get("output", []) or []:
        if item.get("type") == "message":
            for c in item.get("content", []) or []:
                if c.get("type") == "output_text":
                    out_text += c.get("text","")
    out_text = out_text.strip()
    if not out_text:
        return buckets

    j = json.loads(out_text)
    mp = {it["url"]: it for it in j.get("items", [])}

    for sec in SECTION_ORDER:
        for a in buckets[sec]:
            u = a.get("url","")
            m = mp.get(u, {})
            summary = (m.get("summary") or "").strip()
            point = (m.get("point") or "").strip()
            if not summary:
                summary = a.get("description","") or a.get("title","")
            a["summary"] = summary
            a["point"] = point

    return buckets

# =========================
# ìƒì„¸ í˜ì´ì§€(ëª¨ë°”ì¼ ì¹´ë“œ UI)
# =========================
def make_html(buckets: Dict[str, List[dict]], start_kst: datetime, end_kst: datetime) -> str:
    total = sum(len(v) for v in buckets.values())
    span_days = (end_kst.date() - start_kst.date()).days

    def esc(x: str) -> str:
        return html.escape(x or "")

    def card(a: dict) -> str:
        press = esc(a.get("press","ë¯¸ìƒ"))
        hm = esc(a.get("published_hm",""))
        title = esc(a.get("title",""))
        summary = esc((a.get("summary") or "").strip())
        point = esc((a.get("point") or "").strip())
        url = a.get("url","")

        point_html = f'<div class="point">ì²´í¬í¬ì¸íŠ¸: {point}</div>' if point else ""
        return f"""
        <div class="card">
          <div class="meta"><span class="press">{press}</span><span class="time">{hm}</span></div>
          <div class="title">{title}</div>
          <div class="summary">{summary}</div>
          {point_html}
          <a class="btn" href="{esc(url)}" target="_blank" rel="noopener noreferrer">ì›ë¬¸ ì—´ê¸°</a>
        </div>
        """

    sections_html = ""
    for sec in SECTION_ORDER:
        items = buckets.get(sec, [])
        sections_html += f'<div class="section"><h2>{esc(sec)} <span class="count">({len(items)})</span></h2>'
        if not items:
            sections_html += '<div class="empty">íŠ¹ì´ì‚¬í•­ ì—†ìŒ</div></div>'
            continue
        for a in items:
            sections_html += card(a)
        sections_html += "</div>"

    note = ""
    if span_days >= 2:
        note = f"íœ´ì¼/ì£¼ë§ ëˆ„ì  í¬í•¨: {span_days}ì¼"

    return f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ë†ì‚°ë¬¼ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</title>
<style>
  :root {{
    --bg:#f6f7f9; --card:#fff; --line:#e5e7eb; --text:#111827; --muted:#6b7280;
  }}
  body{{margin:0;background:var(--bg);color:var(--text);font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Arial,sans-serif;}}
  .wrap{{max-width:900px;margin:0 auto;padding:14px;}}
  .header{{background:var(--card);border:1px solid var(--line);border-radius:14px;padding:14px;margin-bottom:12px;box-shadow:0 2px 10px rgba(0,0,0,.04);}}
  .h1{{font-size:18px;font-weight:800;margin:0 0 6px;}}
  .sub{{color:var(--muted);font-size:13px;line-height:1.35;}}
  .chips{{margin-top:10px;display:flex;flex-wrap:wrap;gap:8px;}}
  .chip{{font-size:12px;color:#111;border:1px solid var(--line);background:#fff;border-radius:999px;padding:6px 10px;}}
  .section{{margin-top:12px;}}
  h2{{font-size:16px;margin:14px 2px 10px;}}
  .count{{color:var(--muted);font-weight:600;}}
  .empty{{color:var(--muted);background:var(--card);border:1px dashed var(--line);border-radius:12px;padding:12px;}}
  .card{{background:var(--card);border:1px solid var(--line);border-radius:14px;padding:12px;margin:10px 0;box-shadow:0 2px 10px rgba(0,0,0,.03);}}
  .meta{{display:flex;justify-content:space-between;align-items:center;margin-bottom:6px;}}
  .press{{font-weight:800;font-size:13px;}}
  .time{{color:var(--muted);font-size:12px;}}
  .title{{font-size:15px;font-weight:800;line-height:1.35;margin:4px 0 8px;}}
  .summary{{font-size:14px;line-height:1.5;color:#111;margin:0 0 8px;}}
  .point{{font-size:13px;line-height:1.4;color:#0f172a;background:#f3f4f6;border-radius:10px;padding:8px 10px;margin:6px 0 10px;}}
  .btn{{display:inline-block;text-decoration:none;font-weight:800;font-size:14px;border:1px solid var(--line);border-radius:12px;padding:10px 12px;}}
  .footer{{color:var(--muted);font-size:12px;margin:18px 4px 8px;}}
</style>
</head>
<body>
<div class="wrap">
  <div class="header">
    <div class="h1">ë†ì‚°ë¬¼ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</div>
    <div class="sub">ê¸°ê°„: {esc(start_kst.strftime('%Y-%m-%d %H:%M'))} ~ {esc(end_kst.strftime('%Y-%m-%d %H:%M'))} (KST) Â· ì´ {total}ê±´<br>{esc(note)}</div>
    <div class="chips">
      {''.join([f'<div class="chip">{html.escape(sec)} {len(buckets.get(sec, []))}ê±´</div>' for sec in SECTION_ORDER])}
    </div>
  </div>

  {sections_html}

  <div class="footer">* íœ´ì¼/ì£¼ë§ì—ëŠ” ë°œì†¡ì„ ìŠ¤í‚µí•˜ê³  stateê°€ ê°±ì‹ ë˜ì§€ ì•Šì•„, ë‹¤ìŒ ì˜ì—…ì¼ì— ìë™ìœ¼ë¡œ ëˆ„ì  êµ¬ê°„ì´ í™•ì¥ë©ë‹ˆë‹¤(ì¤‘ë³µ URLì€ 1íšŒë§Œ ë°˜ì˜).</div>
</div>
</body>
</html>
"""

def publish_to_pages(html_text: str, end_kst: datetime) -> None:
    if PUBLISH_MODE != "github_pages":
        return
    repo = os.getenv("GITHUB_REPOSITORY", "")
    token = os.getenv("GITHUB_TOKEN", "")
    if not repo or not token:
        logging.warning("[Pages] missing repo/token")
        return
    raw, sha = github_get_file(repo, PAGES_FILE_PATH, token, ref=PAGES_BRANCH)
    _ = raw
    github_put_file(
        repo, PAGES_FILE_PATH, token, html_text,
        branch=PAGES_BRANCH, sha=sha,
        message=f"Publish brief {end_kst.strftime('%Y-%m-%d')}",
    )

# =========================
# ì¹´í†¡ ë©”ì‹œì§€(ë³¸ë¬¸ URL ì œê±°)
# =========================
def auto_pages_url() -> str:
    repo = os.getenv("GITHUB_REPOSITORY", "")
    if not repo or "/" not in repo:
        return ""
    owner, name = repo.split("/", 1)
    return f"https://{owner}.github.io/{name}/"

def build_kakao_message(buckets: Dict[str, List[dict]], end_kst: datetime, span_days: int) -> str:
    total = sum(len(v) for v in buckets.values())
    span_hint = f" (ëˆ„ì  {span_days}ì¼)" if span_days >= 2 else ""
    counts = " / ".join([
        f"í’ˆëª© {len(buckets.get('í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥', []))}",
        f"ì •ì±… {len(buckets.get('ì£¼ìš” ì´ìŠˆ ë° ì •ì±…', []))}",
        f"ë°©ì œ {len(buckets.get('ë³‘í•´ì¶© ë° ë°©ì œ', []))}",
        f"ìœ í†µ {len(buckets.get('ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)', []))}",
    ])

    highlights = []
    for sec in SECTION_ORDER:
        if not buckets.get(sec):
            continue
        a = buckets[sec][0]
        highlights.append(f"- {a.get('press','')}: {clamp(a.get('title',''), 26)}")
        if len(highlights) >= 3:
            break
    if not highlights:
        highlights = ["- í•µì‹¬ ê¸°ì‚¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤(ê¸°ê°„/í•„í„°/í‚¤ì›Œë“œ ì ê²€ í•„ìš”)"]

    msg = "\n".join([
        f"[ë†ì‚°ë¬¼ ë¸Œë¦¬í•‘] {end_kst.strftime('%m/%d')} {RUN_HOUR_KST:02d}ì‹œ Â· {total}ê±´{span_hint}",
        f"ì„¹ì…˜: {counts}",
        "ì˜¤ëŠ˜ í•µì‹¬ 3ì¤„:",
        *highlights,
        "ğŸ‘‡ ë²„íŠ¼ â€˜ë¸Œë¦¬í•‘ ì—´ê¸°â€™ì—ì„œ ìš”ì•½/ì²´í¬í¬ì¸íŠ¸/ì›ë¬¸ í™•ì¸",
    ])
    return clamp(msg, KAKAO_MESSAGE_SOFT_LIMIT)

# =========================
# Main
# =========================
def main():
    now_kst = datetime.now(tz=KST)
    end_kst = compute_fixed_end_kst(now_kst, RUN_HOUR_KST, EARLY_GRACE_MINUTES)

    # ì˜ì—…ì¼ë§Œ ë°œì†¡(íœ´ì¼/ì£¼ë§ì€ ìŠ¤í‚µ -> state ë¯¸ê°±ì‹  -> ë‹¤ìŒ ì˜ì—…ì¼ ëˆ„ì )
    if (not FORCE_SEND) and (not is_business_day(end_kst.date())):
        logging.info("[SKIP] Not a business day in KR: %s (weekend/holiday)", end_kst.date())
        return

    state = load_state(end_kst)
    try:
        start_kst = datetime.fromisoformat(state.last_end_kst_iso)
        if start_kst.tzinfo is None:
            start_kst = start_kst.replace(tzinfo=KST)
    except Exception:
        start_kst = end_kst - timedelta(hours=24)

    if start_kst >= end_kst:
        start_kst = end_kst - timedelta(hours=24)

    logging.info("[INFO] Window KST: %s ~ %s", start_kst, end_kst)

    view_url = BRIEF_VIEW_URL or auto_pages_url()
    if not view_url:
        raise RuntimeError("BRIEF_VIEW_URL is empty and auto_pages_url failed.")

    # 1) ìˆ˜ì§‘(í‚¤ì›Œë“œ ì „ë©´ ì¬ì¡°ì • + ë°±í•„)
    buckets = collect_articles(start_kst, end_kst)

    # 2) ìš”ì•½
    buckets = openai_summarize(buckets, start_kst, end_kst)

    # 3) ìƒì„¸ í˜ì´ì§€ ë°œí–‰
    html_page = make_html(buckets, start_kst, end_kst)
    publish_to_pages(html_page, end_kst)

    # 4) ì¹´í†¡ 1ë©”ì‹œì§€(ë³¸ë¬¸ URL ì œê±°)
    refresh_token = os.getenv("KAKAO_REFRESH_TOKEN", "").strip()
    if not refresh_token:
        raise RuntimeError("Missing KAKAO_REFRESH_TOKEN")
    access = kakao_refresh_access_token(refresh_token)

    span_days = (end_kst.date() - start_kst.date()).days
    msg = build_kakao_message(buckets, end_kst, span_days)
    kakao_send_text(access, msg, view_url)

    # 5) state ì €ì¥(ë°œì†¡ ì„±ê³µ í›„)
    save_state(end_kst)
    logging.info("[DONE] sent and state updated: %s", end_kst.isoformat())

if __name__ == "__main__":
    main()
