# -*- coding: utf-8 -*-
"""
agri-news-brief main.py (production)

âœ… Changes in this version (IMPORTANT):
1) "ë¸Œë¦¬í•‘ ì—´ê¸°"ê°€ gist.github.com ìœ¼ë¡œ ì—´ë¦¬ëŠ” ë¬¸ì œë¥¼ 'ì² ì €íˆ' ì¡ê¸° ìœ„í•œ ì§„ë‹¨/ì°¨ë‹¨ ê°•í™”
   - ì½”ë“œì—ì„œ daily_urlì€ ì ˆëŒ€ gistë¡œ ë§Œë“¤ì§€ ì•ŠìŒ
   - ë°œì†¡ ì „: daily_url ë„ë©”ì¸ì„ ê²€ì‚¬í•˜ì—¬, ì¹´ì¹´ì˜¤ ê°œë°œì ì½˜ì†”(í”Œë«í¼ > Web > ì‚¬ì´íŠ¸ ë„ë©”ì¸)ì—
     ë“±ë¡í•´ì•¼ í•  ë„ë©”ì¸ í›„ë³´ë¥¼ ë¡œê·¸ì— ëª…ì‹œì ìœ¼ë¡œ ì¶œë ¥
   - ë§Œì•½ ì¹´ì¹´ì˜¤ ë„ë©”ì¸ ë¯¸ë“±ë¡ ë•Œë¬¸ì— ë§í¬ê°€ ê°•ì œë¡œ gistë¡œ ì—´ë¦¬ëŠ” ê²½ìš°:
     => ì½”ë“œ ìˆ˜ì •ë§Œìœ¼ë¡œ í•´ê²° ë¶ˆê°€. "ì‚¬ì´íŠ¸ ë„ë©”ì¸"ì— GitHub Pages ë„ë©”ì¸ì„ ì¶”ê°€í•´ì•¼ í•¨.
        (ì˜ˆ: hongtaehwa.github.io)

2) ì¹´ì¹´ì˜¤ ë©”ì‹œì§€ í¬ë§· ê°œì„ (ê°€ë…ì„±):
   - í•­ëª©(ë¸”ë¡) ê°„ì—ë§Œ ë¹ˆ ì¤„ 1ê°œ
   - í•­ëª© ë‚´ë¶€ëŠ” ì¤„ë°”ê¿ˆë§Œ (ë¶ˆí•„ìš”í•œ 1ì¹¸ì”© ë„ìš°ê¸° ì œê±°)
   - (ë§¤ì²´ëª…) ê¸°ì‚¬ì œëª© í˜•íƒœ ê³ ì •

3) ( ) ì•ˆì—ëŠ” ë§í¬ê°€ ì•„ë‹Œ 'ë§¤ì²´ëª…'ì´ ë“¤ì–´ê°€ë„ë¡ press ì¶”ì¶œ/í‘œì‹œ ê°•í™”

ê¸°ëŠ¥:
- Naver News API ê²€ìƒ‰(ì„¹ì…˜ë³„ ë©€í‹° ì¿¼ë¦¬)
- ê°•í•œ ê´€ë ¨ë„ í•„í„°ë§(ì—°ì˜ˆ/ì—¬í–‰/ì£¼ì‹/ë¬´ê´€ ê¸°ì‚¬ ì°¨ë‹¨)
- ì˜ì—…ì¼ ê¸°ì¤€ ìœˆë„ìš°(íœ´ì¼/ì£¼ë§ì€ ë‹¤ìŒ ì˜ì—…ì¼ì— ëˆ„ì )
- OpenAI ìš”ì•½(ì˜µì…˜): ì‹¤íŒ¨/ì¿¼í„°/í‚¤ ì—†ìŒì´ë©´ description ê¸°ë°˜ìœ¼ë¡œ ìë™ í´ë°±
- GitHub Pages ì¶œë ¥:
  - docs/index.html (ìµœì‹ /ì•„ì¹´ì´ë¸Œ)
  - docs/archive/YYYY-MM-DD.html (ì¼ìë³„ ìŠ¤ëƒ…ìƒ·)
- ì¹´ì¹´ì˜¤ "ë‚˜ì—ê²Œ ë³´ë‚´ê¸°" ë‹¨ì¼ ë©”ì‹œì§€ + "ë¸Œë¦¬í•‘ ì—´ê¸°" ë²„íŠ¼(í•´ë‹¹ ë‚ ì§œ í˜ì´ì§€ë¡œ)

ENV REQUIRED:
- NAVER_CLIENT_ID
- NAVER_CLIENT_SECRET
- GITHUB_REPO               (e.g., HongTaeHwa/agri-news-brief) ë˜ëŠ” Actions ê¸°ë³¸ GITHUB_REPOSITORY
- GH_TOKEN or GITHUB_TOKEN  (Actions built-in token OK if permissions: contents: write)
- KAKAO_REST_API_KEY
- KAKAO_REFRESH_TOKEN

OPTIONAL:
- OPENAI_API_KEY            (ì—†ê±°ë‚˜/ì‹¤íŒ¨í•˜ë©´ í´ë°±)
- OPENAI_MODEL              (default: gpt-5.2)
- KAKAO_CLIENT_SECRET
- PAGES_BASE_URL            (ì»¤ìŠ¤í…€ ë„ë©”ì¸/ì¡°ì§ í˜ì´ì§€ ë“±)
- REPORT_HOUR_KST           (default: 7)
- MAX_PER_SECTION           (default: 10)
- MIN_PER_SECTION           (default: 5)
- EXTRA_HOLIDAYS            (comma dates, e.g., 2026-02-17,2026-02-18)
- EXCLUDE_HOLIDAYS          (comma dates to treat as business day)
- KAKAO_INCLUDE_LINK_IN_TEXT (true/false, default false)
- FORCE_REPORT_DATE         (YYYY-MM-DD) backfill test
- FORCE_RUN_ANYDAY          (true/false) íœ´ì¼/ì£¼ë§ì—ë„ ê°•ì œ ì‹¤í–‰(í…ŒìŠ¤íŠ¸ìš©)
- FORCE_END_NOW             (true/false) endë¥¼ "ì§€ê¸ˆ"ìœ¼ë¡œ(í…ŒìŠ¤íŠ¸ìš©)
- STRICT_KAKAO_LINK_CHECK   (true/false, default false)  # trueë©´ ë„ë©”ì¸ ì˜ì‹¬ ì‹œ ë°œì†¡ ì¤‘ë‹¨(í…ŒìŠ¤íŠ¸ìš©)
"""

import os
import re
import json
import base64
import html
import logging
import hashlib
from dataclasses import dataclass
from datetime import datetime, timedelta, date, timezone
from email.utils import parsedate_to_datetime
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import requests


# -----------------------------
# Logging
# -----------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
log = logging.getLogger("agri-brief")


# -----------------------------
# Config
# -----------------------------
KST = timezone(timedelta(hours=9))

REPORT_HOUR_KST = int(os.getenv("REPORT_HOUR_KST", "7"))
MAX_PER_SECTION = int(os.getenv("MAX_PER_SECTION", "10"))
MIN_PER_SECTION = int(os.getenv("MIN_PER_SECTION", "5"))

STATE_FILE_PATH = ".agri_state.json"
ARCHIVE_MANIFEST_PATH = ".agri_archive.json"

DOCS_INDEX_PATH = "docs/index.html"
DOCS_ARCHIVE_DIR = "docs/archive"

DEFAULT_REPO = (os.getenv("GITHUB_REPO") or os.getenv("GITHUB_REPOSITORY") or "").strip()
GH_TOKEN = (os.getenv("GH_TOKEN") or os.getenv("GITHUB_TOKEN") or "").strip()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID", "").strip()
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET", "").strip()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-5.2").strip()

KAKAO_REST_API_KEY = os.getenv("KAKAO_REST_API_KEY", "").strip()
KAKAO_REFRESH_TOKEN = os.getenv("KAKAO_REFRESH_TOKEN", "").strip()
KAKAO_CLIENT_SECRET = os.getenv("KAKAO_CLIENT_SECRET", "").strip()

KAKAO_INCLUDE_LINK_IN_TEXT = os.getenv("KAKAO_INCLUDE_LINK_IN_TEXT", "false").strip().lower() in ("1", "true", "yes")

FORCE_REPORT_DATE = os.getenv("FORCE_REPORT_DATE", "").strip()  # YYYY-MM-DD
FORCE_RUN_ANYDAY = os.getenv("FORCE_RUN_ANYDAY", "false").strip().lower() in ("1", "true", "yes")
FORCE_END_NOW = os.getenv("FORCE_END_NOW", "false").strip().lower() in ("1", "true", "yes")

STRICT_KAKAO_LINK_CHECK = os.getenv("STRICT_KAKAO_LINK_CHECK", "false").strip().lower() in ("1", "true", "yes")

EXTRA_HOLIDAYS = set([s.strip() for s in os.getenv("EXTRA_HOLIDAYS", "").split(",") if s.strip()])
EXCLUDE_HOLIDAYS = set([s.strip() for s in os.getenv("EXCLUDE_HOLIDAYS", "").split(",") if s.strip()])


# -----------------------------
# Domain blocks
# -----------------------------
BLOCKED_DOMAINS = {
    "wikitree.co.kr",
    "theqoo.net",
    "instiz.net",
    "namu.wiki",
    "allurekorea.com",
    "vogue.co.kr",
    "marieclairekorea.com",
    "cosmopolitan.co.kr",
    "gqkorea.co.kr",
}

# Strong agriculture context keywords (raise relevance)
AGRI_STRONG_TERMS = [
    "ê°€ë½ì‹œì¥", "ë„ë§¤ì‹œì¥", "ê³µíŒì¥", "ê²½ë½", "ê²½ë½ê°€", "ê²½ë§¤", "ì²­ê³¼", "ì‚°ì§€", "ì¶œí•˜", "ë¬¼ëŸ‰", "ë°˜ì…",
    "ì‚°ì§€ìœ í†µ", "APC", "ì‚°ì§€ìœ í†µì„¼í„°", "ì„ ë³„", "CAì €ì¥", "ì €ì¥ê³ ", "ì €ì¥ëŸ‰",
    "ì‹œì„¸", "ë„ë§¤ê°€ê²©", "ì†Œë§¤ê°€ê²©", "ê°€ê²©", "ìˆ˜ê¸‰", "ìˆ˜ê¸‰ë™í–¥", "ì‘í™©", "ìƒì‚°ëŸ‰", "ì¬ë°°", "ìˆ˜í™•", "ë©´ì ",
    "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€", "ë†ì‹í’ˆë¶€", "aT", "í•œêµ­ë†ìˆ˜ì‚°ì‹í’ˆìœ í†µê³µì‚¬", "ë†ê´€ì›", "êµ­ë¦½ë†ì‚°ë¬¼í’ˆì§ˆê´€ë¦¬ì›",
    "ê²€ì—­", "í• ë‹¹ê´€ì„¸", "ìˆ˜ì…", "ìˆ˜ì¶œ", "ê´€ì„¸", "í†µê´€", "ì›ì‚°ì§€", "ë¶€ì •ìœ í†µ", "ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥",
    "ë¹„ì¶•ë¯¸", "ì •ë¶€", "ëŒ€ì±…", "ì§€ì›", "í• ì¸ì§€ì›", "ì„±ìˆ˜í’ˆ",
    "ë³‘í•´ì¶©", "ë°©ì œ", "ì•½ì œ", "ì‚´í¬", "ì˜ˆì°°", "ê³¼ìˆ˜í™”ìƒë³‘", "íƒ„ì €ë³‘", "ë™í•´", "ëƒ‰í•´", "ì›”ë™",
]

# Very common off-topic hints (penalize)
OFFTOPIC_HINTS = [
    "ë°°ìš°", "ì•„ì´ëŒ", "ë“œë¼ë§ˆ", "ì˜í™”", "ì˜ˆëŠ¥", "ì½˜ì„œíŠ¸", "íŒ¬", "ìœ íŠœë¸Œ", "ë®¤ì§",
    "ëŒ€í†µë ¹", "êµ­íšŒ", "ì´ì„ ", "ê²€ì°°", "ì¬íŒ", "íƒ„í•µ", "ì •ë‹¹",
    "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì£¼ê°€", "ê¸‰ë“±", "ê¸‰ë½", "ë¹„íŠ¸ì½”ì¸", "í™˜ìœ¨",
    "ì—¬í–‰", "ê´€ê´‘", "í˜¸í…”", "ë¦¬ì¡°íŠ¸", "ë ˆìŠ¤í† ë‘", "ì™€ì¸", "í•´ë³€", "íœ´ì–‘", "íŒŒìš´ë“œ", "ë‹¬ëŸ¬", "ìœ ë¡œ",
]

TRAVEL_MARKET_HINTS = [
    "í˜„ì§€", "ì „í†µì‹œì¥", "ë…¸ì ", "íŒŒìš´ë“œ", "ë¡œì œ", "íƒ€íŒŒìŠ¤", "ë¦¬ë¹„ì—ë¼", "í”„ë‘ìŠ¤", "ë‘ë°”ì´",
]

KOREA_CONTEXT_HINTS = [
    "êµ­ë‚´", "í•œêµ­", "ìš°ë¦¬ë‚˜ë¼", "ë†í˜‘", "ì§€ìì²´", "êµ°", "ì‹œ", "ë„", "ë†ê°€", "ì‚°ì§€", "ê°€ë½ì‹œì¥",
    "ë†ì‹í’ˆë¶€", "aT", "ë†ê´€ì›", "ëŒ€í•œë¯¼êµ­", "ì„¤", "ëª…ì ˆ",
]


# -----------------------------
# Section configuration
# -----------------------------
SECTIONS = [
    {
        "key": "supply",
        "title": "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥",
        "color": "#0f766e",
        "queries": [
            # êµ¬ì¡°/ê¸°í›„/ì¬ë°°ì§€ ì´ë™
            "ê¸°í›„ë³€í™” ì‚¬ê³¼ ì¬ë°°ì§€ ë¶ìƒ ê°•ì›ë„",
            "ê³¼ìˆ˜ ì¬ë°°ë©´ì  ë³€í™” ì‚¬ê³¼ ë°°",
            # ì‚¬ê³¼/ë°°/ê°/ë§Œê°/ê¸°íƒ€
            "ì‚¬ê³¼ ê°€ê²©", "ì‚¬ê³¼ ì‹œì„¸", "ì‚¬ê³¼ ë„ë§¤ì‹œì¥", "ì‚¬ê³¼ ì €ì¥ëŸ‰", "ì‚¬ê³¼ ì¶œí•˜",
            "ë°°(ê³¼ì¼) ê°€ê²©", "ë°°(ê³¼ì¼) ì‹œì„¸", "ë°°(ê³¼ì¼) ë„ë§¤ì‹œì¥",
            "ë‹¨ê° ì‹œì„¸", "ë‹¨ê° ì €ì¥ëŸ‰",
            "ë–«ì€ê° ê³¶ê° íƒ„ì €ë³‘", "ê³¶ê° ê°€ê²©", "ë‘¥ì‹œ ê³¶ê°",
            "ê°ê·¤ ê°€ê²©", "í•œë¼ë´‰ ê°€ê²©", "ë ˆë“œí–¥ ê°€ê²©", "ì²œí˜œí–¥ ê°€ê²©", "ë§Œê°ë¥˜ ì¶œí•˜",
            "ì°¸ë‹¤ë˜ ì‹œì„¸", "í‚¤ìœ„ ì‹œì„¸",
            "ìƒ¤ì¸ë¨¸ìŠ¤ìº£ ì‹œì„¸", "í¬ë„ ê°€ê²©",
            "í’‹ê³ ì¶” ê°€ê²©", "ì˜¤ì´ ê°€ê²©", "ì‹œì„¤ì±„ì†Œ ê°€ê²©",
            "ì ˆí™” ê°€ê²©", "ì¡¸ì—… ì…í•™ ì ˆí™”",
            "ìŒ€ ì‚°ì§€ ê°€ê²©", "ë¹„ì¶•ë¯¸ ë°©ì¶œ",
        ],
        "must_terms": ["ê°€ê²©", "ì‹œì„¸", "ìˆ˜ê¸‰", "ì¶œí•˜", "ë„ë§¤", "ê²½ë½", "ì €ì¥", "ì‘í™©", "ìƒì‚°", "ì¬ë°°", "ìˆ˜í™•", "ë©´ì ", "ë¬¼ëŸ‰"],
    },
    {
        "key": "policy",
        "title": "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…",
        "color": "#1d4ed8",
        "queries": [
            "ë†ì‚°ë¬¼ ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥ í—ˆìœ„ê±°ë˜",
            "ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥ ì´ìƒê±°ë˜ ì „ìˆ˜ì¡°ì‚¬",
            "ë†ì¶•ìˆ˜ì‚°ë¬¼ í• ì¸ì§€ì› ì—°ì¥",
            "í• ë‹¹ê´€ì„¸ ê³¼ì¼ ê²€ì—­ ì™„í™”",
            "ì„±ìˆ˜í’ˆ ê°€ê²© ì•ˆì • ëŒ€ì±…",
            "ëŒ€í•œë¯¼êµ­ ì •ì±…ë¸Œë¦¬í•‘ ë†ì¶•ìˆ˜ì‚°ë¬¼",
            "korea.kr ë†ì¶•ìˆ˜ì‚°ë¬¼ í• ì¸",
            "ë†ì‹í’ˆë¶€ ì •ì±… í• ë‹¹ê´€ì„¸ ë†ì¶•ìˆ˜ì‚°ë¬¼",
        ],
        "must_terms": ["ì •ì±…", "ëŒ€ì±…", "ì§€ì›", "í• ì¸", "í• ë‹¹ê´€ì„¸", "ê²€ì—­", "ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥", "ë¹„ì¶•ë¯¸", "ì„±ìˆ˜í’ˆ", "ìˆ˜ê¸‰", "ë¬¼ê°€"],
    },
    {
        "key": "pest",
        "title": "ë³‘í•´ì¶© ë° ë°©ì œ",
        "color": "#b45309",
        "queries": [
            "ê³¼ìˆ˜í™”ìƒë³‘ ì•½ì œ ì‹ ì²­",
            "ê³¼ìˆ˜í™”ìƒë³‘ ê¶¤ì–‘ ì œê±°",
            "ì›”ë™ í•´ì¶© ë°©ì œ ê¸°ê³„ìœ ìœ ì œ",
            "íƒ„ì €ë³‘ ì˜ˆë°© ë°©ì œ",
            "ë™í•´ ëƒ‰í•´ ê³¼ìˆ˜ í”¼í•´ ëŒ€ë¹„",
        ],
        "must_terms": ["ë°©ì œ", "ë³‘í•´ì¶©", "ì•½ì œ", "ì‚´í¬", "ì˜ˆì°°", "ê³¼ìˆ˜í™”ìƒë³‘", "íƒ„ì €ë³‘", "ëƒ‰í•´", "ë™í•´", "ì›”ë™"],
    },
    {
        "key": "dist",
        "title": "ìœ í†µ ë° í˜„ì¥ (APC/ìˆ˜ì¶œ)",
        "color": "#6d28d9",
        "queries": [
            "APC ìŠ¤ë§ˆíŠ¸í™” AI ì„ ë³„ê¸°",
            "ë†í˜‘ APC ì„ ë³„ ì €ì¥",
            "CAì €ì¥ APC",
            "ë†ì‹í’ˆ ìˆ˜ì¶œ ì‹¤ì  ë°° ë”¸ê¸°",
            "ê°€ë½ì‹œì¥ ê²½ë§¤ ì¬ê°œ ì¼ì •",
            "ì›ì‚°ì§€ ë‹¨ì† ë†ì‚°ë¬¼ ë¶€ì •ìœ í†µ",
        ],
        "must_terms": ["APC", "ì„ ë³„", "CAì €ì¥", "ê³µíŒì¥", "ë„ë§¤ì‹œì¥", "ê°€ë½ì‹œì¥", "ìˆ˜ì¶œ", "ì›ì‚°ì§€", "ìœ í†µ", "ê²€ì—­"],
    },
]

POLICY_DOMAINS = {
    "korea.kr", "www.korea.kr",
    "mafra.go.kr", "www.mafra.go.kr",
    "at.or.kr", "www.at.or.kr",
    "naqs.go.kr", "www.naqs.go.kr",
    "krei.re.kr", "www.krei.re.kr",
}

AGRI_POLICY_KEYWORDS = [
    "ë†ì¶•ìˆ˜ì‚°ë¬¼", "ë†ì¶•ì‚°ë¬¼", "ì„±ìˆ˜í’ˆ", "í• ì¸ì§€ì›", "í• ë‹¹ê´€ì„¸", "ê²€ì—­", "ìˆ˜ê¸‰", "ê°€ê²©", "ê³¼ì¼", "ë¹„ì¶•ë¯¸", "ì›ì‚°ì§€"
]


# -----------------------------
# Data model
# -----------------------------
@dataclass
class Article:
    section: str
    title: str
    description: str
    link: str
    originallink: str
    pub_dt_kst: datetime
    domain: str
    press: str
    norm_key: str
    score: float = 0.0
    summary: str = ""


# -----------------------------
# Utilities
# -----------------------------
def now_kst() -> datetime:
    return datetime.now(tz=KST)

def dt_kst(d: date, hour: int) -> datetime:
    return datetime(d.year, d.month, d.day, hour, 0, 0, tzinfo=KST)

def parse_pubdate_to_kst(pubdate_str: str) -> datetime:
    try:
        dt = parsedate_to_datetime(pubdate_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(KST)
    except Exception:
        return datetime.min.replace(tzinfo=KST)

def clean_text(s: str) -> str:
    if not s:
        return ""
    s = html.unescape(s)
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def domain_of(url: str) -> str:
    try:
        return (urlparse(url).netloc or "").lower()
    except Exception:
        return ""

def strip_tracking_params(url: str) -> str:
    try:
        u = urlparse(url)
        q = [(k, v) for (k, v) in parse_qsl(u.query, keep_blank_values=True)
             if not k.lower().startswith("utm_") and k.lower() not in ("gclid", "fbclid", "igshid", "ref")]
        new_q = urlencode(q, doseq=True)
        return urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))
    except Exception:
        return url

def norm_title_key(title: str) -> str:
    t = title.lower()
    t = re.sub(r"\[[^\]]+\]", " ", t)
    t = re.sub(r"[^0-9a-zê°€-í£]+", "", t)
    return t[:80]

def make_norm_key(originallink: str, link: str, title: str) -> str:
    u = strip_tracking_params(originallink or link or "")
    if u:
        h = hashlib.sha1(u.encode("utf-8")).hexdigest()[:16]
        return f"url:{h}"
    return f"title:{norm_title_key(title)}"

def has_any(text: str, words) -> bool:
    return any(w in text for w in words)

def count_any(text: str, words) -> int:
    return sum(1 for w in words if w in text)

def simplify_domain_for_press(dom: str) -> str:
    """
    ë„ë©”ì¸ë°–ì— ëª¨ë¥´ëŠ” ê²½ìš°ë¼ë„ (www ì œê±°, ë„ˆë¬´ ì§€ì €ë¶„í•˜ì§€ ì•Šê²Œ) í‘œì‹œìš© pressë¥¼ ë§Œë“ ë‹¤.
    ì˜ˆ: www.mbn.co.kr -> mbn
    ì˜ˆ: news.mt.co.kr -> mt
    """
    d = (dom or "").lower()
    if not d:
        return "ì•Œìˆ˜ì—†ìŒ"
    d = d.replace("www.", "")
    parts = d.split(".")
    if len(parts) >= 2:
        return parts[-2].upper() if len(parts[-2]) <= 5 else parts[-2]
    return d


# -----------------------------
# KR business day / holidays
# -----------------------------
def is_weekend(d: date) -> bool:
    return d.weekday() >= 5

def is_korean_holiday(d: date) -> bool:
    s = d.isoformat()
    if s in EXCLUDE_HOLIDAYS:
        return False
    if s in EXTRA_HOLIDAYS:
        return True
    try:
        import holidays  # type: ignore
        kr = holidays.KR(years=[d.year], observed=True)
        return d in kr
    except Exception:
        return False

def is_business_day_kr(d: date) -> bool:
    if is_weekend(d):
        return False
    if is_korean_holiday(d):
        return False
    return True

def previous_business_day(d: date) -> date:
    cur = d - timedelta(days=1)
    while not is_business_day_kr(cur):
        cur -= timedelta(days=1)
    return cur


# -----------------------------
# GitHub Contents API helpers
# -----------------------------
def github_api_headers(token: str):
    return {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github+json",
        "User-Agent": "agri-news-brief-bot",
    }

def github_get_file(repo: str, path: str, token: str, ref: str = "main"):
    url = f"https://api.github.com/repos/{repo}/contents/{path}"
    r = requests.get(url, headers=github_api_headers(token), params={"ref": ref}, timeout=30)
    if r.status_code == 404:
        return None, None
    if not r.ok:
        log.error("[GitHub GET ERROR] %s", r.text)
        r.raise_for_status()
    j = r.json()
    content_b64 = j.get("content", "")
    sha = j.get("sha")
    raw = base64.b64decode(content_b64).decode("utf-8", errors="replace") if content_b64 else ""
    return raw, sha

def github_put_file(repo: str, path: str, content: str, token: str, message: str, sha: str = None, branch: str = "main"):
    url = f"https://api.github.com/repos/{repo}/contents/{path}"
    payload = {
        "message": message,
        "content": base64.b64encode(content.encode("utf-8")).decode("utf-8"),
        "branch": branch,
    }
    if sha:
        payload["sha"] = sha
    r = requests.put(url, headers=github_api_headers(token), json=payload, timeout=30)
    if not r.ok:
        log.error("[GitHub PUT ERROR] %s", r.text)
        r.raise_for_status()
    return r.json()


# -----------------------------
# State / archive manifest (legacy-safe)
# -----------------------------
def load_state(repo: str, token: str):
    raw, _sha = github_get_file(repo, STATE_FILE_PATH, token, ref="main")
    if not raw:
        return {"last_end_iso": None}
    try:
        obj = json.loads(raw)
        return obj if isinstance(obj, dict) else {"last_end_iso": None}
    except Exception:
        return {"last_end_iso": None}

def save_state(repo: str, token: str, last_end: datetime):
    payload = {"last_end_iso": last_end.isoformat()}
    _raw_old, sha = github_get_file(repo, STATE_FILE_PATH, token, ref="main")
    github_put_file(repo, STATE_FILE_PATH, json.dumps(payload, ensure_ascii=False, indent=2), token,
                    f"Update state {last_end.date().isoformat()}", sha=sha, branch="main")

def _normalize_manifest(obj):
    if obj is None:
        return {"dates": []}
    if isinstance(obj, list):
        return {"dates": [str(x) for x in obj if str(x).strip()]}
    if isinstance(obj, dict):
        dates = obj.get("dates", [])
        if isinstance(dates, list):
            return {"dates": [str(x) for x in dates if str(x).strip()]}
        if isinstance(dates, str) and dates.strip():
            return {"dates": [dates.strip()]}
        return {"dates": []}
    return {"dates": []}

def load_archive_manifest(repo: str, token: str):
    raw, sha = github_get_file(repo, ARCHIVE_MANIFEST_PATH, token, ref="main")
    if not raw:
        return {"dates": []}, sha
    try:
        return _normalize_manifest(json.loads(raw)), sha
    except Exception:
        return {"dates": []}, sha

def save_archive_manifest(repo: str, token: str, manifest: dict, sha: str):
    manifest = _normalize_manifest(manifest)
    github_put_file(repo, ARCHIVE_MANIFEST_PATH, json.dumps(manifest, ensure_ascii=False, indent=2), token,
                    "Update archive manifest", sha=sha, branch="main")


# -----------------------------
# Naver News search
# -----------------------------
def naver_news_search(query: str, display: int = 30, start: int = 1, sort: str = "date"):
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        raise RuntimeError("NAVER_CLIENT_ID / NAVER_CLIENT_SECRET not set")
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    params = {"query": query, "display": display, "start": start, "sort": sort}
    r = requests.get(url, headers=headers, params=params, timeout=30)
    if not r.ok:
        log.error("[NAVER ERROR] %s", r.text)
        r.raise_for_status()
    return r.json()


# -----------------------------
# Relevance scoring / filtering
# -----------------------------
def is_blocked_domain(dom: str) -> bool:
    if not dom:
        return False
    dom = dom.lower()
    if dom in BLOCKED_DOMAINS:
        return True
    for b in BLOCKED_DOMAINS:
        if dom.endswith("." + b):
            return True
    return False

def agri_strength_score(text: str) -> int:
    return count_any(text, AGRI_STRONG_TERMS)

def off_topic_penalty(text: str) -> int:
    return count_any(text, OFFTOPIC_HINTS)

def travel_penalty(text: str) -> int:
    return count_any(text, TRAVEL_MARKET_HINTS)

def korea_context_score(text: str) -> int:
    return count_any(text, KOREA_CONTEXT_HINTS)

def section_must_terms_ok(text: str, must_terms) -> bool:
    return has_any(text, must_terms)

def policy_domain_override(dom: str, text: str) -> bool:
    if dom in POLICY_DOMAINS:
        return has_any(text, AGRI_POLICY_KEYWORDS)
    return False

def is_relevant(article: Article, section_conf: dict) -> bool:
    dom = article.domain
    if is_blocked_domain(dom):
        return False

    text = (article.title + " " + article.description).lower()

    # must_terms gate (policy domains can override)
    if not section_must_terms_ok(text, [t.lower() for t in section_conf["must_terms"]]):
        if not policy_domain_override(dom, text):
            return False

    strength = agri_strength_score(text)
    offp = off_topic_penalty(text)
    trav = travel_penalty(text)
    korea = korea_context_score(text)

    if trav >= 1 and korea == 0 and strength < 3:
        return False
    if offp >= 1 and strength < 3:
        return False

    # disambiguation: "ì‚¬ê³¼" apology
    if re.search(r"(ê³µê°œ\s*)?ì‚¬ê³¼(í–ˆë‹¤|í•´ì•¼|í•˜ë¼|ë¬¸|ìš”êµ¬|ìš”ì²­|ë°œí‘œ)", article.title) and strength < 4:
        return False

    # disambiguation: "ë°°" ship
    if re.search(r"(ì„ ë°•|í•´êµ°|í•­ë§Œ|ì¡°ì„ |í•¨ì •|ìŠ¹ì„ |í•­í•´)", text) and strength < 4:
        return False

    return True

def compute_rank_score(article: Article, section_conf: dict) -> float:
    text = (article.title + " " + article.description).lower()
    strength = agri_strength_score(text)
    korea = korea_context_score(text)
    offp = off_topic_penalty(text)
    trav = travel_penalty(text)

    score = 0.0
    score += strength * 2.0
    score += korea * 0.8
    score -= offp * 2.5
    score -= trav * 2.0

    if article.domain in POLICY_DOMAINS:
        score += 3.0

    age_hours = max(0.0, (datetime.now(tz=KST) - article.pub_dt_kst).total_seconds() / 3600.0)
    score += max(0.0, 24.0 - min(age_hours, 24.0)) * 0.05

    for t in section_conf["must_terms"]:
        if t.lower() in article.title.lower():
            score += 0.6

    return score


# -----------------------------
# Press mapping
# -----------------------------
PRESS_MAP = {
    # national
    "www.yna.co.kr": "ì—°í•©ë‰´ìŠ¤", "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "www.mk.co.kr": "ë§¤ì¼ê²½ì œ", "mk.co.kr": "ë§¤ì¼ê²½ì œ",
    "www.joongang.co.kr": "ì¤‘ì•™ì¼ë³´", "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
    "www.chosun.com": "ì¡°ì„ ì¼ë³´", "chosun.com": "ì¡°ì„ ì¼ë³´",
    "www.donga.com": "ë™ì•„ì¼ë³´", "donga.com": "ë™ì•„ì¼ë³´",
    "www.hani.co.kr": "í•œê²¨ë ˆ", "hani.co.kr": "í•œê²¨ë ˆ",
    "www.khan.co.kr": "ê²½í–¥ì‹ ë¬¸", "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
    "www.sedaily.com": "ì„œìš¸ê²½ì œ", "sedaily.com": "ì„œìš¸ê²½ì œ",
    "www.hankyung.com": "í•œêµ­ê²½ì œ", "hankyung.com": "í•œêµ­ê²½ì œ",
    "www.asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ", "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
    "www.mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´", "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
    "www.edaily.co.kr": "ì´ë°ì¼ë¦¬", "edaily.co.kr": "ì´ë°ì¼ë¦¬",
    "www.heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ", "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
    "www.fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤", "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "www.newsis.com": "ë‰´ì‹œìŠ¤", "newsis.com": "ë‰´ì‹œìŠ¤",
    "www.news1.kr": "ë‰´ìŠ¤1", "news1.kr": "ë‰´ìŠ¤1",

    # broadcast / mid-tier
    "www.mbn.co.kr": "MBN", "mbn.co.kr": "MBN",
    "news.sbs.co.kr": "SBS", "www.sbs.co.kr": "SBS", "sbs.co.kr": "SBS",
    "news.kbs.co.kr": "KBS", "www.kbs.co.kr": "KBS", "kbs.co.kr": "KBS",
    "imnews.imbc.com": "MBC", "www.imbc.com": "MBC", "imbc.com": "MBC",
    "www.ytn.co.kr": "YTN", "ytn.co.kr": "YTN",
    "news.jtbc.co.kr": "JTBC", "jtbc.co.kr": "JTBC", "www.jtbc.co.kr": "JTBC",

    # policy
    "www.korea.kr": "ì •ì±…ë¸Œë¦¬í•‘", "korea.kr": "ì •ì±…ë¸Œë¦¬í•‘",
    "www.mafra.go.kr": "ë†ì‹í’ˆë¶€", "mafra.go.kr": "ë†ì‹í’ˆë¶€",
    "www.at.or.kr": "aT", "at.or.kr": "aT",
    "www.naqs.go.kr": "ë†ê´€ì›", "naqs.go.kr": "ë†ê´€ì›",
}

CENTRAL_PRESS_NAMES = {
    "ì—°í•©ë‰´ìŠ¤", "ë§¤ì¼ê²½ì œ", "ì¤‘ì•™ì¼ë³´", "ì¡°ì„ ì¼ë³´", "ë™ì•„ì¼ë³´", "í•œê²¨ë ˆ", "ê²½í–¥ì‹ ë¬¸",
    "ì„œìš¸ê²½ì œ", "í•œêµ­ê²½ì œ", "ì•„ì‹œì•„ê²½ì œ", "ë¨¸ë‹ˆíˆ¬ë°ì´", "í—¤ëŸ´ë“œê²½ì œ", "ì´ë°ì¼ë¦¬",
    "ë‰´ì‹œìŠ¤", "ë‰´ìŠ¤1", "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "SBS", "KBS", "MBC", "YTN", "JTBC", "MBN",
    "ì •ì±…ë¸Œë¦¬í•‘", "ë†ì‹í’ˆë¶€", "aT", "ë†ê´€ì›",
}

def press_tier(press: str, domain: str) -> str:
    """
    ì¤‘ì•™/ì§€ë°© ì§‘ê³„ìš© (ëŒ€ëµì ì¸ ë¶„ë¥˜)
    - ì¤‘ì•™: ì¤‘ì•™/ë°©ì†¡/ì •ì±…ê¸°ê´€
    - ê·¸ ì™¸ëŠ” ì§€ë°©ìœ¼ë¡œ ì§‘ê³„ (í•©ê³„ê°€ totalê³¼ ë§ë„ë¡)
    """
    p = (press or "").strip()
    d = (domain or "").lower()
    if p in CENTRAL_PRESS_NAMES:
        return "central"
    if d in POLICY_DOMAINS or d.endswith(".go.kr"):
        return "central"
    return "local"


# -----------------------------
# Collect articles
# -----------------------------
def collect_articles_for_section(section_conf: dict, start_kst: datetime, end_kst: datetime):
    items: list[Article] = []
    seen_keys = set()
    display = 40

    for q in section_conf["queries"]:
        try:
            data = naver_news_search(q, display=display, start=1, sort="date")
            for it in data.get("items", []):
                title = clean_text(it.get("title", ""))
                desc = clean_text(it.get("description", ""))
                link = strip_tracking_params(it.get("link", "") or "")
                origin = strip_tracking_params(it.get("originallink", "") or link)
                pub = parse_pubdate_to_kst(it.get("pubDate", ""))

                if pub < start_kst or pub >= end_kst:
                    continue

                dom = domain_of(origin) or domain_of(link)
                if is_blocked_domain(dom):
                    continue

                press = PRESS_MAP.get(dom)
                if not press:
                    press = simplify_domain_for_press(dom)

                norm_key = make_norm_key(origin, link, title)
                if norm_key in seen_keys:
                    continue

                art = Article(
                    section=section_conf["key"],
                    title=title,
                    description=desc,
                    link=link,
                    originallink=origin,
                    pub_dt_kst=pub,
                    domain=dom,
                    press=press,
                    norm_key=norm_key,
                )

                if not is_relevant(art, section_conf):
                    continue

                art.score = compute_rank_score(art, section_conf)
                seen_keys.add(norm_key)
                items.append(art)

        except Exception as e:
            log.warning("[WARN] query failed: %s (%s)", q, e)

    items.sort(key=lambda a: (a.score, a.pub_dt_kst), reverse=True)
    return items[:MAX_PER_SECTION]

def collect_all_sections(start_kst: datetime, end_kst: datetime):
    by_section: dict[str, list[Article]] = {}
    for sec in SECTIONS:
        by_section[sec["key"]] = collect_articles_for_section(sec, start_kst, end_kst)

    # broad fill if too few
    for sec in SECTIONS:
        key = sec["key"]
        if len(by_section[key]) >= MIN_PER_SECTION:
            continue

        if key == "supply":
            broad_queries = ["ë†ì‚°ë¬¼ ê°€ê²©", "ê³¼ì¼ ì‹œì„¸", "ë„ë§¤ì‹œì¥ ì‹œì„¸", "ì‚°ì§€ ì¶œí•˜"]
        elif key == "policy":
            broad_queries = ["ë†ì¶•ìˆ˜ì‚°ë¬¼ í• ì¸", "ë†ì‚°ë¬¼ ë¬¼ê°€ ëŒ€ì±…", "í• ë‹¹ê´€ì„¸ ê³¼ì¼"]
        elif key == "pest":
            broad_queries = ["ê³¼ìˆ˜ ë°©ì œ ì•½ì œ", "ê³¼ìˆ˜í™”ìƒë³‘ ë°©ì œ", "ì›”ë™ í•´ì¶© ë°©ì œ"]
        else:
            broad_queries = ["APC ì„ ë³„", "ë†ì‹í’ˆ ìˆ˜ì¶œ ì‹¤ì ", "ê°€ë½ì‹œì¥ ê²½ë§¤"]

        tmp = dict(sec)
        tmp["queries"] = broad_queries

        extra = collect_articles_for_section(tmp, start_kst, end_kst)
        merged = {a.norm_key: a for a in by_section[key]}
        for a in extra:
            merged.setdefault(a.norm_key, a)

        merged_list = list(merged.values())
        merged_list.sort(key=lambda a: (a.score, a.pub_dt_kst), reverse=True)
        by_section[key] = merged_list[:MAX_PER_SECTION]

    return by_section


# -----------------------------
# OpenAI summaries (optional)
# -----------------------------
def openai_extract_text(resp_json: dict) -> str:
    try:
        out = resp_json.get("output", [])
        if not out:
            return ""
        for block in out:
            for c in block.get("content", []):
                if c.get("type") in ("output_text", "text") and "text" in c:
                    return c["text"]
        return ""
    except Exception:
        return ""

def openai_summarize_batch(articles: list[Article]) -> dict:
    if not OPENAI_API_KEY or not articles:
        return {}

    rows = []
    for a in articles:
        rows.append({
            "id": a.norm_key,
            "press": a.press,
            "title": a.title[:180],
            "desc": a.description[:260],
            "section": a.section,
            "url": a.originallink or a.link,
        })

    system = (
        "ë„ˆëŠ” ë†í˜‘ ê²½ì œì§€ì£¼ ì›ì˜ˆìˆ˜ê¸‰ë¶€(ê³¼ìˆ˜í™”í›¼) ì‹¤ë¬´ìë¥¼ ìœ„í•œ 'ë†ì‚°ë¬¼ ë‰´ìŠ¤ ìš”ì•½ê°€'ë‹¤.\n"
        "- ì ˆëŒ€ ìƒìƒ/ì¶”ì •ìœ¼ë¡œ ì‚¬ì‹¤ì„ ë§Œë“¤ì§€ ë§ˆë¼.\n"
        "- ê° ê¸°ì‚¬ ìš”ì•½ì€ 2~3ë¬¸ì¥, 120~220ì ë‚´. í•µì‹¬ íŒ©íŠ¸ ì¤‘ì‹¬.\n"
        "ì¶œë ¥ í˜•ì‹: ê° ì¤„ 'id\\tìš”ì•½' í˜•íƒœë¡œë§Œ ì¶œë ¥."
    )
    user = "ê¸°ì‚¬ ëª©ë¡(JSON):\n" + json.dumps(rows, ensure_ascii=False)

    payload = {
        "model": OPENAI_MODEL,
        "input": [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
    }

    try:
        r = requests.post(
            "https://api.openai.com/v1/responses",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"},
            json=payload,
            timeout=60,
        )
        if not r.ok:
            try:
                body = r.json()
            except Exception:
                body = {"raw": r.text}
            msg = (body.get("error") or {}).get("message") or r.text
            code = (body.get("error") or {}).get("code") or str(r.status_code)
            log.warning("[OpenAI] summarize skipped (%s): %s", code, msg)
            return {}

        text = openai_extract_text(r.json()).strip()
        out = {}
        for line in text.splitlines():
            if "\t" not in line:
                continue
            k, v = line.split("\t", 1)
            k = k.strip()
            v = v.strip()
            if k:
                out[k] = v
        return out

    except Exception as e:
        log.warning("[OpenAI] summarize failed, fallback: %s", e)
        return {}

def fill_summaries(by_section: dict):
    all_articles: list[Article] = []
    for sec in SECTIONS:
        all_articles.extend(by_section.get(sec["key"], []))

    mapping = openai_summarize_batch(all_articles)

    for a in all_articles:
        s = mapping.get(a.norm_key, "").strip()
        if not s:
            s = a.description.strip() or a.title.strip()
        a.summary = s
    return by_section


# -----------------------------
# Rendering (HTML)
# -----------------------------
def esc(s: str) -> str:
    return html.escape(s or "")

def fmt_dt(dt_: datetime) -> str:
    return dt_.strftime("%m/%d %H:%M")

def render_daily_page(report_date: str, start_kst: datetime, end_kst: datetime, by_section: dict, base_url: str) -> str:
    chips = []
    total = 0
    for sec in SECTIONS:
        n = len(by_section.get(sec["key"], []))
        total += n
        chips.append((sec["key"], sec["title"], n, sec["color"]))

    def chip_html(k, title, n, color):
        return (
            f'<a class="chip" style="border-color:{color};color:{color}" href="#sec-{k}">'
            f'{esc(title)} <span class="chipN">{n}</span></a>'
        )

    chips_html = "\n".join([chip_html(*c) for c in chips])

    sections_html = []
    for sec in SECTIONS:
        key = sec["key"]
        title = sec["title"]
        color = sec["color"]
        lst = by_section.get(key, [])
        cards = []
        for a in lst:
            url = a.originallink or a.link
            summary_html = "<br>".join(esc(a.summary).splitlines())
            cards.append(
                f"""
                <div class="card" style="border-left-color:{color}">
                  <div class="meta">
                    <span class="press">{esc(a.press)}</span>
                    <span class="dot">Â·</span>
                    <span class="time">{esc(fmt_dt(a.pub_dt_kst))}</span>
                  </div>
                  <div class="ttl">{esc(a.title)}</div>
                  <div class="sum">{summary_html}</div>
                  <div class="lnk"><a href="{esc(url)}" target="_blank" rel="noopener">ì›ë¬¸ ì—´ê¸°</a></div>
                </div>
                """
            )
        cards_html = '<div class="empty">íŠ¹ì´ì‚¬í•­ ì—†ìŒ</div>' if not cards else "\n".join(cards)

        sections_html.append(
            f"""
            <section id="sec-{key}" class="sec">
              <div class="secHead" style="background:linear-gradient(90deg,{color},#111827);">
                <div class="secTitle">{esc(title)}</div>
                <div class="secCount">{len(lst)}ê±´</div>
              </div>
              <div class="secBody">{cards_html}</div>
            </section>
            """
        )

    sections_html = "\n".join(sections_html)

    title = f"[{report_date} ë†ì‚°ë¬¼ ë‰´ìŠ¤ Brief]"
    period = f"{start_kst.strftime('%Y-%m-%d %H:%M')} ~ {end_kst.strftime('%Y-%m-%d %H:%M')}"
    index_url = f"{base_url}/"

    return f"""<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>{esc(title)}</title>
  <style>
    :root {{
      --bg:#0b1220; --text:#e5e7eb; --muted:#94a3b8; --line:#1f2937;
    }}
    *{{box-sizing:border-box}}
    body{{margin:0;background:radial-gradient(1200px 600px at 20% 10%, #111827, var(--bg)); color:var(--text);
         font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Noto Sans KR", Arial;}}
    .wrap{{max-width:1100px;margin:0 auto;padding:22px 16px 80px;}}
    .topbar{{display:flex;gap:10px;align-items:center;justify-content:space-between;flex-wrap:wrap}}
    h1{{margin:0;font-size:20px;letter-spacing:-0.2px}}
    .sub{{color:var(--muted);font-size:13px;margin-top:6px}}
    .nav a{{color:#cbd5e1;text-decoration:none;font-size:13px;border:1px solid var(--line);padding:8px 10px;border-radius:10px;background:rgba(255,255,255,0.02)}}
    .chips{{display:flex;gap:8px;flex-wrap:wrap;margin-top:14px}}
    .chip{{text-decoration:none;border:1px solid var(--line);padding:8px 10px;border-radius:999px;
          background:rgba(255,255,255,0.02);font-size:13px}}
    .chipN{{margin-left:6px;background:rgba(255,255,255,0.08);padding:2px 8px;border-radius:999px;color:var(--text)}}
    .sec{{margin-top:18px;border:1px solid var(--line);border-radius:16px;overflow:hidden;background:rgba(255,255,255,0.02)}}
    .secHead{{display:flex;align-items:center;justify-content:space-between;padding:12px 14px}}
    .secTitle{{font-size:15px;font-weight:700}}
    .secCount{{font-size:13px;color:#e2e8f0;background:rgba(0,0,0,0.25);padding:4px 10px;border-radius:999px}}
    .secBody{{padding:12px 12px 14px}}
    .card{{background:rgba(15,23,42,0.55);border:1px solid var(--line);border-left:4px solid #334155;
          border-radius:14px;padding:12px;margin:10px 0}}
    .meta{{color:var(--muted);font-size:12px;display:flex;align-items:center;gap:6px}}
    .press{{color:#e2e8f0}}
    .dot{{opacity:.6}}
    .ttl{{margin-top:6px;font-size:15px;line-height:1.35}}
    .sum{{margin-top:8px;color:#cbd5e1;font-size:13px;line-height:1.55}}
    .lnk{{margin-top:10px}}
    .lnk a{{display:inline-block;color:#e5e7eb;text-decoration:none;border:1px solid var(--line);
           padding:8px 10px;border-radius:10px;background:rgba(255,255,255,0.03)}}
    .empty{{color:var(--muted);font-size:13px;padding:10px 2px}}
    .footer{{margin-top:22px;color:var(--muted);font-size:12px}}
  </style>
</head>
<body>
  <div class="wrap">
    <div class="topbar">
      <div>
        <h1>{esc(title)}</h1>
        <div class="sub">ê¸°ê°„: {esc(period)} Â· ê¸°ì‚¬ {total}ê±´</div>
      </div>
      <div class="nav">
        <a href="{esc(index_url)}">ìµœì‹ /ì•„ì¹´ì´ë¸Œ</a>
      </div>
    </div>

    <div class="chips">{chips_html}</div>

    {sections_html}

    <div class="footer">
      * ìë™ ìˆ˜ì§‘ ê²°ê³¼ì´ë©°, ì œëª©/ìš”ì•½ì€ ì›ë¬¸ ê¸°ë°˜ ì •ë¦¬ì…ë‹ˆë‹¤. (í•„ìš” ì‹œ ì›ë¬¸ í™•ì¸)
    </div>
  </div>
</body>
</html>
"""

def render_index_page(manifest: dict, base_url: str) -> str:
    manifest = _normalize_manifest(manifest)
    dates = sorted(manifest.get("dates", []), reverse=True)
    latest = dates[0] if dates else None

    items_html = []
    for d in dates[:30]:
        url = f"{base_url}/archive/{d}.html"
        items_html.append(f'<li><a href="{esc(url)}">{esc(d)}</a></li>')
    ul = "\n".join(items_html) if items_html else "<li>ì•„ì¹´ì´ë¸Œê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.</li>"

    latest_link = f"{base_url}/archive/{latest}.html" if latest else base_url

    return f"""<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ë†ì‚°ë¬¼ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</title>
  <style>
    body{{margin:0;background:#0b1220;color:#e5e7eb;font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Noto Sans KR", Arial;}}
    .wrap{{max-width:900px;margin:0 auto;padding:24px 16px 60px}}
    h1{{margin:0;font-size:22px}}
    .sub{{color:#94a3b8;margin-top:8px;font-size:13px}}
    .btn{{display:inline-block;margin-top:14px;text-decoration:none;color:#e5e7eb;border:1px solid #1f2937;
         padding:10px 12px;border-radius:12px;background:rgba(255,255,255,0.03)}}
    .panel{{margin-top:18px;border:1px solid #1f2937;border-radius:16px;background:rgba(255,255,255,0.02);padding:14px}}
    ul{{margin:10px 0 0 18px}}
    a{{color:#cbd5e1}}
  </style>
</head>
<body>
  <div class="wrap">
    <h1>ë†ì‚°ë¬¼ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</h1>
    <div class="sub">ìµœì‹  ë¸Œë¦¬í•‘ê³¼ ë‚ ì§œë³„ ì•„ì¹´ì´ë¸Œë¥¼ ì œê³µí•©ë‹ˆë‹¤.</div>

    <a class="btn" href="{esc(latest_link)}">ìµœì‹  ë¸Œë¦¬í•‘ ì—´ê¸°</a>

    <div class="panel">
      <div style="font-weight:700;margin-bottom:6px;">ë‚ ì§œë³„ ì•„ì¹´ì´ë¸Œ</div>
      <ul>{ul}</ul>
    </div>
  </div>
</body>
</html>
"""


# -----------------------------
# Pages URL (anti-gist + safer)
# -----------------------------
def get_pages_base_url(repo: str) -> str:
    """
    base_url ê²°ì • ë¡œì§(ì•ˆì „ ê°•í™”):
    - PAGES_BASE_URLì´ ì—†ìœ¼ë©´ ê¸°ë³¸ GitHub Pagesë¡œ
    - PAGES_BASE_URLì´ gist/raw ë“± ì˜ì‹¬ ë„ë©”ì¸ì´ë©´ ë¬´ì‹œí•˜ê³  ê¸°ë³¸ URLë¡œ
    """
    owner, name = repo.split("/", 1)
    default_url = f"https://{owner.lower()}.github.io/{name}".rstrip("/")

    env_url = os.getenv("PAGES_BASE_URL", "").strip().rstrip("/")
    if not env_url:
        return default_url

    bad = ("gist.github.com", "raw.githubusercontent.com")
    if any(b in env_url for b in bad):
        log.warning("[WARN] PAGES_BASE_URL points to gist/raw. Ignoring and using default: %s", default_url)
        return default_url

    if not env_url.startswith("http://") and not env_url.startswith("https://"):
        log.warning("[WARN] PAGES_BASE_URL invalid (no http/https). Ignoring and using default: %s", default_url)
        return default_url

    return env_url


def log_kakao_domain_requirement(daily_url: str):
    """
    âœ… 'ë¸Œë¦¬í•‘ ì—´ê¸°'ê°€ gistë¡œ ì—´ë¦¬ëŠ” ëŒ€í‘œ ì›ì¸:
    - ì¹´ì¹´ì˜¤ ê°œë°œì ì½˜ì†” > í”Œë«í¼ > Web > ì‚¬ì´íŠ¸ ë„ë©”ì¸ì—
      GitHub Pages ë„ë©”ì¸(ì˜ˆ: hongtaehwa.github.io)ì´ ë“±ë¡ë˜ì–´ ìˆì§€ ì•ŠìŒ
    ì´ ê²½ìš° ì¹´ì¹´ì˜¤ê°€ ë§í¬ë¥¼ ì •ìƒ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ê³ , ì´ë¯¸ ë“±ë¡ëœ ë‹¤ë¥¸ ë„ë©”ì¸(ì˜ˆ: gist)ë¡œ ì—´ì–´ë²„ë¦´ ìˆ˜ ìˆìŒ.
    => ëŸ° ë¡œê·¸ì— ë“±ë¡í•´ì•¼ í•  ë„ë©”ì¸ì„ ì •í™•íˆ í‘œì‹œ.
    """
    dom = domain_of(daily_url)
    if not dom:
        return
    # github pages / custom domain ëª¨ë‘ì— ëŒ€í•´ ì•ˆë‚´
    log.info("[KAKAO LINK CHECK] daily_url domain=%s", dom)
    log.info("[KAKAO LINK CHECK] If 'ë¸Œë¦¬í•‘ ì—´ê¸°' opens wrong site, add this domain to Kakao Dev Console:")
    log.info("[KAKAO LINK CHECK] Kakao Developers > ë‚´ ì• í”Œë¦¬ì¼€ì´ì…˜ > ì•± ì„¤ì • > í”Œë«í¼ > Web > ì‚¬ì´íŠ¸ ë„ë©”ì¸ : %s", dom)


def ensure_not_gist(url: str, label: str):
    if "gist.github.com" in url or "raw.githubusercontent.com" in url:
        raise RuntimeError(f"[FATAL] {label} points to gist/raw: {url}")


# -----------------------------
# Kakao message builder (compact, press in parentheses)
# -----------------------------
# ì¹´í†¡ ë©”ì‹œì§€ ì„¹ì…˜ ìˆœì„œ(ìš”ì²­ ê³ ì •): í’ˆëª© â†’ ì •ì±… â†’ ìœ í†µ â†’ ë°©ì œ
KAKAO_MESSAGE_SECTION_ORDER = ["supply", "policy", "dist", "pest"]

def _get_section_conf(key: str):
    for s in SECTIONS:
        if s["key"] == key:
            return s
    return None

def build_kakao_message(report_date: str, by_section: dict) -> str:
    """
    ìš”êµ¬ì‚¬í•­ ë°˜ì˜:
    - í•­ëª© ê°„ì—ë§Œ ë¹ˆ ì¤„ 1ê°œ
    - í•­ëª© ë‚´ë¶€ëŠ” ì¤„ë°”ê¿ˆë§Œ
    - (ë§¤ì²´ëª…) ê¸°ì‚¬ì œëª©
    """
    total = 0
    central = 0
    local = 0
    per = {"supply": 0, "policy": 0, "pest": 0, "dist": 0}

    for key in per.keys():
        lst = by_section.get(key, [])
        per[key] = len(lst)
        total += len(lst)
        for a in lst:
            if press_tier(a.press, a.domain) == "central":
                central += 1
            else:
                local += 1

    lines = []
    lines.append(f"[{report_date} ë†ì‚°ë¬¼ ë‰´ìŠ¤ Brief]")
    lines.append("")  # ë¸”ë¡ ê°„ 1ì¤„

    lines.append(f"ê¸°ì‚¬ : ì´ {total}ê±´ (ì¤‘ì•™ {central}ê±´, ì§€ë°© {local}ê±´)")
    lines.append(f"- í’ˆëª© {per['supply']} Â· ì •ì±… {per['policy']} Â· ë°©ì œ {per['pest']} Â· ìœ í†µ {per['dist']}")
    lines.append("")  # ë¸”ë¡ ê°„ 1ì¤„

    lines.append("ì˜¤ëŠ˜ì˜ ì²´í¬í¬ì¸íŠ¸")
    lines.append("")  # ë¸”ë¡ ê°„ 1ì¤„

    section_num = 0
    for key in KAKAO_MESSAGE_SECTION_ORDER:
        conf = _get_section_conf(key)
        if not conf:
            continue
        section_num += 1

        lines.append(f"{section_num}) {conf['title']}")

        items = by_section.get(key, [])[:2]
        if not items:
            lines.append("   - (ê¸°ì‚¬ ì—†ìŒ)")
        else:
            for a in items:
                # (ë§¤ì²´ëª…) ê¸°ì‚¬ì œëª©
                press = (a.press or "").strip()
                if not press:
                    press = simplify_domain_for_press(a.domain)
                lines.append(f"   - ({press}) {a.title}")

        lines.append("")  # ì„¹ì…˜(í•­ëª©) ê°„ 1ì¤„

    # ë§ˆì§€ë§‰ ë¹ˆ ì¤„ í•˜ë‚˜ ì œê±°(ê°€ë…ì„±)
    while lines and lines[-1] == "":
        lines.pop()

    lines.append("")
    lines.append("ğŸ‘‰ 'ë¸Œë¦¬í•‘ ì—´ê¸°'ì—ì„œ ì„¹ì…˜ë³„ ê¸°ì‚¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    return "\n".join(lines)


# -----------------------------
# Kakao API
# -----------------------------
def kakao_refresh_access_token() -> str:
    if not KAKAO_REST_API_KEY or not KAKAO_REFRESH_TOKEN:
        raise RuntimeError("KAKAO_REST_API_KEY / KAKAO_REFRESH_TOKEN not set")

    url = "https://kauth.kakao.com/oauth/token"
    data = {
        "grant_type": "refresh_token",
        "client_id": KAKAO_REST_API_KEY,
        "refresh_token": KAKAO_REFRESH_TOKEN,
    }
    if KAKAO_CLIENT_SECRET:
        data["client_secret"] = KAKAO_CLIENT_SECRET

    r = requests.post(url, data=data, timeout=30)
    if not r.ok:
        log.error("[KAKAO TOKEN ERROR] %s", r.text)
        r.raise_for_status()
    j = r.json()
    return j["access_token"]

def kakao_send_to_me(text: str, web_url: str):
    access_token = kakao_refresh_access_token()

    # âœ… ì½”ë“œìƒ web_urlì€ gistê°€ ë  ìˆ˜ ì—†ê²Œ í•œë‹¤(ì¹˜ëª… ì‚¬ê³  ë°©ì§€)
    ensure_not_gist(web_url, "Kakao web_url")

    url = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
    headers = {"Authorization": f"Bearer {access_token}"}

    # âœ… "text" í…œí”Œë¦¿: ë²„íŠ¼(ë¸Œë¦¬í•‘ ì—´ê¸°) ë° ë§í’ì„  í´ë¦­ ë§í¬ëŠ” link ê¸°ì¤€
    #    (ë³¸ë¬¸ì— URLì´ ë“¤ì–´ê°€ë©´ ë¯¸ë¦¬ë³´ê¸°/ìë™ ë§í¬ê°€ ì„ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê¸°ë³¸ false ê¶Œì¥)
    template = {
        "object_type": "text",
        "text": text,
        "link": {"web_url": web_url, "mobile_web_url": web_url},
        "button_title": "ë¸Œë¦¬í•‘ ì—´ê¸°",
    }

    r = requests.post(url, headers=headers, data={"template_object": json.dumps(template, ensure_ascii=False)}, timeout=30)
    if not r.ok:
        log.error("[KAKAO SEND ERROR] %s", r.text)
        r.raise_for_status()
    return r.json()


# -----------------------------
# Window calculation
# -----------------------------
def compute_end_kst():
    if FORCE_REPORT_DATE:
        d = datetime.strptime(FORCE_REPORT_DATE, "%Y-%m-%d").date()
        return dt_kst(d, REPORT_HOUR_KST)

    if FORCE_END_NOW:
        return now_kst()

    n = now_kst()
    candidate = n.replace(hour=REPORT_HOUR_KST, minute=0, second=0, microsecond=0)
    if n < candidate:
        candidate -= timedelta(days=1)
    return candidate

def compute_window(repo: str, token: str, end_kst: datetime):
    state = load_state(repo, token)
    last_end_iso = state.get("last_end_iso")

    prev_bd = previous_business_day(end_kst.date())
    prev_cutoff = dt_kst(prev_bd, REPORT_HOUR_KST)

    # ê¸°ë³¸: ì§ì „ ì˜ì—…ì¼ ì»·ì˜¤í”„ë¶€í„°
    start = prev_cutoff

    # ìƒíƒœ íŒŒì¼(last_end)ì´ ë” ê³¼ê±°ë¼ë©´ ë” ê³¼ê±°ë¶€í„°(ëˆ„ë½ ë°©ì§€) / ë” ìµœê·¼ì´ë©´ prev_cutoffë¡œ
    if last_end_iso:
        try:
            st = datetime.fromisoformat(last_end_iso)
            if st.tzinfo is None:
                st = st.replace(tzinfo=KST)
            # ë” ì´ë¥¸ ìª½ìœ¼ë¡œ ì„¤ì •(íœ´ì¼ ëˆ„ì /ëˆ„ë½ ë°©ì§€)
            start = min(st.astimezone(KST), prev_cutoff)
        except Exception:
            start = prev_cutoff

    if start >= end_kst:
        start = end_kst - timedelta(hours=24)

    return start, end_kst


# -----------------------------
# Main
# -----------------------------
def main():
    if not DEFAULT_REPO:
        raise RuntimeError("GITHUB_REPO or GITHUB_REPOSITORY is not set (e.g., ORGNAME/agri-news-brief)")
    if not GH_TOKEN:
        raise RuntimeError("GH_TOKEN or GITHUB_TOKEN is not set")
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        raise RuntimeError("NAVER_CLIENT_ID / NAVER_CLIENT_SECRET is not set")
    if not KAKAO_REST_API_KEY or not KAKAO_REFRESH_TOKEN:
        raise RuntimeError("KAKAO_REST_API_KEY / KAKAO_REFRESH_TOKEN is not set")

    repo = DEFAULT_REPO
    end_kst = compute_end_kst()

    is_bd = is_business_day_kr(end_kst.date())
    if (not FORCE_RUN_ANYDAY) and (not is_bd):
        log.info("[SKIP] Not a business day in KR: %s (weekend/holiday)", end_kst.date().isoformat())
        return
    if FORCE_RUN_ANYDAY and (not is_bd):
        log.info("[FORCE] Non-business day but proceeding for test: %s", end_kst.date().isoformat())

    start_kst, end_kst = compute_window(repo, GH_TOKEN, end_kst)
    log.info("[INFO] Window KST: %s ~ %s", start_kst.isoformat(), end_kst.isoformat())

    report_date = end_kst.date().isoformat()

    # âœ… base_url / daily_url (gist ì ˆëŒ€ ë¶ˆê°€)
    base_url = get_pages_base_url(repo).rstrip("/")
    daily_url = f"{base_url}/archive/{report_date}.html"

    ensure_not_gist(base_url, "base_url")
    ensure_not_gist(daily_url, "daily_url")

    # âœ… ì² ì € ì§„ë‹¨ ë¡œê·¸: ì¹´ì¹´ì˜¤ ë§í¬ ë„ë©”ì¸ ë“±ë¡ í•„ìš” ì—¬ë¶€ í™•ì¸ìš©
    log_kakao_domain_requirement(daily_url)

    # Collect + summarize
    by_section = collect_all_sections(start_kst, end_kst)
    by_section = fill_summaries(by_section)

    # Render pages
    daily_html = render_daily_page(report_date, start_kst, end_kst, by_section, base_url)

    manifest, msha = load_archive_manifest(repo, GH_TOKEN)
    manifest = _normalize_manifest(manifest)
    dates = set(manifest.get("dates", []))
    dates.add(report_date)
    manifest["dates"] = sorted(list(dates))

    index_html = render_index_page(manifest, base_url)

    # Write daily page
    daily_path = f"{DOCS_ARCHIVE_DIR}/{report_date}.html"
    _raw_old, sha_old = github_get_file(repo, daily_path, GH_TOKEN, ref="main")
    github_put_file(repo, daily_path, daily_html, GH_TOKEN, f"Add daily brief {report_date}", sha=sha_old, branch="main")

    # Write index
    _raw_old2, sha_old2 = github_get_file(repo, DOCS_INDEX_PATH, GH_TOKEN, ref="main")
    github_put_file(repo, DOCS_INDEX_PATH, index_html, GH_TOKEN, f"Update index {report_date}", sha=sha_old2, branch="main")

    # Save manifest/state
    save_archive_manifest(repo, GH_TOKEN, manifest, msha)
    save_state(repo, GH_TOKEN, end_kst)

    # Kakao message (compact & readable)
    kakao_text = build_kakao_message(report_date, by_section)

    # ë³¸ë¬¸ì— URL ë„£ê¸° ì˜µì…˜(ê¸°ë³¸ false ê¶Œì¥: ë¯¸ë¦¬ë³´ê¸°/ìë™ ë§í¬ê°€ ì„ì¼ ìˆ˜ ìˆìŒ)
    if KAKAO_INCLUDE_LINK_IN_TEXT:
        kakao_text = kakao_text + "\n" + daily_url

    # âœ… STRICT ëª¨ë“œ: ë§í¬ ë„ë©”ì¸ ì˜ì‹¬ ì‹œ ë°œì†¡ ì¤‘ë‹¨(í…ŒìŠ¤íŠ¸ìš©)
    if STRICT_KAKAO_LINK_CHECK:
        # github.io / custom domain ëª¨ë‘ í—ˆìš©, ë‹¤ë§Œ gist/rawëŠ” ì´ë¯¸ ì°¨ë‹¨
        parsed = urlparse(daily_url)
        if not parsed.scheme.startswith("http") or not parsed.netloc:
            raise RuntimeError(f"[FATAL] daily_url invalid: {daily_url}")

    kakao_send_to_me(kakao_text, daily_url)
    log.info("[OK] Kakao message sent. URL=%s", daily_url)


if __name__ == "__main__":
    main()
