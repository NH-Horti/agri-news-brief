#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
main.py (GitHub Actions)
- ê³¼ìˆ˜Â·í™”í›¼(ì›ì˜ˆ) ë¸Œë¦¬í•‘ ìë™ ìˆ˜ì§‘/ì •ë¦¬/ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

âœ… ë°˜ì˜ ì‚¬í•­ (ìš”ì²­ë¶„)
1) "ì¢‹ì•„ìš”/ë³„ë¡œì—ìš”(í”¼ë“œë°±)" ê´€ë ¨ ì½”ë“œ: ì™„ì „ ì œê±°(ì—†ìŒ)
2) í’ˆëª© í‚¤ì›Œë“œ ì ê²€/ê°•í™”
   - í™”í›¼ í¬í•¨
   - ì‚¬ê³¼, ë°°, ë‹¨ê°, ê°, í‚¤ìœ„, ìœ ì, í¬ë„, ë°¤, ìë‘, ê°ê·¤, ë§Œê°, ë³µìˆ­ì•„, ë§¤ì‹¤ ê¸°ë³¸ í¬í•¨
   - "ê¸°ë³¸ í’ˆëª© í‚¤ì›Œë“œ + ì‹ í˜¸ ë‹¨ì–´" ì¡°í•©ìœ¼ë¡œ ë™ì  ì¿¼ë¦¬ ìƒì„± (ìŠ¤í¬ë˜í•‘ íš¨ìœ¨â†‘)
3) ë‚¨ì€ 3ê°œ ê³ ë„í™”ê¹Œì§€ í¬í•¨ ì™„ì„±
   - (A) ì¤‘ë³µ ì œê±° ê³ ë„í™”: pest ì„¹ì…˜ 'ì‚¬ê±´í‚¤(event key: ì§€ì—­+ë³‘í•´/ê¸°ìƒ+ê¸°ê°„)'ë¡œ ë¬¶ê¸°
   - (B) UX í•„í„°: index.htmlì—ì„œ 'ë§¤ì²´ ê·¸ë£¹' / 'í’ˆëª©' í•„í„° ì œê³µ (search_index.json ê¸°ë°˜)
   - (C) ì „ë‚  fallback: ì„¹ì…˜ ê¸°ì‚¬ ë¶€ì¡± ì‹œ ì „ë‚  search_index.jsonì—ì„œ ì¬í™œìš©(í‘œì‹œ)

ì„ íƒ ì˜µì…˜
- STRICT_HORTI_ONLY=true (ê¸°ë³¸) : ì˜¤ì´/ê³ ì¶”/ì–‘ê³¡/ì¶•ì‚° ë“± ì›ì˜ˆ(ê³¼ìˆ˜Â·í™”í›¼) ê´€ë ¨ ì—†ëŠ” ê²ƒ ì œì™¸ ê°•í™”
- ENABLE_EVENT_DEDUPE=true (ê¸°ë³¸) : ì‚¬ê±´í‚¤ ê¸°ë°˜ ì¤‘ë³µ ì œê±°

í•„ìˆ˜ ENV (GitHub Actions secrets ê¶Œì¥)
- NAVER_CLIENT_ID, NAVER_CLIENT_SECRET
- GITHUB_TOKEN (ë˜ëŠ” GH_TOKEN)
- GITHUB_REPOSITORY (Actions ê¸°ë³¸ ì œê³µ) ë˜ëŠ” GITHUB_REPO/REPO_SLUG

ì„ íƒ ENV
- KAKAO_REST_API_KEY, KAKAO_REFRESH_TOKEN, KAKAO_REDIRECT_URI (ì¹´í†¡ ì „ì†¡ ì‹œ)
- WHITELIST_RSS_URLS (ì½¤ë§ˆ êµ¬ë¶„)  # ë¹„ìš°ë©´ ê¸°ë³¸ ê³µì‹ RSS ì‚¬ìš©
- MIN_PER_SECTION (ê¸°ë³¸ 2)
- MAX_SECTION_QUERIES (ê¸°ë³¸ 18)
- MAX_ITEMS_PER_QUERY (ê¸°ë³¸ 50)
- REPORT_HOUR_KST (ê¸°ë³¸ 7)
- FORCE_RUN=true (íœ´ì¼/ì£¼ë§ì—ë„ ê°•ì œ ì‹¤í–‰)
- DRY_RUN=true (GitHub ì—…ë¡œë“œ/ì¹´í†¡ ì „ì†¡ ì—†ì´ ë¡œì»¬ ë¡œê·¸ë§Œ)
"""

from __future__ import annotations

import os
import re
import json
import time
import math
import base64
import hashlib
import random
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone, date
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode

import requests

# -----------------------------
# Timezone / Session
# -----------------------------
KST = timezone(timedelta(hours=9))
SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "HortiBriefingBot/1.0 (+github actions)"})

def now_kst() -> datetime:
    return datetime.now(tz=KST)

# -----------------------------
# Config
# -----------------------------
REPO = (os.getenv("REPO_SLUG") or os.getenv("GITHUB_REPO") or os.getenv("GITHUB_REPOSITORY") or "").strip()
GH_TOKEN = (os.getenv("GH_TOKEN") or os.getenv("GITHUB_TOKEN") or "").strip()
BRANCH = (os.getenv("BRANCH") or os.getenv("GITHUB_REF_NAME") or "main").strip()

NAVER_CLIENT_ID = (os.getenv("NAVER_CLIENT_ID") or "").strip()
NAVER_CLIENT_SECRET = (os.getenv("NAVER_CLIENT_SECRET") or "").strip()

DOCS_DIR = "docs"
ARCHIVE_DIR = f"{DOCS_DIR}/archive"
SEARCH_INDEX_PATH = f"{DOCS_DIR}/search_index.json"
MANIFEST_PATH = f"{DOCS_DIR}/manifest.json"
INDEX_HTML_PATH = f"{DOCS_DIR}/index.html"

REPORT_HOUR_KST = int(os.getenv("REPORT_HOUR_KST", "7") or "7")

MAX_SECTION_QUERIES = max(5, int(os.getenv("MAX_SECTION_QUERIES", "18") or "18"))
MAX_ITEMS_PER_QUERY = max(10, int(os.getenv("MAX_ITEMS_PER_QUERY", "50") or "50"))
MIN_PER_SECTION = max(0, int(os.getenv("MIN_PER_SECTION", "2") or "2"))

DRY_RUN = (os.getenv("DRY_RUN", "false").lower() in ("1", "true", "yes"))
FORCE_RUN = (os.getenv("FORCE_RUN", "false").lower() in ("1", "true", "yes"))

STRICT_HORTI_ONLY = (os.getenv("STRICT_HORTI_ONLY", "true").lower() in ("1", "true", "yes"))
ENABLE_EVENT_DEDUPE = (os.getenv("ENABLE_EVENT_DEDUPE", "true").lower() in ("1", "true", "yes"))

# -----------------------------
# Default RSS (ê³µì‹ ì†ŒìŠ¤ ìš°ì„ )
# - WHITELIST_RSS_URLS envê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì•„ë˜ ê¸°ë³¸ê°’ ì‚¬ìš©
# -----------------------------
DEFAULT_RSS_URLS = [
    "https://www.korea.kr/rss/dept_mafra.xml",       # ì •ì±…ë¸Œë¦¬í•‘(ë†ì‹í’ˆë¶€)
    "https://www.korea.kr/rss/dept_rda.xml",         # ì •ì±…ë¸Œë¦¬í•‘(ë†ì´Œì§„í¥ì²­)
    "https://www.korea.kr/rss/pressrelease.xml",     # ì •ì±…ë¸Œë¦¬í•‘(ë³´ë„ìë£Œ ì „ì²´)
    "https://www.mafra.go.kr/bbs/home/792/rssList.do?row=50",  # ë†ì‹í’ˆë¶€ ë³´ë„ìë£Œ RSS
    "https://www.mafra.go.kr/bbs/home/793/rssList.do?row=50",  # ë†ì‹í’ˆë¶€ ì„¤ëª…ìë£Œ RSS
]
WHITELIST_RSS_URLS = [u.strip() for u in (os.getenv("WHITELIST_RSS_URLS", "") or "").split(",") if u.strip()] or DEFAULT_RSS_URLS

# -----------------------------
# Required commodity keywords (ìš”ì²­ ê¸°ë³¸ í¬í•¨)
# -----------------------------
REQUIRED_ITEM_KEYWORDS = [
    "í™”í›¼",
    "ì‚¬ê³¼", "ë°°", "ë‹¨ê°", "ê°", "í‚¤ìœ„", "ìœ ì", "í¬ë„", "ë°¤", "ìë‘", "ê°ê·¤", "ë§Œê°", "ë³µìˆ­ì•„", "ë§¤ì‹¤",
]

COMMODITY_SYNONYMS: dict[str, list[str]] = {
    "í™”í›¼": ["í™”í›¼", "ì ˆí™”", "ê½ƒì‹œì¥", "ê½ƒ ê²½ë§¤", "ì–‘ì¬ê½ƒì‹œì¥", "í™”í›¼ê³µíŒì¥", "í™”í›¼ê²½ë§¤", "aT í™”í›¼"],
    "ì‚¬ê³¼": ["ì‚¬ê³¼", "ë¶€ì‚¬", "í™ë¡œ", "í›„ì§€"],
    "ë°°": ["ë°°", "ì‹ ê³ ", "ì›í™©"],
    "ë‹¨ê°": ["ë‹¨ê°", "ë¶€ìœ ", "íƒœì¶”"],
    "ê°": ["ê°", "ë–«ì€ê°", "ëŒ€ë´‰", "ê³¶ê°"],
    "í‚¤ìœ„": ["í‚¤ìœ„", "ì°¸ë‹¤ë˜"],
    "ìœ ì": ["ìœ ì"],
    "í¬ë„": ["í¬ë„", "ìƒ¤ì¸ë¨¸ìŠ¤ìº£", "ê±°ë´‰", "ìº ë²¨"],
    "ë°¤": ["ë°¤"],
    "ìë‘": ["ìë‘"],
    "ê°ê·¤": ["ê°ê·¤", "ê·¤", "ë…¸ì§€ê°ê·¤"],
    "ë§Œê°": ["ë§Œê°", "ë§Œê°ë¥˜", "í•œë¼ë´‰", "ë ˆë“œí–¥", "ì²œí˜œí–¥", "ì¹´ë¼í–¥", "í™©ê¸ˆí–¥"],
    "ë³µìˆ­ì•„": ["ë³µìˆ­ì•„", "ë°±ë„", "í™©ë„"],
    "ë§¤ì‹¤": ["ë§¤ì‹¤"],
    # í™•ì¥(ì›ì˜ˆ ê³¼ìˆ˜) - í•„ìš” ì‹œ
    "ë”¸ê¸°": ["ë”¸ê¸°"],
}

# ì›ì˜ˆ ë¸Œë¦¬í•‘ì—ì„œ ë°°ì œí•  ê°€ëŠ¥ì„±ì´ ë†’ì€ í’ˆëª©(ì„ íƒ1)
NON_HORTI_ITEM_TERMS = [
    "ì˜¤ì´", "ê³ ì¶”", "í’‹ê³ ì¶”", "íŒŒí”„ë¦¬ì¹´", "í† ë§ˆí† ", "ë°°ì¶”", "ë¬´", "ì–‘íŒŒ", "ë§ˆëŠ˜",
    "ê°ì", "ê³ êµ¬ë§ˆ", "ì½©", "íŒ¥", "ìŒ€", "ë²¼", "ë¹„ì¶•ë¯¸", "ë°€", "ë³´ë¦¬",
    "í•œìš°", "ë¼ì§€", "ë‹­", "ê³„ë€", "ìš°ìœ ", "ì¶•ì‚°", "ìˆ˜ì‚°",
]

# -----------------------------
# Section definitions
# -----------------------------
SECTIONS = [
    {"key": "supply", "title": "ìˆ˜ê¸‰Â·ê°€ê²©", "color": "#2563eb"},
    {"key": "policy", "title": "ì •ì±…Â·ì§€ì›", "color": "#16a34a"},
    {"key": "distribution", "title": "ìœ í†µÂ·ë„ë§¤ì‹œì¥", "color": "#f97316"},
    {"key": "pest", "title": "ë³‘í•´ì¶©Â·ê¸°ìƒ", "color": "#dc2626"},
]

SECTION_SIGNAL_TERMS = {
    "supply": ["ìˆ˜ê¸‰", "ê°€ê²©", "ì‘í™©", "ì €ì¥", "ì¶œí•˜"],
    "distribution": ["ê°€ë½ì‹œì¥", "ë„ë§¤ì‹œì¥", "ê²½ë½ê°€", "ë°˜ì…", "ê³µíŒì¥", "ì˜¨ë¼ì¸ë„ë§¤ì‹œì¥"],
    "policy": ["ì§€ì›", "í• ì¸ì§€ì›", "ìˆ˜ë§¤", "ë¹„ì¶•", "í• ë‹¹ê´€ì„¸", "ê²€ì—­", "ì›ì‚°ì§€"],
    "pest": ["í™”ìƒë³‘", "íƒ„ì €ë³‘", "ëƒ‰í•´", "ë™í•´", "ìš°ë°•", "ë³‘í•´ì¶©", "ë°©ì œ", "ì˜ˆì°°", "ê²½ë³´"],
}

CURATED_BASE_QUERIES = {
    "supply": [
        "ì‚¬ê³¼ ìˆ˜ê¸‰ ê°€ê²©", "ë°° ìˆ˜ê¸‰ ê°€ê²©", "ê°ê·¤ ë§Œê° ìˆ˜ê¸‰ ê°€ê²©", "í¬ë„ ìƒ¤ì¸ë¨¸ìŠ¤ìº£ ìˆ˜ê¸‰ ê°€ê²©",
        "ë‹¨ê° ê³¶ê° ìˆ˜ê¸‰ ê°€ê²©", "í‚¤ìœ„ ì°¸ë‹¤ë˜ ìˆ˜ê¸‰ ê°€ê²©", "ìœ ì ìˆ˜ê¸‰ ê°€ê²©",
        "ë³µìˆ­ì•„ ìë‘ ë§¤ì‹¤ ìˆ˜ê¸‰ ê°€ê²©", "ë°¤ ìˆ˜ê¸‰ ê°€ê²©", "í™”í›¼ ì ˆí™” ê²½ë§¤ ê°€ê²©",
    ],
    "distribution": [
        "ê°€ë½ì‹œì¥ ì²­ê³¼ ê²½ë½ê°€", "ë„ë§¤ì‹œì¥ ë°˜ì…ëŸ‰ ê³¼ì¼", "ê³µíŒì¥ ê²½ë§¤ ì²­ê³¼",
        "APC ì„ ë³„ ê³¼ìˆ˜", "CAì €ì¥ ê³¼ìˆ˜", "ì˜¨ë¼ì¸ë„ë§¤ì‹œì¥ ê±°ë˜ëŸ‰", "ë†ì‚°ë¬¼ ìˆ˜ì¶œ ê³¼ì¼ ê²€ì—­",
    ],
    "policy": [
        "ë†ì‹í’ˆë¶€ ë³´ë„ìë£Œ ë†ì‚°ë¬¼", "ì •ì±…ë¸Œë¦¬í•‘ ë†ì¶•ìˆ˜ì‚°ë¬¼ í• ì¸ì§€ì›",
        "í• ë‹¹ê´€ì„¸ ìˆ˜ì… ê³¼ì¼", "ì›ì‚°ì§€ í‘œì‹œ ë‹¨ì† ë†ì‚°ë¬¼", "ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥ ë†ì‹í’ˆë¶€ aT",
        "ê²€ì—­ë³¸ë¶€ ìˆ˜ì¶œì… ê³¼ì¼",
    ],
    "pest": [
        "ê³¼ìˆ˜í™”ìƒë³‘ ì˜ˆì°° ë°©ì œ", "íƒ„ì €ë³‘ ë°©ì œ ê³¼ìˆ˜", "ëƒ‰í•´ ë™í•´ ìš°ë°• ê³¼ìˆ˜ í”¼í•´",
        "ë³‘í•´ì¶© ì˜ˆì°° ê³¼ìˆ˜", "ë†ì—…ê¸°ìˆ ì„¼í„° ê³¼ìˆ˜ ë°©ì œ", "í™”í›¼ ë³‘í•´ì¶© ë°©ì œ",
    ],
}

def build_section_queries(section_key: str) -> list[str]:
    """(ê¸°ë³¸ í’ˆëª©) + (ì‹ í˜¸ ë‹¨ì–´) ì¡°í•©ìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ë§Œë“¤ë˜, í˜¸ì¶œ ìˆ˜ëŠ” ì œí•œ(MAX_SECTION_QUERIES)."""
    base = CURATED_BASE_QUERIES.get(section_key, []).copy()
    signals = SECTION_SIGNAL_TERMS.get(section_key, [])
    dyn = []

    # interleave: ì‹ í˜¸ ìš°ì„  â†’ í’ˆëª© í¸í–¥ ì™„í™”
    commodities = REQUIRED_ITEM_KEYWORDS[:]
    if "ê°" in commodities and "ë‹¨ê°" in commodities:
        commodities.remove("ê°")
        commodities.append("ê°")

    for sig in signals:
        for kw in commodities:
            dyn.append(f"{kw} {sig}")

    # merge unique in order
    seen = set()
    merged = []
    for q in base + dyn:
        qn = " ".join((q or "").split())
        if not qn:
            continue
        if qn in seen:
            continue
        seen.add(qn)
        merged.append(qn)

    return merged[:MAX_SECTION_QUERIES]

# -----------------------------
# Models
# -----------------------------
@dataclass
class Article:
    section: str
    title: str
    description: str
    link: str
    originallink: str
    pub_dt_kst: datetime
    domain: str
    press: str
    canon_url: str
    norm_key: str
    title_key: str
    score: float = 0.0
    summary: str = ""
    tags: list[str] = field(default_factory=list)
    commodities: list[str] = field(default_factory=list)
    press_group: str = ""
    urgent: bool = False
    reused: bool = False
    reused_from: str = ""

# -----------------------------
# Helpers: normalize / commodity / tags
# -----------------------------
def normalize_host(url_or_host: str) -> str:
    if not url_or_host:
        return ""
    if "://" in url_or_host:
        try:
            return (urlparse(url_or_host).netloc or "").lower()
        except Exception:
            return url_or_host.lower()
    return url_or_host.lower()

def canonicalize_url(url: str) -> str:
    """URL ì •ê·œí™”(utm ë“± ì œê±°)."""
    if not url:
        return ""
    try:
        u = urlparse(url)
        q = parse_qs(u.query)
        # í”í•œ íŠ¸ë˜í‚¹ íŒŒë¼ë¯¸í„° ì œê±°
        for k in list(q.keys()):
            if k.lower().startswith("utm_") or k.lower() in ("fbclid", "gclid", "igshid"):
                q.pop(k, None)
        query = urlencode({k: v[0] for k, v in q.items()}, doseq=False)
        return urlunparse((u.scheme, u.netloc, u.path, "", query, ""))
    except Exception:
        return url

def norm_title_key(title: str) -> str:
    t = (title or "").lower()
    t = re.sub(r"\[[^\]]+\]", " ", t)
    t = re.sub(r"\([^)]*\)", " ", t)
    t = re.sub(r"[^0-9a-zê°€-í£]+", "", t)
    return t[:90]

def make_norm_key(canon_url: str, press: str, title_key: str) -> str:
    if canon_url:
        h = hashlib.sha1(canon_url.encode("utf-8")).hexdigest()[:16]
        return f"url:{h}"
    base = f"{(press or '').strip()}|{title_key}"
    h = hashlib.sha1(base.encode("utf-8")).hexdigest()[:16]
    return f"t:{h}"

def detect_commodities(text: str) -> list[str]:
    t = (text or "").lower()
    found = []
    for canon, syns in COMMODITY_SYNONYMS.items():
        for s in syns:
            if s and s.lower() in t:
                found.append(canon)
                break
    order = {k: i for i, k in enumerate(REQUIRED_ITEM_KEYWORDS + sorted(COMMODITY_SYNONYMS.keys()))}
    return sorted(set(found), key=lambda x: order.get(x, 999))

_NUM_UNIT_RX = re.compile(r"(\d[\d,\.]*\s*(ì›|ë§Œì›|ì²œì›|kg|ã|g|í†¤|t|%))")
_DATE_HINT_RX = re.compile(r"(\d{1,2}ì›”\s*\d{1,2}ì¼|\d{4}ë…„\s*\d{1,2}ì›”|\d{1,2}ì›”|ì´ë²ˆ\s*ì£¼|ì´\s*ë‹¬|ë‹¤ìŒ\s*ì£¼|ë‚´\s*ë‹¬)")
URGENT_TERMS = ["ê³¼ìˆ˜í™”ìƒë³‘", "í™”ìƒë³‘", "íƒ„ì €ë³‘", "ëƒ‰í•´", "ë™í•´", "ìš°ë°•", "ì„œë¦¬", "ë³‘í•´ì¶© ê²½ë³´", "ê¸´ê¸‰", "ì£¼ì˜ë³´", "ê²½ë³´", "íŠ¹ë³´"]

def compute_press_group(press: str, domain: str) -> str:
    p = (press or "").strip()
    d = (domain or "").lower()
    if d.endswith(".go.kr") or "korea.kr" in d:
        return "ê³µì‹"
    if any(k in p for k in ["ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€", "ë†ì‹í’ˆë¶€", "ë†ì´Œì§„í¥ì²­", "ê²€ì—­", "ì •ì±…ë¸Œë¦¬í•‘", "aT", "ì •ë¶€"]):
        return "ê³µì‹"
    if "ë†ë¯¼ì‹ ë¬¸" in p or "nongmin" in d:
        return "ë†ë¯¼ì‹ ë¬¸"
    # ëŒ€ëµì  ë¶„ë¥˜
    major_kw = ["ì—°í•©ë‰´ìŠ¤", "KBS", "MBC", "SBS", "YTN", "ì¡°ì„ ", "ì¤‘ì•™", "ë™ì•„", "í•œê²¨ë ˆ", "ê²½í–¥", "í•œêµ­ê²½ì œ", "ë§¤ì¼ê²½ì œ"]
    if any(k in p for k in major_kw):
        return "ì£¼ìš”"
    if any(k in p for k in ["ì¼ë³´", "ì‹ ë¬¸", "ë°©ì†¡", "ë‰´ìŠ¤"]):
        return "ì§€ì—­/ê¸°íƒ€"
    return "ê¸°íƒ€"

def analyze_signals(title: str, desc: str, section_key: str) -> tuple[list[str], list[str], bool]:
    text = f"{title} {desc}".strip()
    tags: set[str] = set()
    urgent = False

    if _NUM_UNIT_RX.search(text):
        tags.add("ìˆ˜ì¹˜")
    if _DATE_HINT_RX.search(text):
        tags.add("ê¸°ê°„")

    if any(k in text for k in ["ê°€ê²©", "ì‹œì„¸", "ê²½ë½", "ê²½ë½ê°€", "ë„ë§¤ê°€", "ì†Œë§¤ê°€", "ê°•ì„¸", "ì•½ì„¸", "ìƒìŠ¹", "í•˜ë½"]):
        tags.add("ê°€ê²©")
    if any(k in text for k in ["ìˆ˜ê¸‰", "ë¬¼ëŸ‰", "ë°˜ì…", "ì¶œí•˜", "ì¬ê³ ", "ì €ì¥", "ìƒì‚°", "ìˆ˜í™•", "ê³µê¸‰", "ê°ì†Œ", "ì¦ê°€"]):
        tags.add("ë¬¼ëŸ‰/ìˆ˜ê¸‰")
    if any(k in text for k in ["ì§€ì›", "í• ì¸ì§€ì›", "ìˆ˜ë§¤", "ë¹„ì¶•", "í• ë‹¹ê´€ì„¸", "ê´€ì„¸", "ëŒ€ì±…", "ë³´ì¡°", "ì˜ˆì‚°", "ì •ì±…", "ì¡°ì¹˜"]):
        tags.add("ì •ì±…")
    if any(k in text for k in ["ë³‘í•´ì¶©", "ë°©ì œ", "ì˜ˆì°°", "ê²½ë³´", "ë°œìƒ", "í™•ì‚°", "ì•½ì œ", "ì‚´í¬", "ê²€ì—­"]):
        tags.add("ë³‘í•´/ë°©ì œ")

    if section_key == "distribution":
        tags.add("ìœ í†µ")
    if section_key == "pest":
        tags.add("ë³‘í•´/ë°©ì œ")
        if any(k in text for k in URGENT_TERMS):
            urgent = True
            tags.add("ê¸´ê¸‰")

    comms = detect_commodities(text)
    if comms:
        tags.add("í’ˆëª©")

    tag_order = ["ê¸´ê¸‰", "ê°€ê²©", "ë¬¼ëŸ‰/ìˆ˜ê¸‰", "ì •ì±…", "ìœ í†µ", "ë³‘í•´/ë°©ì œ", "ìˆ˜ì¹˜", "ê¸°ê°„", "í’ˆëª©"]
    ordered = sorted(tags, key=lambda x: tag_order.index(x) if x in tag_order else 999)
    return ordered, comms, urgent

# -----------------------------
# Strict relevance filter (ì„ íƒ1 í¬í•¨)
# -----------------------------
BAN_KWS = [
    "êµ¬ì¸", "ì±„ìš©", "ëª¨ì§‘", "ì•„ë¥´ë°”ì´íŠ¸", "ì•Œë°”", "ëŒ€ì¶œ", "ì¹´ì§€ë…¸", "ë„ë°•", "ì„±ì¸", "19ê¸ˆ",
    "ë¶€ë™ì‚°", "ë¶„ì–‘", "ì˜¤í”¼ìŠ¤í…”", "ì „ì„¸", "ì›”ì„¸", "ê´‘ê³ ", "í˜‘ì°¬", "ì²´í—˜ë‹¨",
]

def is_relevant(section_key: str, title: str, desc: str, domain: str) -> bool:
    text = f"{title} {desc}".lower()
    if any(k in text for k in BAN_KWS):
        return False

    # âœ… ì„ íƒ1: ì›ì˜ˆ ì™¸ í’ˆëª© ê°•ì œ ì œì™¸ (ë‹¨, ì›ì˜ˆ í’ˆëª©(ê³¼ìˆ˜/í™”í›¼)ì´ ê°™ì´ ìˆìœ¼ë©´ í—ˆìš©)
    if STRICT_HORTI_ONLY:
        if any(t.lower() in text for t in NON_HORTI_ITEM_TERMS):
            if not detect_commodities(text):
                return False

    # sectionë³„ ìµœì†Œ ë§¥ë½
    if section_key == "policy":
        # ì •ì±…ì€ ê³µê³µ/ë†ì—… ë§¥ë½ ì—†ìœ¼ë©´ ì œì™¸
        agri_ctx = ["ë†ì‹í’ˆ", "ë†ì—…", "ë†ì‚°ë¬¼", "ì›ì˜ˆ", "ê³¼ìˆ˜", "ì²­ê³¼", "ë„ë§¤ì‹œì¥", "ê²€ì—­", "ì›ì‚°ì§€", "í• ì¸ì§€ì›", "ì„±ìˆ˜í’ˆ", "ê°€ê²© ì•ˆì •", "aT", "ë†í˜‘"]
        if not any(k in text for k in agri_ctx) and not (domain.endswith(".go.kr") or "korea.kr" in domain):
            return False

    if section_key == "distribution":
        dist_ctx = ["ê°€ë½ì‹œì¥", "ë„ë§¤ì‹œì¥", "ê³µíŒì¥", "ê²½ë½", "ê²½ë½ê°€", "ë°˜ì…", "APC", "ì„ ë³„", "ì˜¨ë¼ì¸ë„ë§¤ì‹œì¥", "ìˆ˜ì¶œ", "ê²€ì—­"]
        if not any(k.lower() in text for k in dist_ctx):
            return False

    if section_key == "pest":
        pest_ctx = ["ë³‘í•´ì¶©", "ë°©ì œ", "ì˜ˆì°°", "í™”ìƒë³‘", "íƒ„ì €ë³‘", "ëƒ‰í•´", "ë™í•´", "ìš°ë°•", "ì„œë¦¬", "ê²½ë³´", "ì•½ì œ", "ì‚´í¬"]
        if sum(1 for k in pest_ctx if k.lower() in text) < 2:
            return False

    # supplyëŠ” ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ë¯€ë¡œ ìµœì†Œ ì›ì˜ˆ/ìˆ˜ê¸‰ ë§¥ë½ ë³´ì¥
    if section_key == "supply":
        supply_ctx = ["ìˆ˜ê¸‰", "ê°€ê²©", "ì‘í™©", "ì¶œí•˜", "ì €ì¥", "ë¬¼ëŸ‰", "ë°˜ì…", "ê²½ë½", "ë„ë§¤"]
        if not any(k in text for k in supply_ctx) and not detect_commodities(text):
            return False

    return True

# -----------------------------
# Scoring (Decision signal ì¤‘ì‹¬)
# -----------------------------
def score_article(section_key: str, title: str, desc: str, press: str, domain: str, pub_dt: datetime) -> float:
    text = f"{title} {desc}"
    score = 0.0

    tags, comms, urgent = analyze_signals(title, desc, section_key)

    # ì‹ í˜¸ ê°€ì¤‘ì¹˜
    if "ê°€ê²©" in tags: score += 4.0
    if "ë¬¼ëŸ‰/ìˆ˜ê¸‰" in tags: score += 3.5
    if "ì •ì±…" in tags: score += 3.2
    if "ìœ í†µ" in tags: score += 2.6
    if "ë³‘í•´/ë°©ì œ" in tags: score += 3.0
    if "ìˆ˜ì¹˜" in tags: score += 2.2
    if "ê¸°ê°„" in tags: score += 0.8
    if urgent: score += 3.0

    # í’ˆëª© ë“±ì¥ ê°€ì‚°
    if comms:
        score += min(2.0, 0.6 * len(comms))

    # ê³µì‹/ë†ë¯¼ì‹ ë¬¸/ì£¼ìš”ë§¤ì²´ ê°€ì‚°
    pg = compute_press_group(press, domain)
    if pg == "ê³µì‹": score += 3.0
    elif pg == "ë†ë¯¼ì‹ ë¬¸": score += 2.0
    elif pg == "ì£¼ìš”": score += 1.6
    elif pg == "ì¤‘ê²¬/ì „ë¬¸": score += 1.0

    # ìµœì‹ ì„±
    try:
        age_h = (now_kst() - pub_dt).total_seconds() / 3600.0
        if age_h <= 8: score += 0.8
        elif age_h <= 24: score += 0.4
        elif age_h <= 48: score += 0.2
    except Exception:
        pass

    return round(score, 3)

# -----------------------------
# Dedup (URL/title) + pest ì‚¬ê±´í‚¤
# -----------------------------
_REGION_RX = re.compile(r"[ê°€-í£]{2,}(?:íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ|íŠ¹ë³„ìì¹˜ì‹œ|íŠ¹ë³„ìì¹˜ë„|ë„|ì‹œ|êµ°|êµ¬|ì|ë©´)")
_BARE_REGION_RX = re.compile(r"([ê°€-í£]{2,6})(?=(?:\s*)?(?:ë†ì—…ê¸°ìˆ ì„¼í„°|ë†ê¸°ì„¼í„°|êµ°ì²­|ì‹œì²­|êµ¬ì²­|ë†ì—…ê¸°ìˆ ì›|ë†ì—…ê¸°ìˆ ê³¼))")

_PEST_DISEASE_TERMS = [
    "ê³¼ìˆ˜í™”ìƒë³‘", "í™”ìƒë³‘", "íƒ„ì €ë³‘",
    "ëƒ‰í•´", "ë™í•´", "ìš°ë°•", "ì„œë¦¬", "í•œíŒŒ", "í­ì„¤",
    "ë³‘í•´ì¶©", "ë°©ì œ", "ì˜ˆì°°", "ê²½ë³´", "ì£¼ì˜ë³´", "íŠ¹ë³´",
]

def _pest_region_key(text: str) -> str:
    t = text or ""
    ms = list(_REGION_RX.finditer(t))
    if not ms:
        m2 = _BARE_REGION_RX.search(t)
        return (m2.group(1) if m2 else "") or ""
    for m in ms:
        r = m.group(0)
        if r.endswith(("êµ°", "ì‹œ", "êµ¬")):
            return r
    return ms[0].group(0)

def _pest_disease_key(text: str) -> str:
    for k in _PEST_DISEASE_TERMS:
        if k in (text or ""):
            return "í™”ìƒë³‘" if k in ("ê³¼ìˆ˜í™”ìƒë³‘", "í™”ìƒë³‘") else k
    return ""

def _pest_time_key(text: str) -> str:
    t = text or ""
    m = re.search(r"(\d{1,2})ì›”\s*(\d{1,2})ì¼", t)
    if m:
        return f"{int(m.group(1)):02d}{int(m.group(2)):02d}"
    m = re.search(r"(\d{4})ë…„\s*(\d{1,2})ì›”", t)
    if m:
        return f"{m.group(1)}{int(m.group(2)):02d}"
    m = re.search(r"(\d{1,2})ì›”", t)
    if m:
        return f"{int(m.group(1)):02d}xx"
    if "ì´ë²ˆì£¼" in t or "ì´ë²ˆ ì£¼" in t:
        return "thisweek"
    if "ì´ë‹¬" in t or "ì´ë²ˆë‹¬" in t or "ì´ë²ˆ ë‹¬" in t:
        return "thismonth"
    return ""

def pest_event_key(title: str, desc: str) -> str:
    blob = f"{title} {desc}".strip()
    region = _pest_region_key(blob)
    disease = _pest_disease_key(blob)
    tkey = _pest_time_key(blob)
    key = "|".join([k for k in (region, disease, tkey) if k])
    return key

def near_dup_title(a: Article, b: Article) -> bool:
    # ê°„ë‹¨ í† í° ìœ ì‚¬ë„
    ta = set(re.findall(r"[0-9a-zê°€-í£]{2,}", (a.title or "").lower()))
    tb = set(re.findall(r"[0-9a-zê°€-í£]{2,}", (b.title or "").lower()))
    if not ta or not tb:
        return False
    inter = len(ta & tb)
    union = len(ta | tb)
    j = inter / union if union else 0.0
    return j >= 0.78

# -----------------------------
# Naver Search
# -----------------------------
NAVER_API_URL = "https://openapi.naver.com/v1/search/news.json"

def naver_search(query: str, display: int = 50, start: int = 1) -> dict:
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {"query": query, "display": display, "start": start, "sort": "date"}
    # 429 ëŒ€ì‘: ì§€ìˆ˜ ë°±ì˜¤í”„
    backoff = 1.0
    for _ in range(6):
        r = SESSION.get(NAVER_API_URL, headers=headers, params=params, timeout=15)
        if r.status_code == 429:
            time.sleep(backoff + random.random() * 0.2)
            backoff = min(30.0, backoff * 2)
            continue
        r.raise_for_status()
        return r.json()
    return {"items": []}

def clean_html(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"<[^>]+>", " ", s)
    s = s.replace("&quot;", "\"").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def parse_naver_item(it: dict) -> tuple[str, str, str, str, datetime]:
    title = clean_html(it.get("title", ""))
    desc = clean_html(it.get("description", ""))
    link = it.get("link", "") or ""
    origin = it.get("originallink", "") or link
    pub = it.get("pubDate", "")
    # naver pubDate RFC822
    try:
        from email.utils import parsedate_to_datetime
        dt = parsedate_to_datetime(pub)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        pub_kst = dt.astimezone(KST)
    except Exception:
        pub_kst = now_kst()
    return title, desc, link, origin, pub_kst

# -----------------------------
# RSS ingestion
# -----------------------------
def fetch_rss(url: str) -> list[dict]:
    try:
        r = SESSION.get(url, timeout=20)
        if not r.ok:
            return []
        txt = r.text
        import xml.etree.ElementTree as ET
        root = ET.fromstring(txt)
        items = []
        for it in root.findall(".//item"):
            title = (it.findtext("title") or "").strip()
            link = (it.findtext("link") or "").strip()
            desc = (it.findtext("description") or "").strip()
            pub = (it.findtext("pubDate") or "").strip()
            pub_kst = now_kst()
            try:
                from email.utils import parsedate_to_datetime
                dt = parsedate_to_datetime(pub)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                pub_kst = dt.astimezone(KST)
            except Exception:
                pass
            if title and link:
                items.append({"title": title, "description": clean_html(desc), "originallink": link, "link": link, "pub_dt_kst": pub_kst})
        return items
    except Exception:
        return []

# -----------------------------
# Press name guess
# -----------------------------
def press_from_domain(domain: str) -> str:
    d = (domain or "").lower()
    if "korea.kr" in d:
        return "ì •ì±…ë¸Œë¦¬í•‘"
    if d.endswith(".go.kr"):
        return "ê³µê³µê¸°ê´€"
    if "mafra.go.kr" in d:
        return "ë†ì‹í’ˆë¶€"
    if "rda.go.kr" in d:
        return "ë†ì´Œì§„í¥ì²­"
    if "nongmin.com" in d:
        return "ë†ë¯¼ì‹ ë¬¸"
    return domain

# -----------------------------
# Window computation (ì „ì¼ 07:00 ~ ê¸ˆì¼ 07:00 ê¸°ë³¸)
# -----------------------------
def scheduled_end_kst(now: datetime) -> datetime:
    d = now.date()
    end = datetime(d.year, d.month, d.day, REPORT_HOUR_KST, 0, 0, tzinfo=KST)
    if now >= end:
        return end
    return now

def compute_window() -> tuple[datetime, datetime, str]:
    end = scheduled_end_kst(now_kst())
    start = end - timedelta(days=1)
    report_date = end.date().isoformat()
    return start, end, report_date

# -----------------------------
# Selection / Fallback
# -----------------------------
def select_top(section_key: str, items: list[Article], max_n: int = 8) -> list[Article]:
    items = sorted(items, key=lambda a: (a.score, a.pub_dt_kst), reverse=True)

    # 1) URL/title dedupe
    out = []
    seen_url = set()
    for a in items:
        if a.canon_url and a.canon_url in seen_url:
            continue
        if any(near_dup_title(a, b) for b in out):
            continue
        seen_url.add(a.canon_url)
        out.append(a)

    # 2) pest ì‚¬ê±´í‚¤ dedupe(ì„ íƒ2) + region ë³´ì¡°
    if section_key == "pest" and ENABLE_EVENT_DEDUPE:
        used_event = set()
        used_region = set()
        filtered = []
        for a in out:
            ek = pest_event_key(a.title, a.description)
            if ek and ek in used_event:
                continue
            if ek:
                used_event.add(ek)
            else:
                rk = _pest_region_key(a.title)
                if rk and rk in used_region:
                    continue
                if rk:
                    used_region.add(rk)
            filtered.append(a)
        out = filtered

    return out[:max_n]

def load_search_index_from_repo() -> dict:
    if not REPO or not GH_TOKEN:
        return {"version": 1, "updated_at": "", "items": []}
    raw = github_get_file(REPO, SEARCH_INDEX_PATH, GH_TOKEN, ref=BRANCH)
    if not raw:
        return {"version": 1, "updated_at": "", "items": []}
    try:
        obj = json.loads(raw)
        if isinstance(obj, list):
            return {"version": 1, "updated_at": "", "items": obj}
        if isinstance(obj, dict):
            obj.setdefault("version", 1)
            obj.setdefault("items", [])
            return obj
    except Exception:
        pass
    return {"version": 1, "updated_at": "", "items": []}

def apply_prev_day_fallback(by_section: dict[str, list[Article]], report_date: str) -> None:
    if MIN_PER_SECTION <= 0:
        return
    if all(len(by_section.get(sec["key"], [])) >= MIN_PER_SECTION for sec in SECTIONS):
        return

    idx = load_search_index_from_repo()
    items = idx.get("items", []) if isinstance(idx, dict) else []
    if not items:
        return

    dates = sorted({it.get("date") for it in items if it.get("date")}, reverse=False)
    prev_dates = [d for d in dates if d < report_date]
    if not prev_dates:
        return
    prev_date = prev_dates[-1]

    # sectionë³„ í›„ë³´
    prev_by_sec: dict[str, list[dict]] = {}
    for it in items:
        if it.get("date") != prev_date:
            continue
        sk = it.get("section")
        if not sk:
            continue
        prev_by_sec.setdefault(sk, []).append(it)

    # fallback
    for sec in SECTIONS:
        sk = sec["key"]
        cur = by_section.get(sk, [])
        need = max(0, MIN_PER_SECTION - len(cur))
        if need <= 0:
            continue
        cands = prev_by_sec.get(sk, [])
        if not cands:
            continue

        used_urls = {a.canon_url for a in cur if a.canon_url}
        used_titles = {a.title_key for a in cur if a.title_key}

        cands_sorted = sorted(cands, key=lambda x: float(x.get("score", 0.0)), reverse=True)
        added = 0
        for it in cands_sorted:
            if added >= need:
                break
            url = (it.get("url") or it.get("originallink") or "").strip()
            title = (it.get("title") or "").strip()
            if not url or not title:
                continue
            canon = canonicalize_url(url)
            tkey = norm_title_key(title)
            if canon in used_urls or tkey in used_titles:
                continue

            domain = normalize_host(url)
            press = (it.get("press") or press_from_domain(domain)).strip()
            desc = (it.get("snippet") or it.get("summary") or "").strip()
            pub_dt = datetime.fromisoformat(prev_date).replace(tzinfo=KST)

            tags = it.get("tags") or []
            comms = it.get("commodities") or detect_commodities(f"{title} {desc}")
            urgent = bool(it.get("urgent", False))
            pg = it.get("press_group") or compute_press_group(press, domain)

            a = Article(
                section=sk,
                title=title,
                description=desc,
                link=url,
                originallink=url,
                pub_dt_kst=pub_dt,
                domain=domain,
                press=press,
                canon_url=canon,
                title_key=tkey,
                norm_key=make_norm_key(canon, press, tkey),
                score=float(it.get("score", 0.0)),
                summary=(it.get("summary") or desc),
                tags=list(tags),
                commodities=list(comms),
                press_group=str(pg),
                urgent=urgent,
                reused=True,
                reused_from=prev_date,
            )
            cur.append(a)
            used_urls.add(canon)
            used_titles.add(tkey)
            added += 1

        by_section[sk] = sorted(cur, key=lambda x: (x.score, x.pub_dt_kst), reverse=True)

# -----------------------------
# Rendering: archive html + index html + search index json
# -----------------------------
def esc(s: str) -> str:
    return (s or "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace('"', "&quot;")

def fmt_dt(dt: datetime) -> str:
    try:
        return dt.astimezone(KST).strftime("%m/%d %H:%M")
    except Exception:
        return ""

def render_archive(report_date: str, start_kst: datetime, end_kst: datetime, by_section: dict[str, list[Article]]) -> str:
    # chips
    chips = []
    for sec in SECTIONS:
        lst = by_section.get(sec["key"], [])
        chips.append((sec["key"], sec["title"], len(lst), sec["color"]))

    chips_html = "\n".join(
        f'<a class="chip" href="#sec-{k}" style="border-color:{c}"><span>{esc(t)}</span><b>{n}</b></a>'
        for k, t, n, c in chips
    )

    sections_html = []
    for sec in SECTIONS:
        key = sec["key"]
        title = sec["title"]
        color = sec["color"]
        lst = by_section.get(key, [])

        cards = []
        for i, a in enumerate(lst):
            url = a.originallink or a.link
            core_badge = '<span class="badge core">í•µì‹¬</span>' if i < 2 else ""
            urgent_badge = '<span class="badge urgent">ğŸš¨ê¸´ê¸‰</span>' if a.urgent else ""
            reused_badge = f'<span class="badge reused">ì „ë‚ ({esc(a.reused_from)})</span>' if a.reused else ""
            tags = " ".join(f'<span class="tag">{esc(t)}</span>' for t in a.tags[:6])
            comms = ", ".join(a.commodities[:5])

            cards.append(f"""
            <div class="card" style="border-left-color:{color}">
              <div class="top">
                {core_badge}{urgent_badge}{reused_badge}
                <span class="press">{esc(a.press)}</span>
                <span class="dot">Â·</span>
                <span class="time">{esc(fmt_dt(a.pub_dt_kst))}</span>
                <span class="dot">Â·</span>
                <span class="score">score {a.score:.1f}</span>
              </div>
              <a class="title" href="{esc(url)}" target="_blank" rel="noopener">{esc(a.title)}</a>
              <div class="meta">{tags}</div>
              <div class="summary">{esc((a.summary or a.description or "")[:220])}</div>
              <div class="bottom">
                <span class="comms">{esc(comms)}</span>
                <button class="copy" data-url="{esc(url)}">ë§í¬ë³µì‚¬</button>
              </div>
            </div>
            """)

        sections_html.append(f"""
        <section id="sec-{key}">
          <h2 style="border-left:6px solid {color}; padding-left:10px;">{esc(title)}</h2>
          <div class="grid">
            {''.join(cards) if cards else '<div class="empty">í•´ë‹¹ ì„¹ì…˜ ê¸°ì‚¬ ì—†ìŒ</div>'}
          </div>
        </section>
        """)

    return f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ì›ì˜ˆ(ê³¼ìˆ˜Â·í™”í›¼) ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ - {esc(report_date)}</title>
<style>
  body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Noto Sans KR,Apple SD Gothic Neo,sans-serif; margin:0; background:#0b1220; color:#e5e7eb}}
  a{{color:inherit; text-decoration:none}}
  .wrap{{max-width:980px; margin:0 auto; padding:18px}}
  .head{{display:flex; gap:10px; align-items:flex-end; justify-content:space-between; flex-wrap:wrap}}
  .title{{font-size:22px; font-weight:800}}
  .sub{{font-size:12px; color:#93c5fd}}
  .chips{{display:flex; gap:8px; flex-wrap:wrap; margin:12px 0 18px}}
  .chip{{display:inline-flex; gap:8px; align-items:center; border:1px solid #334155; padding:6px 10px; border-radius:999px; background:#0f172a}}
  .chip b{{background:#111827; padding:2px 8px; border-radius:999px}}
  section{{margin:18px 0 26px}}
  h2{{margin:0 0 12px}}
  .grid{{display:grid; grid-template-columns:1fr; gap:10px}}
  .card{{background:#0f172a; border:1px solid #1f2937; border-left:6px solid #334155; border-radius:12px; padding:12px}}
  .top{{display:flex; gap:8px; align-items:center; flex-wrap:wrap; font-size:12px; color:#cbd5e1}}
  .press{{font-weight:700}}
  .dot{{opacity:.6}}
  .title{{display:block; margin:8px 0 6px; font-size:16px; font-weight:800}}
  .summary{{font-size:13px; color:#e2e8f0; line-height:1.45}}
  .meta{{display:flex; gap:6px; flex-wrap:wrap; margin:8px 0 6px}}
  .tag{{font-size:11px; padding:2px 8px; background:#111827; border:1px solid #1f2937; border-radius:999px; color:#cbd5e1}}
  .badge{{font-size:11px; padding:2px 8px; border-radius:999px; font-weight:800; border:1px solid #334155}}
  .badge.core{{background:#1d4ed8; border-color:#1d4ed8}}
  .badge.urgent{{background:#b91c1c; border-color:#b91c1c}}
  .badge.reused{{background:#374151; border-color:#374151}}
  .bottom{{display:flex; justify-content:space-between; gap:10px; margin-top:10px; align-items:center; flex-wrap:wrap}}
  .copy{{cursor:pointer; border:1px solid #334155; background:#111827; color:#e5e7eb; padding:6px 10px; border-radius:10px}}
  .empty{{padding:14px; color:#94a3b8; background:#0f172a; border:1px dashed #334155; border-radius:12px}}
</style>
</head>
<body>
<div class="wrap">
  <div class="head">
    <div>
      <div class="title">ì›ì˜ˆ(ê³¼ìˆ˜Â·í™”í›¼) ë°ì¼ë¦¬ ë¸Œë¦¬í•‘</div>
      <div class="sub">{esc(report_date)} Â· ê¸°ê°„: {esc(start_kst.strftime("%Y-%m-%d %H:%M"))} ~ {esc(end_kst.strftime("%Y-%m-%d %H:%M"))}</div>
    </div>
    <div><a href="../index.html" style="opacity:.9; text-decoration:underline;">ê²€ìƒ‰/í•„í„°ë¡œ ë³´ê¸°</a></div>
  </div>

  <div class="chips">{chips_html}</div>

  {''.join(sections_html)}
</div>

<script>
document.addEventListener('click', (e) => {{
  const btn = e.target.closest('button.copy');
  if(!btn) return;
  const url = btn.getAttribute('data-url');
  if(!url) return;
  navigator.clipboard.writeText(url).then(()=>{{
    btn.textContent = 'ë³µì‚¬ë¨!';
    setTimeout(()=>btn.textContent='ë§í¬ë³µì‚¬', 900);
  }});
}});
</script>
</body>
</html>
"""

def render_index_html(site_path: str) -> str:
    # search_index.jsonì„ ë¡œë”©í•˜ì—¬ í•„í„° ì œê³µ
    return f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ì›ì˜ˆ(ê³¼ìˆ˜Â·í™”í›¼) ë¸Œë¦¬í•‘ - ê²€ìƒ‰/í•„í„°</title>
<style>
  body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Noto Sans KR,Apple SD Gothic Neo,sans-serif; margin:0; background:#0b1220; color:#e5e7eb}}
  .wrap{{max-width:980px; margin:0 auto; padding:18px}}
  a{{color:inherit}}
  h1{{margin:0 0 10px; font-size:22px}}
  .bar{{display:flex; gap:8px; flex-wrap:wrap; align-items:center; margin:12px 0}}
  input,select{{background:#0f172a; color:#e5e7eb; border:1px solid #334155; border-radius:10px; padding:8px 10px}}
  input{{flex:1; min-width:220px}}
  .btn{{cursor:pointer; padding:8px 12px; border-radius:10px; border:1px solid #334155; background:#111827}}
  .meta{{font-size:12px; color:#94a3b8}}
  .list{{display:grid; grid-template-columns:1fr; gap:10px; margin-top:12px}}
  .item{{background:#0f172a; border:1px solid #1f2937; border-radius:12px; padding:12px}}
  .top{{display:flex; gap:8px; align-items:center; flex-wrap:wrap; font-size:12px; color:#cbd5e1}}
  .badge{{font-size:11px; padding:2px 8px; border-radius:999px; font-weight:800; border:1px solid #334155}}
  .badge.urgent{{background:#b91c1c; border-color:#b91c1c}}
  .badge.reused{{background:#374151; border-color:#374151}}
  .title{{display:block; margin:8px 0 6px; font-size:15px; font-weight:800; text-decoration:none}}
  .tags{{display:flex; gap:6px; flex-wrap:wrap; margin-top:6px}}
  .tag{{font-size:11px; padding:2px 8px; background:#111827; border:1px solid #1f2937; border-radius:999px; color:#cbd5e1}}
</style>
</head>
<body>
<div class="wrap">
  <h1>ì›ì˜ˆ(ê³¼ìˆ˜Â·í™”í›¼) ë¸Œë¦¬í•‘ Â· ê²€ìƒ‰/í•„í„°</h1>
  <div class="meta">ë§¤ì²´/í’ˆëª© í•„í„° Â· ì‚¬ê±´í‚¤ ì¤‘ë³µì œê±°(pest) Â· ì „ë‚  fallback ì ìš©</div>

  <div class="bar">
    <input id="q" placeholder="í‚¤ì›Œë“œ ê²€ìƒ‰ (ì˜ˆ: ì‚¬ê³¼ 10kg 6ë§Œì› / í™”ìƒë³‘ / ê°€ë½ì‹œì¥)" />
    <select id="sec"></select>
    <select id="press"></select>
    <select id="comm"></select>
    <select id="date"></select>
    <button class="btn" id="reset">ì´ˆê¸°í™”</button>
  </div>

  <div class="meta" id="stat">loading...</div>
  <div class="list" id="list"></div>
</div>

<script>
const sitePath = {json.dumps(site_path)};
const secSel = document.getElementById('sec');
const pressSel = document.getElementById('press');
const commSel = document.getElementById('comm');
const dateSel = document.getElementById('date');
const qInput = document.getElementById('q');
const listEl = document.getElementById('list');
const statEl = document.getElementById('stat');

let DATA = [];

function uniq(arr) {{ return Array.from(new Set(arr.filter(Boolean))); }}
function opt(sel, value, label) {{
  const o = document.createElement('option');
  o.value = value; o.textContent = label;
  sel.appendChild(o);
}}
function fillSelects() {{
  secSel.innerHTML = ''; pressSel.innerHTML=''; commSel.innerHTML=''; dateSel.innerHTML='';
  opt(secSel, '', 'ì „ì²´ ì„¹ì…˜');
  opt(pressSel, '', 'ì „ì²´ ë§¤ì²´');
  opt(commSel, '', 'ì „ì²´ í’ˆëª©');
  opt(dateSel, '', 'ì „ì²´ ë‚ ì§œ');

  const secs = uniq(DATA.map(x => x.section));
  const press = uniq(DATA.map(x => x.press_group));
  const comms = uniq(DATA.flatMap(x => (x.commodities||[])));
  const dates = uniq(DATA.map(x => x.date)).sort().reverse();

  secs.forEach(s => opt(secSel, s, s));
  press.forEach(p => opt(pressSel, p, p));
  comms.forEach(c => opt(commSel, c, c));
  dates.slice(0, 90).forEach(d => opt(dateSel, d, d));
}}

function matches(item) {{
  const q = (qInput.value||'').trim().toLowerCase();
  const sec = secSel.value;
  const pg = pressSel.value;
  const comm = commSel.value;
  const dt = dateSel.value;

  if (sec && item.section !== sec) return false;
  if (pg && item.press_group !== pg) return false;
  if (dt && item.date !== dt) return false;
  if (comm) {{
    const cs = item.commodities || [];
    if (!cs.includes(comm)) return false;
  }}
  if (!q) return true;

  const hay = (item.title + ' ' + (item.summary||'') + ' ' + (item.tags||[]).join(' ') + ' ' + (item.commodities||[]).join(' ')).toLowerCase();
  return hay.includes(q);
}}

function render() {{
  const filtered = DATA.filter(matches)
    .sort((a,b)=> (b.score||0) - (a.score||0));

  statEl.textContent = `ì´ ${DATA.length}ê±´ Â· í•„í„° ê²°ê³¼ ${filtered.length}ê±´`;

  listEl.innerHTML = '';
  filtered.slice(0, 220).forEach(it => {{
    const url = it.url;
    const badges = [
      it.urgent ? '<span class="badge urgent">ğŸš¨ê¸´ê¸‰</span>' : '',
      it.reused ? `<span class="badge reused">ì „ë‚ </span>` : ''
    ].join('');
    const tags = (it.tags||[]).slice(0,6).map(t=>`<span class="tag">${{t}}</span>`).join('');
    const comms = (it.commodities||[]).slice(0,5).join(', ');
    const el = document.createElement('div');
    el.className = 'item';
    el.innerHTML = `
      <div class="top">
        ${{badges}}
        <span><b>${{it.section}}</b></span><span style="opacity:.6">Â·</span>
        <span>${{it.press}}</span><span style="opacity:.6">Â·</span>
        <span>${{it.date}}</span><span style="opacity:.6">Â·</span>
        <span>score ${{(it.score||0).toFixed(1)}}</span>
      </div>
      <a class="title" href="${{url}}" target="_blank" rel="noopener">${{it.title}}</a>
      <div class="meta">${{(it.summary||'').slice(0, 160)}}</div>
      <div class="tags">${{tags}}</div>
      <div class="meta" style="margin-top:8px">í’ˆëª©: ${{comms || '-'}}</div>
      <div class="meta" style="margin-top:6px"><a href="${{sitePath}}archive/${{it.date}}.html">í•´ë‹¹ ë‚ ì§œ ë¸Œë¦¬í•‘ ë³´ê¸°</a></div>
    `;
    listEl.appendChild(el);
  }});
}}

async function init() {{
  const res = await fetch(sitePath + 'search_index.json', {{cache:'no-store'}});
  const obj = await res.json();
  DATA = (obj.items||[]);
  fillSelects();
  render();
}}

[qInput, secSel, pressSel, commSel, dateSel].forEach(el => el.addEventListener('input', render));
document.getElementById('reset').addEventListener('click', ()=> {{
  qInput.value=''; secSel.value=''; pressSel.value=''; commSel.value=''; dateSel.value='';
  render();
}});

init();
</script>
</body>
</html>
"""

def build_site_path(repo: str) -> str:
    # project pages: /REPO/
    if not repo or "/" not in repo:
        return "/"
    owner, name = repo.split("/", 1)
    if name.lower().endswith(".github.io"):
        return "/"
    return f"/{name}/"

# -----------------------------
# GitHub Content API (upload/download)
# -----------------------------
def gh_api_headers(token: str) -> dict:
    return {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}

def github_get_file(repo: str, path: str, token: str, ref: str = "main") -> str:
    """Return decoded content string or ''"""
    try:
        url = f"https://api.github.com/repos/{repo}/contents/{path}"
        r = SESSION.get(url, headers=gh_api_headers(token), params={"ref": ref}, timeout=20)
        if r.status_code == 404:
            return ""
        r.raise_for_status()
        j = r.json()
        if isinstance(j, dict) and j.get("content"):
            raw = base64.b64decode(j["content"]).decode("utf-8", errors="replace")
            return raw
        return ""
    except Exception:
        return ""

def github_put_file(repo: str, path: str, token: str, content_text: str, message: str, branch: str = "main") -> None:
    """Create/update a file in repo via Contents API."""
    url = f"https://api.github.com/repos/{repo}/contents/{path}"
    # get sha if exists
    sha = None
    r0 = SESSION.get(url, headers=gh_api_headers(token), params={"ref": branch}, timeout=20)
    if r0.ok:
        try:
            sha = r0.json().get("sha")
        except Exception:
            sha = None
    payload = {
        "message": message,
        "content": base64.b64encode(content_text.encode("utf-8")).decode("ascii"),
        "branch": branch,
    }
    if sha:
        payload["sha"] = sha
    r = SESSION.put(url, headers=gh_api_headers(token), json=payload, timeout=30)
    r.raise_for_status()

# -----------------------------
# Search index builder
# -----------------------------
def load_manifest() -> dict:
    if not REPO or not GH_TOKEN:
        return {"dates": []}
    raw = github_get_file(REPO, MANIFEST_PATH, GH_TOKEN, ref=BRANCH)
    if not raw:
        return {"dates": []}
    try:
        return json.loads(raw)
    except Exception:
        return {"dates": []}

def update_manifest(report_date: str) -> dict:
    mf = load_manifest()
    dates = mf.get("dates", [])
    if report_date not in dates:
        dates.append(report_date)
    dates = sorted(set(dates), reverse=True)
    mf["dates"] = dates
    mf["updated_at"] = now_kst().isoformat()
    return mf

def update_search_index(report_date: str, by_section: dict[str, list[Article]], site_path: str) -> dict:
    cur = load_search_index_from_repo()
    items = cur.get("items", []) if isinstance(cur, dict) else []
    # drop same date
    items = [it for it in items if it.get("date") != report_date]

    new_items = []
    for sec in SECTIONS:
        sk = sec["key"]
        for a in by_section.get(sk, []):
            url = a.originallink or a.link
            new_items.append({
                "date": report_date,
                "section": sk,
                "title": a.title,
                "url": url,
                "press": a.press,
                "domain": a.domain,
                "score": a.score,
                "summary": a.summary or a.description,
                "tags": a.tags,
                "commodities": a.commodities,
                "press_group": a.press_group,
                "urgent": a.urgent,
                "reused": a.reused,
            })

    out = {
        "version": 2,
        "updated_at": now_kst().isoformat(),
        "items": (items + new_items),
    }
    # keep recent only
    out["items"] = sorted(out["items"], key=lambda x: (x.get("date", ""), float(x.get("score", 0.0))), reverse=True)[:6000]
    return out

# -----------------------------
# Main collection
# -----------------------------
def collect_section(section_key: str, start_kst: datetime, end_kst: datetime) -> list[Article]:
    queries = build_section_queries(section_key)
    dedupe_url = set()
    dedupe_key = set()
    out: list[Article] = []

    # 1) Naver
    for q in queries:
        if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
            break
        js = naver_search(q, display=min(100, MAX_ITEMS_PER_QUERY), start=1)
        items = js.get("items", []) or []
        for it in items:
            title, desc, link, origin, pub = parse_naver_item(it)
            if not title:
                continue
            # window filter
            if pub < start_kst or pub > end_kst:
                continue
            dom = normalize_host(origin or link)
            if not is_relevant(section_key, title, desc, dom):
                continue

            canon = canonicalize_url(origin or link)
            tkey = norm_title_key(title)
            if canon in dedupe_url or tkey in dedupe_key:
                continue
            dedupe_url.add(canon)
            dedupe_key.add(tkey)

            press = press_from_domain(dom)
            tags, comms, urgent = analyze_signals(title, desc, section_key)
            pg = compute_press_group(press, dom)
            s = score_article(section_key, title, desc, press, dom, pub)

            out.append(Article(
                section=section_key,
                title=title,
                description=desc,
                link=link,
                originallink=origin or link,
                pub_dt_kst=pub,
                domain=dom,
                press=press,
                canon_url=canon,
                title_key=tkey,
                norm_key=make_norm_key(canon, press, tkey),
                score=s,
                summary="",  # later fill
                tags=tags,
                commodities=comms,
                press_group=pg,
                urgent=urgent,
            ))

    # 2) RSS (ê³µì‹ ì†ŒìŠ¤ ìš°ì„  ë³´ê°•)
    for rss_url in WHITELIST_RSS_URLS:
        rss_items = fetch_rss(rss_url)
        for it in rss_items:
            title = it.get("title", "")
            desc = it.get("description", "")
            origin = it.get("originallink", "")
            pub = it.get("pub_dt_kst") or now_kst()
            if not title or not origin:
                continue
            if pub < start_kst or pub > end_kst:
                continue
            dom = normalize_host(origin)
            if not is_relevant(section_key, title, desc, dom):
                continue
            canon = canonicalize_url(origin)
            tkey = norm_title_key(title)
            if canon in dedupe_url or tkey in dedupe_key:
                continue
            dedupe_url.add(canon)
            dedupe_key.add(tkey)

            press = press_from_domain(dom)
            tags, comms, urgent = analyze_signals(title, desc, section_key)
            pg = compute_press_group(press, dom)
            s = score_article(section_key, title, desc, press, dom, pub)

            out.append(Article(
                section=section_key,
                title=title,
                description=desc,
                link=origin,
                originallink=origin,
                pub_dt_kst=pub,
                domain=dom,
                press=press,
                canon_url=canon,
                title_key=tkey,
                norm_key=make_norm_key(canon, press, tkey),
                score=s,
                summary="",
                tags=tags,
                commodities=comms,
                press_group=pg,
                urgent=urgent,
            ))

    # ì •ë ¬
    out.sort(key=lambda a: (a.score, a.pub_dt_kst), reverse=True)
    return out

def fill_summaries(by_section: dict[str, list[Article]]) -> None:
    # ë¹„ìš© ì—†ëŠ” ê°„ë‹¨ ìš”ì•½(ì›ë¬¸ ì„¤ëª… or ì œëª©)
    for sec in SECTIONS:
        for a in by_section.get(sec["key"], []):
            if a.summary:
                continue
            a.summary = (a.description or a.title).strip()

# -----------------------------
# Kakao (optional)
# -----------------------------
KAKAO_REST_API_KEY = (os.getenv("KAKAO_REST_API_KEY") or "").strip()
KAKAO_REFRESH_TOKEN = (os.getenv("KAKAO_REFRESH_TOKEN") or "").strip()
KAKAO_REDIRECT_URI = (os.getenv("KAKAO_REDIRECT_URI") or "").strip()

def kakao_refresh_access_token() -> str:
    if not (KAKAO_REST_API_KEY and KAKAO_REFRESH_TOKEN):
        return ""
    url = "https://kauth.kakao.com/oauth/token"
    data = {
        "grant_type": "refresh_token",
        "client_id": KAKAO_REST_API_KEY,
        "refresh_token": KAKAO_REFRESH_TOKEN,
    }
    if KAKAO_REDIRECT_URI:
        data["redirect_uri"] = KAKAO_REDIRECT_URI
    r = SESSION.post(url, data=data, timeout=20)
    if not r.ok:
        return ""
    return (r.json().get("access_token") or "").strip()

def kakao_send_memo(text: str) -> bool:
    token = kakao_refresh_access_token()
    if not token:
        return False
    url = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
    headers = {"Authorization": f"Bearer {token}"}
    template = {
        "object_type": "text",
        "text": text[:990],
        "link": {"web_url": "https://example.com", "mobile_web_url": "https://example.com"},
        "button_title": "ë¸Œë¦¬í•‘ ë³´ê¸°",
    }
    r = SESSION.post(url, headers=headers, data={"template_object": json.dumps(template, ensure_ascii=False)}, timeout=20)
    return r.ok

def build_kakao_text(report_date: str, site_url: str, by_section: dict[str, list[Article]]) -> str:
    lines = [f"ğŸ“Œ ì›ì˜ˆ(ê³¼ìˆ˜Â·í™”í›¼) ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ {report_date}", f"ğŸ”— {site_url}", ""]
    for sec in SECTIONS:
        sk = sec["key"]
        lst = by_section.get(sk, [])
        if not lst:
            continue
        lines.append(f"â–  {sec['title']} (ìƒìœ„ 2)")
        for a in lst[:2]:
            u = a.originallink or a.link
            badge = "ğŸš¨" if a.urgent else ""
            why = ", ".join([t for t in a.tags if t in ("ê°€ê²©", "ë¬¼ëŸ‰/ìˆ˜ê¸‰", "ì •ì±…", "ìœ í†µ", "ë³‘í•´/ë°©ì œ", "ìˆ˜ì¹˜", "ê¸°ê°„")][:3])
            why_txt = f" [{why}]" if why else ""
            lines.append(f"- {badge}{a.title}{why_txt}")
            lines.append(f"  Â· {a.press} Â· {u}")
        lines.append("")
    return "\n".join(lines).strip()

# -----------------------------
# Main
# -----------------------------
def main():
    if not REPO:
        print("[ERROR] REPO_SLUG/GITHUB_REPOSITORY not set.")
        return
    if not DRY_RUN and not GH_TOKEN:
        print("[ERROR] GITHUB_TOKEN/GH_TOKEN not set.")
        return

    start_kst, end_kst, report_date = compute_window()
    site_path = build_site_path(REPO)
    site_url = f"https://{REPO.split('/')[0]}.github.io{site_path}archive/{report_date}.html"

    print(f"[INFO] repo={REPO} branch={BRANCH}")
    print(f"[INFO] window={start_kst.isoformat()} ~ {end_kst.isoformat()} report_date={report_date}")
    print(f"[INFO] strict_horti={STRICT_HORTI_ONLY} event_dedupe={ENABLE_EVENT_DEDUPE} min_per_section={MIN_PER_SECTION}")

    # Collect
    by_section: dict[str, list[Article]] = {}
    for sec in SECTIONS:
        sk = sec["key"]
        cands = collect_section(sk, start_kst, end_kst)
        picked = select_top(sk, cands, max_n=8)
        by_section[sk] = picked
        print(f"[INFO] {sk}: candidates={len(cands)} selected={len(picked)}")

    # Fallback (ì „ë‚ )
    apply_prev_day_fallback(by_section, report_date)

    # Fill summaries
    fill_summaries(by_section)

    # Render archive + index + update manifest + search index
    archive_html = render_archive(report_date, start_kst, end_kst, by_section)
    index_html = render_index_html(site_path)

    manifest = update_manifest(report_date)
    search_index = update_search_index(report_date, by_section, site_path)

    if DRY_RUN:
        print("[DRY_RUN] skip GitHub upload & Kakao")
        print("[DRY_RUN] archive length:", len(archive_html))
        return

    # Upload files to GitHub
    github_put_file(REPO, f"{ARCHIVE_DIR}/{report_date}.html", GH_TOKEN, archive_html,
                    message=f"chore: update archive {report_date}", branch=BRANCH)
    github_put_file(REPO, INDEX_HTML_PATH, GH_TOKEN, index_html,
                    message="chore: update index.html", branch=BRANCH)
    github_put_file(REPO, MANIFEST_PATH, GH_TOKEN, json.dumps(manifest, ensure_ascii=False, indent=2),
                    message="chore: update manifest", branch=BRANCH)
    github_put_file(REPO, SEARCH_INDEX_PATH, GH_TOKEN, json.dumps(search_index, ensure_ascii=False),
                    message="chore: update search index", branch=BRANCH)

    print("[INFO] GitHub upload done.")

    # Optional Kakao
    if KAKAO_REST_API_KEY and KAKAO_REFRESH_TOKEN:
        msg = build_kakao_text(report_date, site_url, by_section)
        ok = kakao_send_memo(msg)
        print("[INFO] Kakao sent:", ok)
    else:
        print("[INFO] Kakao skipped (no keys).")

if __name__ == "__main__":
    main()
