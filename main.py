# main.py
# -*- coding: utf-8 -*-

from __future__ import annotations

import os, re, json, time, base64, html, logging
from dataclasses import dataclass
from datetime import datetime, timedelta, date
from email.utils import parsedate_to_datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse

import requests
from zoneinfo import ZoneInfo

KST = ZoneInfo("Asia/Seoul")
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# =========================
# ìš´ì˜ íŒŒë¼ë¯¸í„°
# =========================
RUN_HOUR_KST = int(os.getenv("RUN_HOUR_KST", "7"))
EARLY_GRACE_MINUTES = int(os.getenv("EARLY_GRACE_MINUTES", "20"))
FORCE_SEND = (os.getenv("FORCE_SEND", "0") == "1")

MAX_ARTICLES_PER_SECTION = int(os.getenv("MAX_ARTICLES_PER_SECTION", "10"))
MIN_ARTICLES_PER_SECTION = int(os.getenv("MIN_ARTICLES_PER_SECTION", "7"))
GLOBAL_BACKFILL_LIMIT = int(os.getenv("GLOBAL_BACKFILL_LIMIT", "140"))
MAX_PAGES_PER_QUERY = int(os.getenv("MAX_PAGES_PER_QUERY", "3"))  # ë„¤ì´ë²„ API í˜ì´ì§€(50ê°œì”©)

PUBLISH_MODE = os.getenv("PUBLISH_MODE", "github_pages")
PAGES_BRANCH = os.getenv("PAGES_BRANCH", "main")
PAGES_LATEST_PATH = os.getenv("PAGES_LATEST_PATH", "docs/index.html")  # ìµœì‹ 
ARCHIVE_DIR = os.getenv("ARCHIVE_DIR", "docs/archive")                # ì¼ìë³„ ì €ì¥
ARCHIVE_INDEX_PATH = os.getenv("ARCHIVE_INDEX_PATH", "docs/archive/index.html")
ARCHIVE_MANIFEST_PATH = os.getenv("ARCHIVE_MANIFEST_PATH", ".agri_archive.json")

STATE_BACKEND = os.getenv("STATE_BACKEND", "repo")
STATE_FILE_PATH = os.getenv("STATE_FILE_PATH", ".agri_state.json")

BRIEF_VIEW_URL = os.getenv("BRIEF_VIEW_URL", "").strip()  # ë¹„ìš°ë©´ ìë™ pages ì£¼ì†Œ ì‚¬ìš©
KAKAO_MESSAGE_SOFT_LIMIT = int(os.getenv("KAKAO_MESSAGE_SOFT_LIMIT", "360"))

# =========================
# Kakao
# =========================
KAKAO_TOKEN_URL = "https://kauth.kakao.com/oauth/token"
KAKAO_MEMO_SEND_API = "https://kapi.kakao.com/v2/api/talk/memo/default/send"

# =========================
# Naver OpenAPI
# =========================
NAVER_NEWS_API = "https://openapi.naver.com/v1/search/news.json"

# =========================
# OpenAI
# =========================
OPENAI_RESPONSES_URL = "https://api.openai.com/v1/responses"
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-5.2")
OPENAI_REASONING_EFFORT = os.getenv("OPENAI_REASONING_EFFORT", "minimal")
OPENAI_MAX_OUTPUT_TOKENS = int(os.getenv("OPENAI_MAX_OUTPUT_TOKENS", "2200"))

# =========================
# ì„¹ì…˜(ìˆœì„œ ê³ ì •)
# =========================
SECTION_ORDER: List[str] = [
    "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥",
    "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…",
    "ë³‘í•´ì¶© ë° ë°©ì œ",
    "ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)",
]

# ==========================================================
# 1) í‚¤ì›Œë“œ ì „ë©´ ì¬ì¡°ì •(ì˜¤ì—¼/ë‹¤ì˜ì–´ ì œê±° + ë†ì—… ì•µì»¤ ê°•í™”)
# ==========================================================
# ë‹¤ì˜ì–´/ì˜¤ì—¼ ê°€ëŠ¥ ë‹¨ì–´ëŠ” ì›í˜•(ê°, ë°¤ ë“±) ëŒ€ì‹  â€œêµ¬ì²´ í’ˆëª©â€ìœ¼ë¡œ
FRUITS = [
    "ì‚¬ê³¼","ë°°","ì‹ ê³ ë°°",
    "ë‹¨ê°","ë–«ì€ê°","ê³¶ê°",
    "ê°ê·¤","ë§Œê°ë¥˜","í•œë¼ë´‰","ë ˆë“œí–¥","ì²œí˜œí–¥",
    "ì°¸ë‹¤ë˜","í‚¤ìœ„",
    "í¬ë„","ìƒ¤ì¸ë¨¸ìŠ¤ìº£",
    "ë”¸ê¸°","ë³µìˆ­ì•„","ìë‘","ë§¤ì‹¤","ìœ ì",
    "ì•Œë°¤",
]
VEGGIES = [
    "ì˜¤ì´","í’‹ê³ ì¶”","ì• í˜¸ë°•","í† ë§ˆí† ","íŒŒí”„ë¦¬ì¹´","ê°€ì§€",
    "ìƒì¶”","ê¹»ì","ë°°ì¶”","ë¬´","ì–‘íŒŒ","ëŒ€íŒŒ","ë§ˆëŠ˜","ê°ì","ê³ êµ¬ë§ˆ",
]
FLOWERS = ["ì ˆí™”","í™”í›¼","ê½ƒê°’","êµ­í™”","ì¥ë¯¸","ë°±í•©","í”„ë¦¬ì§€ì•„"]
STAPLES = ["ìŒ€","ì‚°ì§€ìŒ€ê°’","ë¹„ì¶•ë¯¸"]

# ë†ì—… ì•µì»¤(ì´ê²Œ ìµœì†Œ 1ê°œëŠ” ìˆì–´ì•¼ â€œì—…ë¬´ ê´€ë ¨â€ìœ¼ë¡œ ì¸ì •)
AGRI_ANCHORS = [
    "ë†ì‚°ë¬¼","ë†ì—…","ë†ê°€","ê³¼ìˆ˜","ì›ì˜ˆ","ì²­ê³¼","ì‚°ì§€","ë„ë§¤","ê²½ë½","ê²½ë§¤",
    "ì¶œí•˜","ì‘í™©","ì¬ë°°","ìˆ˜í™•","ì €ì¥ëŸ‰","ì¬ê³ ",
    "ê°€ë½ì‹œì¥","ê³µíŒì¥","ë„ë§¤ì‹œì¥","ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥",
    "APC","ì‚°ì§€ìœ í†µ","ì„ ë³„","CAì €ì¥","ì €ì¥ê³ ",
    "ìˆ˜ì¶œ","ê²€ì—­","í• ë‹¹ê´€ì„¸",
    "ê³¼ìˆ˜í™”ìƒë³‘","í™”ìƒë³‘","íƒ„ì €ë³‘","ë³‘í•´ì¶©","ë°©ì œ","ì•½ì œ","ì˜ˆì°°",
    "ì›ì‚°ì§€","ë†ì‘ë¬¼ì¬í•´ë³´í—˜","ì¬í•´ë³´í—˜","ì‹œì„¤ì›ì˜ˆ","ì‹œì„¤ì±„ì†Œ",
    "ì ˆí™”","í™”í›¼",
    "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€","ì •ì±…ë¸Œë¦¬í•‘","aT","ë†ê´€ì›",
]

# ì—…ë¬´ í•µì‹¬(ìˆ˜ê¸‰Â·ê°€ê²©Â·ì •ì±…Â·ë°©ì œÂ·ìœ í†µ ë“± â€œì—…ë¬´ì„±â€ì„ í™•ì¸í•˜ëŠ” í‚¤)
WORK_SIGNALS = [
    "ìˆ˜ê¸‰","ê°€ê²©","ì‹œì„¸","ë¬¼ëŸ‰","ìƒì‚°","ìƒì‚°ëŸ‰","ì¶œí•˜","ë„ë§¤","ê²½ë½","ê²½ë§¤",
    "ì €ì¥ëŸ‰","ì¬ê³ ","ì‘í™©","ì¬ë°°","ìˆ˜í™•",
    "ë‹¨ì†","ì¡°ì‚¬","ì „ìˆ˜ì¡°ì‚¬","í—ˆìœ„ê±°ë˜","ì´ìƒê±°ë˜","ì‹¤ì  ë¶€í’€ë¦¬ê¸°",
    "í• ì¸","ë¬¼ê°€","ëŒ€ì±…","ì§€ì›","ë°©ì¶œ","ë¹„ì¶•",
    "ê²€ì—­","í• ë‹¹ê´€ì„¸","ìˆ˜ì…","ìˆ˜ì¶œ",
    "ë°©ì œ","ì•½ì œ","ì˜ˆì°°","í”¼í•´","ë™í•´","ëƒ‰í•´","ì„œë¦¬","ê°€ë­„","í­ì—¼",
    "APC","ì„ ë³„","ì €ì¥","CA",
]

# â€œì €ì¥â€ì€ ë‹¤ì˜ì–´ê°€ ì‹¬í•˜ë¯€ë¡œ â€œì €ì¥ëŸ‰/ì €ì¥ê³ /CAì €ì¥â€ë§Œ ì¸ì • (ì €ì¥ ë‹¨ë…ì€ ì œê±°)
BANNED_PHRASES = [
    "ë‚´ ë§ˆìŒì†ì— ì €ì¥",  # ì—°ì˜ˆ/ì¸í„°ë·°ì—ì„œ ìì£¼ ë“±ì¥
    "ì €ì¥ì„±",             # ì¤‘êµ­ ì €ì¥(æµ™æ±Ÿ)
]

# ì—°ì˜ˆ/ìŠ¤í¬ì¸ /ì •ì¹˜/ë¶€ë™ì‚°/IT/ë‚ ì”¨ ë“± ê°•í•œ ë…¸ì´ì¦ˆ ì‹ í˜¸
NOISE_TOPICS = [
    "ë°°ìš°","ì•„ì´ëŒ","ë“œë¼ë§ˆ","ì˜í™”","ì˜ˆëŠ¥","ì½˜ì„œíŠ¸","ë®¤ì§€ì»¬","íŒ¬",
    "ì•¼êµ¬","ì¶•êµ¬","ë†êµ¬","ê³¨í”„","ì„ ìˆ˜","ê²½ê¸°",
    "ì½”ìŠ¤í”¼","ì£¼ê°€","ì¦ì‹œ","ìƒì¥","ë°˜ë„ì²´","í•˜ì´í…Œí¬","ì „ê¸°ì°¨","AI ë¸Œë¼ìš°ì €","ê²€ìƒ‰ íŒ",
    "ë¶€ë™ì‚°","ì§‘ê°’","ì•„íŒŒíŠ¸","ì²­ì•½",
    "ëŒ€í†µë ¹","êµ­íšŒ","ì´ì„ ","ëŒ€ì„ ","ì •ì¹˜",
    "ë‚ ì”¨","ê¸°ìƒ","ê°•í’","ë¯¸ì„¸ë¨¼ì§€","í’ë‘",
]

# ì‚¬íšŒ ë¯¸ë‹´/ê¸°ë¶€ë¥˜ëŠ” ì›ì˜ˆìˆ˜ê¸‰ë¶€ ì—…ë¬´ì™€ ê±°ë¦¬ê°€ ë©€ì–´ ìš°ì„  ì œì™¸(ì •ì±…/ìˆ˜ê¸‰ ê¸°ì‚¬ì™€ êµ¬ë¶„)
LOW_VALUE_HINTS = ["ê¸°ë¶€","ë‚˜ëˆ”","ë´‰ì‚¬","ì°¨ìƒìœ„","ìˆ˜ê¸‰ì","ëª…ì ˆ ì„ ë¬¼","ë¬´ë£Œ ë°°í¬","í›„ì›","ìº í˜ì¸"]

# ----------------------------------------------------------
# ì„¹ì…˜ë³„ ì¿¼ë¦¬ ìƒì„±: ëª¨ë“  ì¿¼ë¦¬ëŠ” ë†ì—… ì•µì»¤ í¬í•¨(â€œë¬¼ê°€â€ ë‹¨ë… ê¸ˆì§€)
# ----------------------------------------------------------
STRUCTURAL_QUERIES = [
    "ê¸°í›„ë³€í™” ê³¼ìˆ˜ ì¬ë°°ì§€ ë¶ìƒ",
    "ì‚¬ê³¼ ì¬ë°°ì§€ ë¶ìƒ ê°•ì› ê³¼ìˆ˜",
    "ê³¼ìˆ˜ ë™í•´ í”¼í•´ ë†ê°€",
    "ì‹œì„¤ì›ì˜ˆ ì¼ì¡°ëŸ‰ ë¶€ì¡± ìˆ˜ê¸‰",
]

POLICY_CORE = [
    "ë†ì‚°ë¬¼ ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥ í—ˆìœ„ê±°ë˜",
    "ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥ ì´ìƒê±°ë˜ ì „ìˆ˜ì¡°ì‚¬",
    "ê°€ë½ì‹œì¥ íœ´ë¬´ ê²½ë§¤ ì¬ê°œ ì²­ê³¼",
    "ë†ì‚°ë¬¼ ë¬¼ê°€ ëŒ€ì±… í• ì¸",
    "ë†ì¶•ì‚°ë¬¼ í• ì¸ì§€ì› ì—°ì¥",
    "ìˆ˜ì… ê³¼ì¼ í• ë‹¹ê´€ì„¸ ê²€ì—­",
    "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€ ë†ì‚°ë¬¼ ëŒ€ì±…",
    "ì •ì±…ë¸Œë¦¬í•‘ ë†ì‚°ë¬¼ ë¬¼ê°€",
]

PEST_CORE = [
    "ê³¼ìˆ˜í™”ìƒë³‘ ë°©ì œ ì•½ì œ ì‹ ì²­",
    "ê³¼ìˆ˜í™”ìƒë³‘ ê¶¤ì–‘ ì œê±° ê³¨ë“ íƒ€ì„",
    "ì›”ë™í•´ì¶© ë°©ì œ ê¸°ê³„ìœ ìœ ì œ ê³¼ìˆ˜",
    "íƒ„ì €ë³‘ ì˜ˆë°© ë°©ì œ ê³¼ìˆ˜",
    "ëƒ‰í•´ ëŒ€ë¹„ ê³¼ìˆ˜ ë°©ì œ",
    "ë™í•´ í”¼í•´ ê³¼ìˆ˜ ë†ê°€",
]

DIST_CORE = [
    "ë†í˜‘ APC ìŠ¤ë§ˆíŠ¸ ì„ ë³„",
    "ì‚°ì§€ìœ í†µì„¼í„° APC CAì €ì¥",
    "ê°€ë½ì‹œì¥ ì²­ê³¼ ë„ë§¤ì‹œì¥ ë¬¼ëŸ‰",
    "ë†ì‹í’ˆ ìˆ˜ì¶œ ê³¼ì¼ ë°° ë”¸ê¸°",
    "ë†ì‚°ë¬¼ ìˆ˜ì¶œ ê²€ì—­ ë¬¼ë¥˜",
]

def uniq_keep_order(xs: List[str]) -> List[str]:
    seen = set()
    out = []
    for x in xs:
        x = re.sub(r"\s+", " ", (x or "").strip())
        if not x or x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out

def build_supply_queries() -> List[str]:
    qs: List[str] = []
    qs += STRUCTURAL_QUERIES

    def add_item(item: str):
        # âœ… â€œì €ì¥â€ ë‹¨ë… ê¸ˆì§€ â†’ ì €ì¥ëŸ‰/ì €ì¥ê³ /CAì €ì¥ë§Œ
        qs.extend([
            f"{item} ë†ì‚°ë¬¼ ìˆ˜ê¸‰",
            f"{item} ì‚°ì§€ ì¶œí•˜",
            f"{item} ë„ë§¤ ê²½ë½",
            f"{item} ê°€ë½ì‹œì¥ ì²­ê³¼",
            f"{item} ê°€ê²© ì‹œì„¸",
            f"{item} ì €ì¥ëŸ‰",
            f"{item} ì‘í™© ì¬ë°°",
        ])

    for it in FRUITS + VEGGIES + FLOWERS + STAPLES:
        add_item(it)

    # ì¹´í…Œê³ ë¦¬/ì‹œì¥ ë‹¨ìœ„
    qs += [
        "ì²­ê³¼ ë„ë§¤ê°€ê²© ê²½ë½",
        "ê³¼ì¼ ë„ë§¤ì‹œì¥ ë¬¼ëŸ‰",
        "ì‹œì„¤ì±„ì†Œ ìˆ˜ê¸‰ ê°€ê²©",
        "ì ˆí™” í™”í›¼ ê°€ê²©",
        "ë§Œê°ë¥˜ ì¶œí•˜ ìˆ˜ê¸‰",
    ]
    return uniq_keep_order(qs)

def build_policy_queries() -> List[str]:
    return uniq_keep_order(POLICY_CORE + [
        "ë†ì‚°ë¬¼ ë¬¼ê°€ ì•ˆì • ëŒ€ì±…",
        "ë†ì¶•ì‚°ë¬¼ í• ì¸ í–‰ì‚¬ ì§€ì›",
        "í• ë‹¹ê´€ì„¸ ìˆ˜ì…ê³¼ì¼ ì‹œì¥ ì˜í–¥",
        "ì›ì‚°ì§€ ë‹¨ì† ë†ì‚°ë¬¼",
    ])

def build_pest_queries() -> List[str]:
    return uniq_keep_order(PEST_CORE + [
        "ê³¼ìˆ˜ ë³‘í•´ì¶© ì˜ˆì°° ë°©ì œ",
        "ì‹œì„¤ì›ì˜ˆ ë³‘í•´ì¶© ë°©ì œ",
        "ë†ì‘ë¬¼ì¬í•´ë³´í—˜ ê³¼ìˆ˜ ì‚¬ê³¼ ë°°",
    ])

def build_dist_queries() -> List[str]:
    return uniq_keep_order(DIST_CORE + [
        "APC ì„ ë³„ ì €ì¥ê³ ",
        "ì‚°ì§€ìœ í†µ í˜ì‹  ë†í˜‘",
        "ê³µíŒì¥ ì²­ê³¼ ê²½ë§¤",
        "ë†ì‹í’ˆ ìˆ˜ì¶œ ì‹¤ì  ê³¼ì¼",
    ])

SECTION_QUERIES: Dict[str, List[str]] = {
    "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥": build_supply_queries(),
    "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…": build_policy_queries(),
    "ë³‘í•´ì¶© ë° ë°©ì œ": build_pest_queries(),
    "ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)": build_dist_queries(),
}

GLOBAL_BACKFILL_QUERIES = uniq_keep_order([
    "ë†ì‚°ë¬¼ ìˆ˜ê¸‰ ê°€ê²©",
    "ì²­ê³¼ ë„ë§¤ ê²½ë½",
    "ê°€ë½ì‹œì¥ ì²­ê³¼ ë¬¼ëŸ‰",
    "ê³¼ìˆ˜ ë™í•´ ëƒ‰í•´ í”¼í•´",
    "ê³¼ìˆ˜í™”ìƒë³‘ ë°©ì œ ì•½ì œ",
    "ë†ì‚°ë¬¼ ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥",
    "ë†ì¶•ì‚°ë¬¼ í• ì¸ ë¬¼ê°€",
    "ë†ì‹í’ˆ ìˆ˜ì¶œ ê³¼ì¼",
    "ë†í˜‘ APC ì‚°ì§€ìœ í†µ",
])

# =========================
# 2) ë§¤ì²´ ì •ì±…: ì°¨ë‹¨ ëª©ë¡ + ë‚˜ë¨¸ì§€ëŠ” ì ìˆ˜í™”(ì§€ë°©ì§€/ë°©ì†¡ í¬í•¨)
# =========================
BLOCKED_DOMAINS = {
    "wikitree.co.kr", "donghaengmedia.net", "sidae.com",
    "namu.wiki", "blog.naver.com", "post.naver.com",
}

TRUSTED_DOMAINS = {
    # í†µì‹ /ì •ì±…/ê³µê³µ
    "yna.co.kr", "newsis.com", "korea.kr", "mafra.go.kr", "at.or.kr", "krei.re.kr", "naqs.go.kr",
    # ì¤‘ì•™/ê²½ì œ/ì¢…í•©
    "mk.co.kr", "mt.co.kr", "hankyung.com", "sedaily.com", "edaily.co.kr", "asiae.co.kr", "heraldcorp.com",
    "joongang.co.kr", "donga.com", "hani.co.kr", "khan.co.kr", "chosun.com",
    # ì¤‘ê²¬
    "fnnews.com", "kmib.co.kr", "munhwa.com", "segye.com", "dt.co.kr", "nocutnews.co.kr", "news1.kr",
    # ë°©ì†¡(ì „êµ­)
    "kbs.co.kr","imbc.com","sbs.co.kr","jtbc.co.kr","ytn.co.kr","mbn.co.kr","yonhapnewstv.co.kr",
    # ë†ì—… ì „ë¬¸
    "nongmin.com","ikpnews.net","aflnews.co.kr",
    # ì§€ë°©ì§€/ì§€ë°©ë°©ì†¡(ëŒ€í‘œ)
    "kwnews.co.kr","kado.net","kyeonggi.com","joongboo.com","cctoday.co.kr","imaeil.com","yeongnam.com",
    "gnnews.co.kr","namdonews.com","jeonmae.co.kr",
    "g1tv.co.kr","cjb.co.kr","tjb.co.kr","kbc.co.kr","jibs.co.kr","obn.co.kr",
}

PRESS_MAP = {
    "yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "newsis.com": "ë‰´ì‹œìŠ¤",
    "korea.kr": "ì •ì±…ë¸Œë¦¬í•‘",
    "mafra.go.kr": "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€",
    "at.or.kr": "aT",
    "krei.re.kr": "KREI",
    "naqs.go.kr": "ë†ê´€ì›",
    "mk.co.kr": "ë§¤ì¼ê²½ì œ",
    "mt.co.kr": "ë¨¸ë‹ˆíˆ¬ë°ì´",
    "hankyung.com": "í•œêµ­ê²½ì œ",
    "sedaily.com": "ì„œìš¸ê²½ì œ",
    "edaily.co.kr": "ì´ë°ì¼ë¦¬",
    "asiae.co.kr": "ì•„ì‹œì•„ê²½ì œ",
    "heraldcorp.com": "í—¤ëŸ´ë“œê²½ì œ",
    "joongang.co.kr": "ì¤‘ì•™ì¼ë³´",
    "donga.com": "ë™ì•„ì¼ë³´",
    "hani.co.kr": "í•œê²¨ë ˆ",
    "khan.co.kr": "ê²½í–¥ì‹ ë¬¸",
    "chosun.com": "ì¡°ì„ ì¼ë³´",
    "fnnews.com": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
    "kmib.co.kr": "êµ­ë¯¼ì¼ë³´",
    "munhwa.com": "ë¬¸í™”ì¼ë³´",
    "segye.com": "ì„¸ê³„ì¼ë³´",
    "dt.co.kr": "ë””ì§€í„¸íƒ€ì„ìŠ¤",
    "nocutnews.co.kr": "ë…¸ì»·ë‰´ìŠ¤",
    "news1.kr": "ë‰´ìŠ¤1",
    "yonhapnewstv.co.kr": "ì—°í•©ë‰´ìŠ¤TV",
    "ytn.co.kr": "YTN",
    "mbn.co.kr": "MBN",
    "nongmin.com": "ë†ë¯¼ì‹ ë¬¸",
    "aflnews.co.kr": "ë†ìˆ˜ì¶•ì‚°ì‹ ë¬¸",
    "ikpnews.net": "í•œêµ­ë†ì–´ë¯¼ì‹ ë¬¸",
    "kwnews.co.kr": "ê°•ì›ì¼ë³´",
    "kado.net": "ê°•ì›ë„ë¯¼ì¼ë³´",
    "kyeonggi.com": "ê²½ê¸°ì¼ë³´",
    "joongboo.com": "ì¤‘ë¶€ì¼ë³´",
    "cctoday.co.kr": "ì¶©ì²­íˆ¬ë°ì´",
    "imaeil.com": "ë§¤ì¼ì‹ ë¬¸",
    "yeongnam.com": "ì˜ë‚¨ì¼ë³´",
    "gnnews.co.kr": "ê²½ë‚¨ì‹ ë¬¸",
    "namdonews.com": "ë‚¨ë„ì¼ë³´",
    "jeonmae.co.kr": "ì „êµ­ë§¤ì¼ì‹ ë¬¸",
    "g1tv.co.kr": "G1",
    "cjb.co.kr": "CJB",
    "tjb.co.kr": "TJB",
    "kbc.co.kr": "kbc",
    "jibs.co.kr": "JIBS",
    "obn.co.kr": "OBN",
}

# =========================
# íœ´ì¼/ì˜ì—…ì¼ ìœ í‹¸
# =========================
def is_weekend(d: date) -> bool:
    return d.weekday() >= 5

def is_korean_holiday(d: date) -> bool:
    try:
        import holidays  # type: ignore
        return d in holidays.KR()
    except Exception:
        return False

def is_business_day(d: date) -> bool:
    return (not is_weekend(d)) and (not is_korean_holiday(d))

def compute_fixed_end_kst(now_kst: datetime, run_hour: int, early_grace_minutes: int) -> datetime:
    today_end = now_kst.replace(hour=run_hour, minute=0, second=0, microsecond=0)
    if now_kst >= today_end:
        return today_end
    if (today_end - now_kst) <= timedelta(minutes=early_grace_minutes):
        return today_end
    return today_end - timedelta(days=1)

def clean_html(s: str) -> str:
    s = html.unescape(s or "")
    s = re.sub(r"<[^>]+>", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def domain_of(url: str) -> str:
    try:
        host = (urlparse(url).hostname or "").lower()
        return host[4:] if host.startswith("www.") else host
    except Exception:
        return ""

def press_name(dom: str) -> str:
    if not dom:
        return "ë¯¸ìƒ"
    if dom in PRESS_MAP:
        return PRESS_MAP[dom]
    for k, v in PRESS_MAP.items():
        if dom.endswith("." + k):
            return v
    return dom

def has_any(text: str, keys: List[str]) -> bool:
    return any(k in text for k in keys)

def is_blocked_domain(dom: str) -> bool:
    if not dom:
        return True
    if dom in BLOCKED_DOMAINS:
        return True
    return False

def is_trusted_domain(dom: str) -> bool:
    if not dom:
        return False
    if dom in TRUSTED_DOMAINS:
        return True
    if dom.endswith(".go.kr") or dom.endswith(".or.kr"):
        return True
    return False

def clamp(s: str, n: int) -> str:
    return s if len(s) <= n else (s[: max(0, n-1)] + "â€¦")

# =========================
# âœ… í•µì‹¬: ì—…ë¬´ ê´€ë ¨ì„± ê²Œì´íŠ¸(ë¬´ê´€ ê¸°ì‚¬ ì „ë©´ ì°¨ë‹¨)
# =========================
def agri_relevance_ok(title: str, desc: str) -> bool:
    t = f"{title} {desc}"

    # ê¸ˆì§€ ë¬¸êµ¬(ì €ì¥ì„±/ë‚´ ë§ˆìŒì† ì €ì¥ ë“±) ì¦‰ì‹œ ì»·
    if has_any(t, BANNED_PHRASES):
        return False

    # ì‚¬íšŒ ë¯¸ë‹´/ê¸°ë¶€ë¥˜ëŠ” ëŒ€ë¶€ë¶„ ì—…ë¬´ì™€ ë¬´ê´€ â†’ ì»·(ë‹¨, ê°€ê²©/ìˆ˜ê¸‰/ë‹¨ì† ì‹ í˜¸ê°€ ê°•í•˜ë©´ ì˜ˆì™¸)
    if has_any(t, LOW_VALUE_HINTS) and (not has_any(t, ["ê°€ê²©","ì‹œì„¸","ìˆ˜ê¸‰","ë„ë§¤","ê²½ë½","ë‹¨ì†","ì¡°ì‚¬","ì „ìˆ˜ì¡°ì‚¬"])):
        return False

    # ë†ì—… ì•µì»¤ 1ê°œ + ì—…ë¬´ ì‹ í˜¸ 1ê°œë¥¼ â€œí•„ìˆ˜â€ë¡œ ìš”êµ¬
    has_anchor = has_any(t, AGRI_ANCHORS)
    has_work = has_any(t, WORK_SIGNALS)

    if not (has_anchor and has_work):
        return False

    # ë…¸ì´ì¦ˆ í† í”½ì´ ê°•í•˜ë©´ ì¶”ê°€ ê²€ì¦: ë†ì—… ì•µì»¤ê°€ ì•½í•˜ë©´ ì»·
    if has_any(t, NOISE_TOPICS):
        strong_agri = has_any(t, [
            "ë†ì‚°ë¬¼","ê³¼ìˆ˜","ì›ì˜ˆ","ì²­ê³¼","ê°€ë½ì‹œì¥","ê³µíŒì¥","ë„ë§¤","ê²½ë½","ì¶œí•˜","ì €ì¥ëŸ‰","ì¬ê³ ",
            "APC","ì‚°ì§€ìœ í†µ","ê²€ì—­","í• ë‹¹ê´€ì„¸","ìˆ˜ì¶œ","í™”ìƒë³‘","íƒ„ì €ë³‘","ë°©ì œ","ë³‘í•´ì¶©"
        ])
        if not strong_agri:
            return False

    # â€œì‚¬ê³¼/ë°°â€ ë‹¤ì˜ì–´ ë³´í˜¸(ë†ì—… ë§¥ë½ì´ ë” ê°•í•´ì•¼ í†µê³¼)
    if "ì‚¬ê³¼" in t and (not has_any(t, ["ê³¼ìˆ˜","ë†ì‚°ë¬¼","ì‚°ì§€","ë„ë§¤","ê²½ë½","ì¶œí•˜","ì¬ë°°","ì‘í™©","ì €ì¥ëŸ‰","ì²­ê³¼","ê°€ë½ì‹œì¥"])):
        return False
    # ë°°(ì„ ë°•/ë°°ìš° ë“±) ì˜¤ì—¼ ë°©ì§€: â€œê³¼ìˆ˜/ê³¼ì¼/ì²­ê³¼/ì‹ ê³ ë°°/ê²½ë½â€ ê°™ì€ ë‹¨ì„œ ìš”êµ¬
    if " ë°°" in (" " + t) and (("ë°°" in t) and (not has_any(t, ["ê³¼ìˆ˜","ê³¼ì¼","ì²­ê³¼","ì‹ ê³ ë°°","ì›í™©","ì‚°ì§€","ë„ë§¤","ê²½ë½","ì¶œí•˜","ì €ì¥ëŸ‰","ê°€ë½ì‹œì¥"]))):
        # (ë„ì–´ì“°ê¸° ê¸°ë°˜ ì™„ì „íŒì€ ì•„ë‹ˆì§€ë§Œ ì˜¤ì—¼ì„ í¬ê²Œ ì¤„ì„)
        pass

    # â€œì €ì¥â€ì€ ì €ì¥ëŸ‰/ì €ì¥ê³ /CAì €ì¥ ë§¥ë½ì´ë©´ OK, ê·¸ ì™¸ ì €ì¥ ë‹¨ë…ì€ ì‹ í˜¸ë¡œ ë³´ì§€ ì•ŠìŒ(ì´ë¯¸ ì¿¼ë¦¬ì—ì„œ ì œê±°)
    return True

# =========================
# GitHub repo íŒŒì¼ read/write
# =========================
def github_get_file(repo: str, path: str, token: str, ref: str = "main") -> Tuple[Optional[str], Optional[str]]:
    url = f"https://api.github.com/repos/{repo}/contents/{path}?ref={ref}"
    r = requests.get(url, headers={"Authorization": f"Bearer {token}"}, timeout=20)
    if r.status_code == 404:
        return None, None
    if not r.ok:
        logging.error("[GitHub GET ERROR] %s", r.text)
        return None, None
    j = r.json()
    sha = j.get("sha")
    b64 = j.get("content", "")
    if j.get("encoding") == "base64" and b64:
        raw = base64.b64decode(b64).decode("utf-8", errors="replace")
        return raw, sha
    return None, sha

def github_put_file(repo: str, path: str, token: str, content_text: str,
                    branch: str = "main", sha: Optional[str] = None, message: str = "Update file") -> bool:
    url = f"https://api.github.com/repos/{repo}/contents/{path}"
    b64 = base64.b64encode(content_text.encode("utf-8")).decode("ascii")
    payload = {"message": message, "content": b64, "branch": branch}
    if sha:
        payload["sha"] = sha
    r = requests.put(url, headers={"Authorization": f"Bearer {token}"}, json=payload, timeout=20)
    if not r.ok:
        logging.error("[GitHub PUT ERROR] %s", r.text)
        return False
    return True

@dataclass
class State:
    last_end_kst_iso: str

def load_state(default_last_end: datetime) -> State:
    fallback = State(last_end_kst_iso=(default_last_end - timedelta(hours=24)).isoformat())
    if STATE_BACKEND != "repo":
        return fallback
    repo = os.getenv("GITHUB_REPOSITORY", "")
    token = os.getenv("GITHUB_TOKEN", "")
    if not repo or not token:
        return fallback
    raw, _ = github_get_file(repo, STATE_FILE_PATH, token, ref=PAGES_BRANCH)
    if not raw:
        return fallback
    try:
        j = json.loads(raw)
        v = j.get("last_end_kst_iso")
        return State(last_end_kst_iso=v) if v else fallback
    except Exception:
        return fallback

def save_state(end_kst: datetime) -> None:
    if STATE_BACKEND != "repo":
        return
    repo = os.getenv("GITHUB_REPOSITORY", "")
    token = os.getenv("GITHUB_TOKEN", "")
    if not repo or not token:
        return
    raw, sha = github_get_file(repo, STATE_FILE_PATH, token, ref=PAGES_BRANCH)
    _ = raw
    content = json.dumps({"last_end_kst_iso": end_kst.isoformat()}, ensure_ascii=False, indent=2)
    github_put_file(repo, STATE_FILE_PATH, token, content, branch=PAGES_BRANCH, sha=sha, message="Update agri-news state")

def load_archive_manifest() -> List[dict]:
    repo = os.getenv("GITHUB_REPOSITORY", "")
    token = os.getenv("GITHUB_TOKEN", "")
    if not repo or not token:
        return []
    raw, _ = github_get_file(repo, ARCHIVE_MANIFEST_PATH, token, ref=PAGES_BRANCH)
    if not raw:
        return []
    try:
        j = json.loads(raw)
        if isinstance(j, list):
            return j
        return []
    except Exception:
        return []

def save_archive_manifest(items: List[dict]) -> None:
    repo = os.getenv("GITHUB_REPOSITORY", "")
    token = os.getenv("GITHUB_TOKEN", "")
    if not repo or not token:
        return
    raw, sha = github_get_file(repo, ARCHIVE_MANIFEST_PATH, token, ref=PAGES_BRANCH)
    _ = raw
    content = json.dumps(items, ensure_ascii=False, indent=2)
    github_put_file(repo, ARCHIVE_MANIFEST_PATH, token, content, branch=PAGES_BRANCH, sha=sha, message="Update archive manifest")

# =========================
# Kakao
# =========================
def kakao_refresh_access_token(refresh_token: str) -> str:
    rest_api_key = os.getenv("KAKAO_REST_API_KEY", "").strip()
    client_secret = os.getenv("KAKAO_CLIENT_SECRET", "").strip()
    if not rest_api_key or not client_secret:
        raise RuntimeError("Missing KAKAO_REST_API_KEY / KAKAO_CLIENT_SECRET")
    data = {
        "grant_type": "refresh_token",
        "client_id": rest_api_key,
        "client_secret": client_secret,
        "refresh_token": refresh_token,
    }
    r = requests.post(KAKAO_TOKEN_URL, data=data, timeout=20)
    if not r.ok:
        logging.error("[Kakao token ERROR] %s", r.text)
        r.raise_for_status()
    access = r.json().get("access_token")
    if not access:
        raise RuntimeError("Kakao access_token missing")
    return access

def kakao_send_text(access_token: str, text: str, link_url: str) -> None:
    template_object = {
        "object_type": "text",
        "text": text,
        "link": {"web_url": link_url, "mobile_web_url": link_url},
        "button_title": "ë¸Œë¦¬í•‘ ì—´ê¸°",
    }
    r = requests.post(
        KAKAO_MEMO_SEND_API,
        headers={"Authorization": f"Bearer {access_token}"},
        data={"template_object": json.dumps(template_object, ensure_ascii=False)},
        timeout=20,
    )
    if not r.ok:
        logging.error("[Kakao send ERROR] %s", r.text)
    r.raise_for_status()

# =========================
# Naver OpenAPI
# =========================
def naver_api_search(query: str, display: int = 50, start: int = 1) -> List[dict]:
    cid = os.getenv("NAVER_CLIENT_ID", "").strip()
    csec = os.getenv("NAVER_CLIENT_SECRET", "").strip()
    if not cid or not csec:
        raise RuntimeError("Missing NAVER_CLIENT_ID / NAVER_CLIENT_SECRET")
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    params = {"query": query, "display": display, "start": start, "sort": "date"}
    r = requests.get(NAVER_NEWS_API, headers=headers, params=params, timeout=20)
    r.raise_for_status()
    return r.json().get("items", [])

def naver_search_window(query: str, start_kst: datetime, end_kst: datetime, max_pages: int) -> List[dict]:
    collected: List[dict] = []
    display = 50
    start_idx = 1

    for _ in range(max_pages):
        items = naver_api_search(query, display=display, start=start_idx)
        if not items:
            break

        stop_early = False
        for it in items:
            pub = it.get("pubDate", "")
            try:
                dt = parsedate_to_datetime(pub).astimezone(KST)
            except Exception:
                continue

            if dt < start_kst:
                stop_early = True
                continue
            if not (start_kst <= dt < end_kst):
                continue

            title = clean_html(it.get("title", ""))
            desc = clean_html(it.get("description", ""))
            origin = (it.get("originallink") or "").strip()
            nlink = (it.get("link") or "").strip()
            url = origin or nlink
            dom = domain_of(url)

            if (not url) or is_blocked_domain(dom):
                continue

            # âœ… ì—…ë¬´ ê´€ë ¨ì„± ê²Œì´íŠ¸ (ë¬´ê´€ ê¸°ì‚¬ ì „ë©´ ì°¨ë‹¨)
            if not agri_relevance_ok(title, desc):
                continue

            collected.append({
                "title": title,
                "description": desc,
                "published_kst": dt.isoformat(),
                "published_hm": dt.strftime("%m/%d %H:%M"),
                "domain": dom,
                "press": press_name(dom),
                "url": url,
                "query": query,
            })

        if stop_early:
            break

        start_idx += display
        time.sleep(0.05)

    return collected

def dedupe(items: List[dict]) -> List[dict]:
    seen = set()
    out = []
    for it in items:
        u = (it.get("url") or "").strip()
        t = (it.get("title") or "").strip().lower()
        if not u or not t:
            continue
        key = u[:280] + "|" + re.sub(r"\s+", " ", t)[:140]
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def quality_score(it: dict) -> int:
    s = 0
    dom = it.get("domain", "")
    text = f"{it.get('title','')} {it.get('description','')}"
    if is_trusted_domain(dom):
        s += 8
    if dom.endswith(".go.kr") or dom.endswith(".or.kr"):
        s += 2
    # ë†ì—… ì•µì»¤/ì—…ë¬´ ì‹ í˜¸ ê°€ì 
    if has_any(text, AGRI_ANCHORS):
        s += 4
    if has_any(text, WORK_SIGNALS):
        s += 3
    return s

def classify_section(it: dict) -> str:
    t = f"{it.get('title','')} {it.get('description','')}"
    if any(k in t for k in ["ì˜¨ë¼ì¸ ë„ë§¤ì‹œì¥","í—ˆìœ„ê±°ë˜","ì´ìƒê±°ë˜","ì „ìˆ˜ì¡°ì‚¬","í• ë‹¹ê´€ì„¸","ê²€ì—­","í• ì¸","ë¬¼ê°€","ëŒ€ì±…","íœ´ë¬´","ê²½ë§¤ ì¬ê°œ","ê°€ë½ì‹œì¥","ë„ë§¤ì‹œì¥","ì›ì‚°ì§€"]):
        return "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…"
    if any(k in t for k in ["í™”ìƒë³‘","ë³‘í•´ì¶©","ë°©ì œ","ì•½ì œ","íƒ„ì €ë³‘","ê¸°ê³„ìœ ìœ ì œ","ì˜ˆì°°","ë™í•´","ëƒ‰í•´","ì„œë¦¬","ì¬í•´ë³´í—˜"]):
        return "ë³‘í•´ì¶© ë° ë°©ì œ"
    if any(k in t for k in ["APC","ì‚°ì§€ìœ í†µ","ì„ ë³„","CAì €ì¥","ì €ì¥ê³ ","ê³µíŒì¥","ìˆ˜ì¶œ","ë¬¼ë¥˜","ì½œë“œì²´ì¸"]):
        return "ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)"
    return "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥"

def collect_articles(start_kst: datetime, end_kst: datetime) -> Dict[str, List[dict]]:
    buckets: Dict[str, List[dict]] = {s: [] for s in SECTION_ORDER}
    seen_urls: set[str] = set()

    # 1) ì„¹ì…˜ë³„ ì •ë°€ ìˆ˜ì§‘
    for sec in SECTION_ORDER:
        local: List[dict] = []
        for q in SECTION_QUERIES.get(sec, []):
            local.extend(naver_search_window(q, start_kst, end_kst, max_pages=MAX_PAGES_PER_QUERY))
            if len(local) >= MAX_ARTICLES_PER_SECTION * 12:
                break

        local = dedupe(local)
        local.sort(key=lambda x: (quality_score(x), x.get("published_kst","")), reverse=True)

        picked: List[dict] = []
        for it in local:
            u = it["url"]
            if u in seen_urls:
                continue
            picked.append(it)
            seen_urls.add(u)
            if len(picked) >= MAX_ARTICLES_PER_SECTION:
                break

        buckets[sec] = picked
        logging.info("[Collect] %s: %d", sec, len(buckets[sec]))

    # 2) ë°±í•„(ë¶€ì¡± ì„¹ì…˜ì´ ìˆìœ¼ë©´ ë„“ê²Œ ê¸ê³  ë¶„ë¥˜)
    if any(len(buckets[s]) < MIN_ARTICLES_PER_SECTION for s in SECTION_ORDER):
        pool: List[dict] = []
        for q in GLOBAL_BACKFILL_QUERIES:
            pool.extend(naver_search_window(q, start_kst, end_kst, max_pages=MAX_PAGES_PER_QUERY))

        pool = dedupe(pool)
        pool.sort(key=lambda x: (quality_score(x), x.get("published_kst","")), reverse=True)
        pool = pool[:GLOBAL_BACKFILL_LIMIT]

        for it in pool:
            u = it["url"]
            if u in seen_urls:
                continue
            sec = classify_section(it)
            if len(buckets[sec]) >= MAX_ARTICLES_PER_SECTION:
                continue
            buckets[sec].append(it)
            seen_urls.add(u)

        for sec in SECTION_ORDER:
            logging.info("[Backfill] %s: %d", sec, len(buckets[sec]))

    return buckets

# =========================
# OpenAI ìš”ì•½(2~3ë¬¸ì¥ + ì²´í¬í¬ì¸íŠ¸)
# =========================
def openai_summarize(buckets: Dict[str, List[dict]], start_kst: datetime, end_kst: datetime) -> Dict[str, List[dict]]:
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    total = sum(len(v) for v in buckets.values())
    if total == 0:
        return buckets

    if not api_key:
        for sec in SECTION_ORDER:
            for a in buckets[sec]:
                a["summary"] = a.get("description","") or a.get("title","")
                a["point"] = ""
        return buckets

    compact = []
    for sec in SECTION_ORDER:
        for a in buckets[sec]:
            compact.append({
                "section": sec,
                "press": a.get("press",""),
                "title": a.get("title",""),
                "description": a.get("description",""),
                "url": a.get("url",""),
            })

    schema = {
        "type": "object",
        "properties": {
            "items": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string"},
                        "summary": {"type": "string"},
                        "point": {"type": "string"},
                    },
                    "required": ["url","summary","point"],
                    "additionalProperties": False,
                },
            }
        },
        "required": ["items"],
        "additionalProperties": False,
    }

    system = (
        "ë„ˆëŠ” ë†í˜‘ ê²½ì œì§€ì£¼ ì›ì˜ˆìˆ˜ê¸‰ë¶€(ê³¼ìˆ˜í™”í›¼íŒ€ ì¤‘ì‹¬) ë‚´ë¶€ ê³µìœ ìš© ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì‘ì„±ìë‹¤.\n"
        "ê° ê¸°ì‚¬ë§ˆë‹¤ ì•„ë˜ ìˆœì„œë¥¼ ë°˜ë“œì‹œ ì§€ì¼œë¼:\n"
        "1) summary: 2~3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ(ìˆ˜ê¸‰/ê°€ê²©/ë¬¼ëŸ‰/ì¶œí•˜/ì €ì¥ëŸ‰/ìœ í†µ/ì •ì±…/ë°©ì œ ê´€ì )\n"
        "2) point: ì²´í¬í¬ì¸íŠ¸ 1ë¬¸ì¥(íŒ€ì´ ë¬´ì—‡ì„ í™•ì¸/ëŒ€ì‘í•´ì•¼ í•˜ëŠ”ì§€)\n"
        "ê³¼ì¥/ì¶”ì¸¡ ê¸ˆì§€. ì• ë§¤í•˜ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ.\n"
        "ë¬¸ì¥ ì§§ê³  ê°€ë…ì„± ì¢‹ê²Œ.\n"
    )
    user = f"ê¸°ê°„(KST): {start_kst.isoformat()} ~ {end_kst.isoformat()}\n{json.dumps(compact, ensure_ascii=False)}"

    payload = {
        "model": OPENAI_MODEL,
        "input": [{"role":"system","content":system},{"role":"user","content":user}],
        "reasoning_effort": OPENAI_REASONING_EFFORT,
        "max_output_tokens": OPENAI_MAX_OUTPUT_TOKENS,
        "text": {"format": {"type":"json_schema","name":"agri_summaries","strict":True,"schema":schema}},
        "store": False,
    }

    r = requests.post(
        OPENAI_RESPONSES_URL,
        headers={"Authorization": f"Bearer {api_key}", "Content-Type":"application/json"},
        json=payload,
        timeout=90,
    )
    if not r.ok:
        logging.error("[OpenAI ERROR] %s", r.text)
        for sec in SECTION_ORDER:
            for a in buckets[sec]:
                a["summary"] = a.get("description","") or a.get("title","")
                a["point"] = ""
        return buckets

    data = r.json()
    out_text = ""
    for item in data.get("output", []) or []:
        if item.get("type") == "message":
            for c in item.get("content", []) or []:
                if c.get("type") == "output_text":
                    out_text += c.get("text","")
    out_text = out_text.strip()
    if not out_text:
        return buckets

    j = json.loads(out_text)
    mp = {it["url"]: it for it in j.get("items", [])}

    for sec in SECTION_ORDER:
        for a in buckets[sec]:
            u = a.get("url","")
            m = mp.get(u, {})
            summary = (m.get("summary") or "").strip()
            point = (m.get("point") or "").strip()
            if not summary:
                summary = a.get("description","") or a.get("title","")
            a["summary"] = summary
            a["point"] = point

    return buckets

# =========================
# Pages URL
# =========================
def auto_pages_url() -> str:
    repo = os.getenv("GITHUB_REPOSITORY", "")
    if not repo or "/" not in repo:
        return ""
    owner, name = repo.split("/", 1)
    return f"https://{owner}.github.io/{name}/"

def archive_url_for_date(base_url: str, report_date: str) -> str:
    return f"{base_url}archive/{report_date}.html"

# =========================
# ìƒì„¸ í˜ì´ì§€(ëª¨ë°”ì¼ ì¹´ë“œ UI) + ì„¹ì…˜ ì•µì»¤ + ì¹© ì´ë™
# =========================
def make_html(buckets: Dict[str, List[dict]], start_kst: datetime, end_kst: datetime, archive_link: str) -> str:
    total = sum(len(v) for v in buckets.values())
    span_days = (end_kst.date() - start_kst.date()).days

    def esc(x: str) -> str:
        return html.escape(x or "")

    sec_ids = {
        "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥": "sec-supply",
        "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…": "sec-policy",
        "ë³‘í•´ì¶© ë° ë°©ì œ": "sec-pest",
        "ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)": "sec-dist",
    }

    def card(a: dict) -> str:
        press = esc(a.get("press","ë¯¸ìƒ"))
        hm = esc(a.get("published_hm",""))
        title = esc(a.get("title",""))
        summary = esc((a.get("summary") or "").strip())
        point = esc((a.get("point") or "").strip())
        url = a.get("url","")

        point_html = f'<div class="point">ì²´í¬í¬ì¸íŠ¸: {point}</div>' if point else ""
        return f"""
        <div class="card">
          <div class="meta"><span class="press">{press}</span><span class="time">{hm}</span></div>
          <div class="title">{title}</div>
          <div class="summary">{summary}</div>
          {point_html}
          <a class="btn" href="{esc(url)}" target="_blank" rel="noopener noreferrer">ì›ë¬¸ ì—´ê¸°</a>
        </div>
        """

    chips_html = ""
    for sec in SECTION_ORDER:
        sid = sec_ids[sec]
        chips_html += f'<a class="chip" href="#{sid}">{esc(sec)} {len(buckets.get(sec, []))}ê±´</a>'

    sections_html = ""
    for sec in SECTION_ORDER:
        sid = sec_ids[sec]
        items = buckets.get(sec, [])
        sections_html += f'<div class="section" id="{sid}"><div class="secbar"><h2>{esc(sec)} <span class="count">({len(items)})</span></h2></div>'
        if not items:
            sections_html += '<div class="empty">íŠ¹ì´ì‚¬í•­ ì—†ìŒ</div></div>'
            continue
        for a in items:
            sections_html += card(a)
        sections_html += "</div>"

    note = ""
    if span_days >= 2:
        note = f"íœ´ì¼/ì£¼ë§ ëˆ„ì  í¬í•¨: {span_days}ì¼"

    return f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ë†ì‚°ë¬¼ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</title>
<style>
  :root {{
    --bg:#f6f7f9; --card:#fff; --line:#e5e7eb; --text:#111827; --muted:#6b7280;
    --bar:#111827;
  }}
  html{{scroll-behavior:smooth;}}
  body{{margin:0;background:var(--bg);color:var(--text);font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Arial,sans-serif;}}
  .wrap{{max-width:980px;margin:0 auto;padding:14px;}}
  .header{{background:var(--card);border:1px solid var(--line);border-radius:14px;padding:14px;margin-bottom:12px;box-shadow:0 2px 10px rgba(0,0,0,.04);}}
  .toprow{{display:flex;justify-content:space-between;align-items:flex-start;gap:10px;}}
  .h1{{font-size:18px;font-weight:900;margin:0 0 6px;}}
  .sub{{color:var(--muted);font-size:13px;line-height:1.35;}}
  .linkrow a{{font-size:12px;color:#111;text-decoration:none;border:1px solid var(--line);border-radius:999px;padding:6px 10px;display:inline-block;background:#fff;}}
  .chips{{margin-top:10px;display:flex;flex-wrap:wrap;gap:8px;}}
  .chip{{font-size:12px;color:#111;border:1px solid var(--line);background:#fff;border-radius:999px;padding:7px 10px;text-decoration:none;font-weight:800;}}
  .chip:active{{transform:scale(.99);}}
  .section{{margin-top:14px;border-top:3px solid var(--bar);padding-top:10px;}}
  .secbar{{background:var(--card);border:1px solid var(--line);border-radius:14px;padding:10px 12px;box-shadow:0 2px 10px rgba(0,0,0,.03);}}
  h2{{font-size:16px;margin:0;display:flex;justify-content:space-between;align-items:center;}}
  .count{{color:var(--muted);font-weight:700;}}
  .empty{{color:var(--muted);background:var(--card);border:1px dashed var(--line);border-radius:12px;padding:12px;margin-top:10px;}}
  .card{{background:var(--card);border:1px solid var(--line);border-radius:14px;padding:12px;margin:10px 0;box-shadow:0 2px 10px rgba(0,0,0,.03);}}
  .meta{{display:flex;justify-content:space-between;align-items:center;margin-bottom:6px;}}
  .press{{font-weight:900;font-size:13px;}}
  .time{{color:var(--muted);font-size:12px;}}
  .title{{font-size:15px;font-weight:900;line-height:1.35;margin:4px 0 8px;}}
  .summary{{font-size:14px;line-height:1.55;color:#111;margin:0 0 8px;}}
  .point{{font-size:13px;line-height:1.45;color:#0f172a;background:#f3f4f6;border-radius:10px;padding:8px 10px;margin:6px 0 10px;}}
  .btn{{display:inline-block;text-decoration:none;font-weight:900;font-size:14px;border:1px solid var(--line);border-radius:12px;padding:10px 12px;}}
  .footer{{color:var(--muted);font-size:12px;margin:18px 4px 8px;}}
</style>
</head>
<body>
<div class="wrap">
  <div class="header">
    <div class="toprow">
      <div>
        <div class="h1">ë†ì‚°ë¬¼ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</div>
        <div class="sub">ê¸°ê°„: {esc(start_kst.strftime('%Y-%m-%d %H:%M'))} ~ {esc(end_kst.strftime('%Y-%m-%d %H:%M'))} (KST) Â· ì´ {total}ê±´<br>{esc(note)}</div>
      </div>
      <div class="linkrow">
        <a href="{esc(archive_link)}">ì•„ì¹´ì´ë¸Œ ëª©ë¡</a>
      </div>
    </div>
    <div class="chips">
      {chips_html}
    </div>
  </div>

  {sections_html}

  <div class="footer">* íœ´ì¼/ì£¼ë§ì—ëŠ” ë°œì†¡ì„ ìŠ¤í‚µí•˜ê³  stateê°€ ê°±ì‹ ë˜ì§€ ì•Šì•„, ë‹¤ìŒ ì˜ì—…ì¼ì— ëˆ„ì  êµ¬ê°„ì´ ìë™ í™•ì¥ë©ë‹ˆë‹¤(ì¤‘ë³µ URLì€ 1íšŒë§Œ ë°˜ì˜).</div>
</div>
</body>
</html>
"""

def make_archive_index_html(base_url: str, manifest: List[dict]) -> str:
    def esc(x: str) -> str:
        return html.escape(x or "")
    rows = ""
    # ìµœì‹ ì´ ìœ„ë¡œ
    for it in sorted(manifest, key=lambda x: x.get("date",""), reverse=True)[:120]:
        d = it.get("date","")
        total = it.get("total",0)
        start = it.get("start","")
        end = it.get("end","")
        url = archive_url_for_date(base_url, d)
        rows += f'<a class="row" href="{esc(url)}"><div class="d">{esc(d)}</div><div class="m">ì´ {total}ê±´ Â· {esc(start)}~{esc(end)}</div></a>'

    return f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ë†ì‚°ë¬¼ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì•„ì¹´ì´ë¸Œ</title>
<style>
  :root{{--bg:#f6f7f9;--card:#fff;--line:#e5e7eb;--text:#111827;--muted:#6b7280;}}
  body{{margin:0;background:var(--bg);color:var(--text);font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Arial,sans-serif;}}
  .wrap{{max-width:860px;margin:0 auto;padding:14px;}}
  .header{{background:var(--card);border:1px solid var(--line);border-radius:14px;padding:14px;box-shadow:0 2px 10px rgba(0,0,0,.04);}}
  .h1{{font-size:18px;font-weight:900;margin:0 0 6px;}}
  .sub{{color:var(--muted);font-size:13px;line-height:1.35;}}
  .list{{margin-top:12px;}}
  .row{{display:block;background:var(--card);border:1px solid var(--line);border-radius:14px;padding:12px;margin:10px 0;text-decoration:none;color:var(--text);box-shadow:0 2px 10px rgba(0,0,0,.03);}}
  .d{{font-weight:900;font-size:15px;margin-bottom:4px;}}
  .m{{color:var(--muted);font-size:12px;}}
</style>
</head>
<body>
<div class="wrap">
  <div class="header">
    <div class="h1">ë¸Œë¦¬í•‘ ì•„ì¹´ì´ë¸Œ</div>
    <div class="sub">ë‚ ì§œë¥¼ ëˆ„ë¥´ë©´ í•´ë‹¹ ì¼ìì˜ ë¸Œë¦¬í•‘ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.</div>
  </div>
  <div class="list">{rows or '<div class="row"><div class="d">ê¸°ë¡ ì—†ìŒ</div></div>'}</div>
</div>
</body>
</html>
"""

def publish_file(path: str, html_text: str, message: str) -> None:
    if PUBLISH_MODE != "github_pages":
        return
    repo = os.getenv("GITHUB_REPOSITORY", "")
    token = os.getenv("GITHUB_TOKEN", "")
    if not repo or not token:
        logging.warning("[Pages] missing repo/token")
        return
    raw, sha = github_get_file(repo, path, token, ref=PAGES_BRANCH)
    _ = raw
    github_put_file(repo, path, token, html_text, branch=PAGES_BRANCH, sha=sha, message=message)

# =========================
# ì¹´í†¡ ë©”ì‹œì§€(ê°€ë…ì„± ì „ë©´ ê°œí¸)
# =========================
def build_kakao_message(buckets: Dict[str, List[dict]], start_kst: datetime, end_kst: datetime) -> str:
    total = sum(len(v) for v in buckets.values())
    span_days = (end_kst.date() - start_kst.date()).days
    span_hint = f" Â· ëˆ„ì  {span_days}ì¼" if span_days >= 2 else ""

    counts = {
        "í’ˆëª©": len(buckets.get("í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥", [])),
        "ì •ì±…": len(buckets.get("ì£¼ìš” ì´ìŠˆ ë° ì •ì±…", [])),
        "ë°©ì œ": len(buckets.get("ë³‘í•´ì¶© ë° ë°©ì œ", [])),
        "ìœ í†µ": len(buckets.get("ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)", [])),
    }

    # ì„¹ì…˜ë³„ Top 1 (ìµœëŒ€ 4ì¤„)
    top_lines = []
    label_map = {
        "í’ˆëª© ë° ìˆ˜ê¸‰ ë™í–¥": "í’ˆëª©",
        "ì£¼ìš” ì´ìŠˆ ë° ì •ì±…": "ì •ì±…",
        "ë³‘í•´ì¶© ë° ë°©ì œ": "ë°©ì œ",
        "ìœ í†µ ë° í˜„ì¥(APC/ìˆ˜ì¶œ)": "ìœ í†µ",
    }
    for sec in SECTION_ORDER:
        items = buckets.get(sec, [])
        if not items:
            continue
        a = items[0]
        press = a.get("press","")
        title = clamp(a.get("title",""), 28)
        top_lines.append(f"- {label_map[sec]}: {press} | {title}")

    if not top_lines:
        top_lines = ["- ì˜¤ëŠ˜ì€ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê¸°ì‚¬ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤(í‚¤ì›Œë“œ/í•„í„° ì ê²€ í•„ìš”)"]

    msg = "\n".join([
        f"[ë†ì‚°ë¬¼ ë¸Œë¦¬í•‘] {end_kst.strftime('%m/%d')} {RUN_HOUR_KST:02d}ì‹œ",
        f"ê¸°ê°„: {start_kst.strftime('%m/%d %H:%M')}~{end_kst.strftime('%m/%d %H:%M')} (KST){span_hint}",
        f"ì´ {total}ê±´ | í’ˆëª© {counts['í’ˆëª©']} Â· ì •ì±… {counts['ì •ì±…']} Â· ë°©ì œ {counts['ë°©ì œ']} Â· ìœ í†µ {counts['ìœ í†µ']}",
        "í•µì‹¬:",
        *top_lines[:4],
        "ğŸ‘‰ ë²„íŠ¼ â€˜ë¸Œë¦¬í•‘ ì—´ê¸°â€™ì—ì„œ ì„¹ì…˜ë³„ ìš”ì•½/ì²´í¬í¬ì¸íŠ¸/ì›ë¬¸ í™•ì¸",
    ])
    return clamp(msg, KAKAO_MESSAGE_SOFT_LIMIT)

# =========================
# Main
# =========================
def main():
    now_kst = datetime.now(tz=KST)
    end_kst = compute_fixed_end_kst(now_kst, RUN_HOUR_KST, EARLY_GRACE_MINUTES)

    # ì˜ì—…ì¼ë§Œ ë°œì†¡(íœ´ì¼/ì£¼ë§ì€ ìŠ¤í‚µ -> state ë¯¸ê°±ì‹  -> ë‹¤ìŒ ì˜ì—…ì¼ ëˆ„ì )
    if (not FORCE_SEND) and (not is_business_day(end_kst.date())):
        logging.info("[SKIP] Not a business day in KR: %s (weekend/holiday)", end_kst.date())
        return

    state = load_state(end_kst)
    try:
        start_kst = datetime.fromisoformat(state.last_end_kst_iso)
        if start_kst.tzinfo is None:
            start_kst = start_kst.replace(tzinfo=KST)
    except Exception:
        start_kst = end_kst - timedelta(hours=24)

    if start_kst >= end_kst:
        start_kst = end_kst - timedelta(hours=24)

    logging.info("[INFO] Window KST: %s ~ %s", start_kst, end_kst)

    base_url = BRIEF_VIEW_URL or auto_pages_url()
    if not base_url:
        raise RuntimeError("BRIEF_VIEW_URL is empty and auto_pages_url failed.")
    if not base_url.endswith("/"):
        base_url += "/"

    report_date = end_kst.strftime("%Y-%m-%d")
    today_archive_web = archive_url_for_date(base_url, report_date)
    archive_index_web = f"{base_url}archive/"

    # 1) ìˆ˜ì§‘(ì „ë©´ í•„í„° ì ìš© + ë°±í•„)
    buckets = collect_articles(start_kst, end_kst)

    # 2) ìš”ì•½
    buckets = openai_summarize(buckets, start_kst, end_kst)

    # 3) ìƒì„¸ í˜ì´ì§€ ìƒì„±(ì¼ìë³„ + ìµœì‹ )
    html_page = make_html(buckets, start_kst, end_kst, archive_link=archive_index_web)

    # ìµœì‹  í˜ì´ì§€ ê°±ì‹ 
    publish_file(PAGES_LATEST_PATH, html_page, message=f"Publish latest brief {report_date}")

    # ì¼ìë³„ ì•„ì¹´ì´ë¸Œ ì €ì¥
    archive_path = f"{ARCHIVE_DIR}/{report_date}.html"
    publish_file(archive_path, html_page, message=f"Publish archive brief {report_date}")

    # ì•„ì¹´ì´ë¸Œ ëª©ë¡/manifest ê°±ì‹ 
    manifest = load_archive_manifest()
    # ë™ì¼ ë‚ ì§œ ìˆìœ¼ë©´ ê°±ì‹ 
    manifest = [m for m in manifest if m.get("date") != report_date]
    manifest.append({
        "date": report_date,
        "start": start_kst.strftime("%m/%d %H:%M"),
        "end": end_kst.strftime("%m/%d %H:%M"),
        "total": sum(len(v) for v in buckets.values()),
    })
    save_archive_manifest(manifest)

    archive_index_html = make_archive_index_html(base_url, manifest)
    publish_file(ARCHIVE_INDEX_PATH, archive_index_html, message="Update archive index")

    # 4) ì¹´í†¡ 1ë©”ì‹œì§€(ë³¸ë¬¸ ë§í¬ ì œê±°, ë²„íŠ¼ì€ ì˜¤ëŠ˜ì ì•„ì¹´ì´ë¸Œë¡œ)
    refresh_token = os.getenv("KAKAO_REFRESH_TOKEN", "").strip()
    if not refresh_token:
        raise RuntimeError("Missing KAKAO_REFRESH_TOKEN")
    access = kakao_refresh_access_token(refresh_token)

    msg = build_kakao_message(buckets, start_kst, end_kst)
    kakao_send_text(access, msg, today_archive_web)

    # 5) state ì €ì¥(ë°œì†¡ ì„±ê³µ í›„)
    save_state(end_kst)
    logging.info("[DONE] sent and state updated: %s", end_kst.isoformat())

if __name__ == "__main__":
    main()
